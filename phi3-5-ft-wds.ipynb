{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":14348447,"sourceType":"datasetVersion","datasetId":9161641}],"dockerImageVersionId":31240,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"de70f508-616e-4fa9-86ea-1408f8f1a4bd","cell_type":"markdown","source":"# Phi‑3.5 Fine‑Tuning on Kaggle (T4) — JSONL from `/kaggle/input` or `/kaggle/working`\n\nThis Kaggle notebook fine‑tunes **microsoft/Phi‑3.5‑mini‑instruct** using **PEFT LoRA + TRL SFTTrainer** with **T4‑safe settings**.\n\n## Dataset format (JSONL)\nEach line:\n```json\n{\"conversations\":[{\"from\":\"human\",\"value\":\"...\"},{\"from\":\"gpt\",\"value\":\"...\"}]}\n```\n\n## Where to put the dataset\n- Recommended: add your JSONL as a Kaggle Dataset → it will appear in `/kaggle/input/<dataset_name>/`\n- Or upload/copy it into `/kaggle/working/`\n\n## Important\nAfter the install cell, **Restart Session** (Kaggle requirement) so imports load the correct versions.","metadata":{}},{"id":"4cc90a3e-df5e-4661-b932-c13a6168cae5","cell_type":"code","source":"# ===== (0) GPU sanity check =====\nimport torch\nprint(\"CUDA available:\", torch.cuda.is_available())\nif torch.cuda.is_available():\n    print(\"GPU:\", torch.cuda.get_device_name(0))\n    !nvidia-smi\nelse:\n    print(\"No GPU detected. Enable GPU in Kaggle Settings.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-31T00:20:04.747405Z","iopub.execute_input":"2025-12-31T00:20:04.747668Z","iopub.status.idle":"2025-12-31T00:20:06.809797Z","shell.execute_reply.started":"2025-12-31T00:20:04.747649Z","shell.execute_reply":"2025-12-31T00:20:06.808974Z"}},"outputs":[{"name":"stdout","text":"CUDA available: True\nGPU: Tesla T4\nWed Dec 31 00:20:06 2025       \n+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 570.172.08             Driver Version: 570.172.08     CUDA Version: 12.8     |\n|-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n| N/A   69C    P8             16W /   70W |       3MiB /  15360MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n|   1  Tesla T4                       Off |   00000000:00:05.0 Off |                    0 |\n| N/A   75C    P8             17W /   70W |       3MiB /  15360MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n                                                                                         \n+-----------------------------------------------------------------------------------------+\n| Processes:                                                                              |\n|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n|        ID   ID                                                               Usage      |\n|=========================================================================================|\n|  No running processes found                                                             |\n+-----------------------------------------------------------------------------------------+\n","output_type":"stream"}],"execution_count":1},{"id":"996f5f7f-b9c3-4099-84e5-c0d2ac785b77","cell_type":"markdown","source":"## 1) Clean install (fixes mixed Transformers / dependency issues)\nRun the next cell once, then **Restart Session**, then continue.","metadata":{}},{"id":"40541a8d-5f62-4b32-8e1b-2dcc721bb303","cell_type":"code","source":"# ===== (1) Hard clean + install pinned compatible versions =====\n!pip -q uninstall -y transformers tokenizers huggingface-hub safetensors accelerate datasets peft trl sentencepiece\n!pip -q uninstall -y bitsandbytes triton unsloth\n\n!pip -q install --no-cache-dir --force-reinstall \\\n  \"transformers==4.44.2\" \\\n  \"tokenizers==0.19.1\" \\\n  \"huggingface-hub==0.24.6\" \\\n  \"safetensors==0.4.4\" \\\n  \"accelerate==0.33.0\" \\\n  \"datasets==2.20.0\" \\\n  \"peft==0.12.0\" \\\n  \"trl==0.9.6\" \\\n  \"sentencepiece\"\n\nprint(\"Install complete. NOW: Kaggle → Restart Session, then continue.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-31T00:20:06.810818Z","iopub.execute_input":"2025-12-31T00:20:06.811339Z","iopub.status.idle":"2025-12-31T00:22:40.671273Z","shell.execute_reply.started":"2025-12-31T00:20:06.811302Z","shell.execute_reply":"2025-12-31T00:22:40.670071Z"}},"outputs":[{"name":"stdout","text":"\u001b[33mWARNING: Skipping bitsandbytes as it is not installed.\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Skipping unsloth as it is not installed.\u001b[0m\u001b[33m\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.5/40.5 kB\u001b[0m \u001b[31m240.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.7/57.7 kB\u001b[0m \u001b[31m26.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m240.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.2/91.2 kB\u001b[0m \u001b[31m61.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.1/75.1 kB\u001b[0m \u001b[31m302.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m127.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m124.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m417.5/417.5 kB\u001b[0m \u001b[31m371.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m435.4/435.4 kB\u001b[0m \u001b[31m356.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m315.1/315.1 kB\u001b[0m \u001b[31m363.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m547.8/547.8 kB\u001b[0m \u001b[31m356.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m296.4/296.4 kB\u001b[0m \u001b[31m350.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m245.8/245.8 kB\u001b[0m \u001b[31m380.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m322.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m214.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.1/316.1 kB\u001b[0m \u001b[31m376.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m396.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m327.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.5/66.5 kB\u001b[0m \u001b[31m250.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.7/47.7 MB\u001b[0m \u001b[31m104.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m806.6/806.6 kB\u001b[0m \u001b[31m371.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m800.4/800.4 kB\u001b[0m \u001b[31m319.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m317.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m899.8/899.8 MB\u001b[0m \u001b[31m293.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m594.3/594.3 MB\u001b[0m \u001b[31m362.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m206.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.0/88.0 MB\u001b[0m \u001b[31m342.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m954.8/954.8 kB\u001b[0m \u001b[31m385.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m706.8/706.8 MB\u001b[0m \u001b[31m345.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.1/193.1 MB\u001b[0m \u001b[31m165.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m401.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.6/63.6 MB\u001b[0m \u001b[31m309.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m267.5/267.5 MB\u001b[0m \u001b[31m314.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m288.2/288.2 MB\u001b[0m \u001b[31m248.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m287.2/287.2 MB\u001b[0m \u001b[31m237.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.3/322.3 MB\u001b[0m \u001b[31m263.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.3/39.3 MB\u001b[0m \u001b[31m316.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.7/124.7 MB\u001b[0m \u001b[31m294.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m322.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m170.4/170.4 MB\u001b[0m \u001b[31m212.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 kB\u001b[0m \u001b[31m269.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m275.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m180.7/180.7 kB\u001b[0m \u001b[31m320.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m336.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m292.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.7/154.7 kB\u001b[0m \u001b[31m356.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.9/193.9 kB\u001b[0m \u001b[31m329.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.6/67.6 kB\u001b[0m \u001b[31m296.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m159.4/159.4 kB\u001b[0m \u001b[31m362.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m151.6/151.6 kB\u001b[0m \u001b[31m325.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m231.1/231.1 kB\u001b[0m \u001b[31m320.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.0/71.0 kB\u001b[0m \u001b[31m271.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m246.7/246.7 kB\u001b[0m \u001b[31m369.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m353.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m210.0/210.0 kB\u001b[0m \u001b[31m358.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m229.9/229.9 kB\u001b[0m \u001b[31m300.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m509.2/509.2 kB\u001b[0m \u001b[31m380.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m349.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m348.5/348.5 kB\u001b[0m \u001b[31m365.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m131.2/131.2 kB\u001b[0m \u001b[31m328.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m365.8/365.8 kB\u001b[0m \u001b[31m372.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.9/134.9 kB\u001b[0m \u001b[31m360.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m344.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\nxformers 0.0.22.post7 requires torch==2.1.0, but you have torch 2.9.1 which is incompatible.\ndask-cudf-cu12 25.2.2 requires pandas<2.2.4dev0,>=2.0, but you have pandas 2.3.3 which is incompatible.\npylibcudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\ncudf-cu12 25.2.2 requires pandas<2.2.4dev0,>=2.0, but you have pandas 2.3.3 which is incompatible.\ncudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\nonnx 1.18.0 requires protobuf>=4.25.1, but you have protobuf 3.20.3 which is incompatible.\ns3fs 2025.3.0 requires fsspec==2025.3.0.*, but you have fsspec 2024.5.0 which is incompatible.\na2a-sdk 0.3.10 requires protobuf>=5.29.5, but you have protobuf 3.20.3 which is incompatible.\nray 2.51.1 requires click!=8.3.0,>=7.0, but you have click 8.3.0 which is incompatible.\npreprocessing 0.1.13 requires nltk==3.2.4, but you have nltk 3.9.2 which is incompatible.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\ngoogle-colab 1.0.0 requires notebook==6.5.7, but you have notebook 6.5.4 which is incompatible.\ngoogle-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.3.3 which is incompatible.\ngoogle-colab 1.0.0 requires requests==2.32.3, but you have requests 2.32.5 which is incompatible.\ngoogle-colab 1.0.0 requires tornado==6.4.2, but you have tornado 6.5.2 which is incompatible.\ndopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\nbigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\nibis-framework 9.5.0 requires toolz<1,>=0.11, but you have toolz 1.1.0 which is incompatible.\nthinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\nopencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\ndiffusers 0.34.0 requires huggingface-hub>=0.27.0, but you have huggingface-hub 0.24.6 which is incompatible.\nlibcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\ntorchaudio 2.6.0+cu124 requires torch==2.6.0, but you have torch 2.9.1 which is incompatible.\nopencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\ngradio 5.38.1 requires huggingface-hub>=0.28.1, but you have huggingface-hub 0.24.6 which is incompatible.\ngradio 5.38.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.4 which is incompatible.\ncudf-polars-cu12 25.6.0 requires pylibcudf-cu12==25.6.*, but you have pylibcudf-cu12 25.2.2 which is incompatible.\npydrive2 1.21.3 requires cryptography<44, but you have cryptography 46.0.3 which is incompatible.\npydrive2 1.21.3 requires pyOpenSSL<=24.2.1,>=19.1.0, but you have pyopenssl 25.3.0 which is incompatible.\nimbalanced-learn 0.13.0 requires scikit-learn<2,>=1.3.2, but you have scikit-learn 1.2.2 which is incompatible.\nydf 0.13.0 requires protobuf<7.0.0,>=5.29.1, but you have protobuf 3.20.3 which is incompatible.\ntorchvision 0.21.0+cu124 requires torch==2.6.0, but you have torch 2.9.1 which is incompatible.\nopencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\nplotnine 0.14.5 requires matplotlib>=3.8.0, but you have matplotlib 3.7.2 which is incompatible.\npylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\njupyter-kernel-gateway 2.5.2 requires jupyter-client<8.0,>=5.2.0, but you have jupyter-client 8.6.3 which is incompatible.\numap-learn 0.5.9.post2 requires scikit-learn>=1.6, but you have scikit-learn 1.2.2 which is incompatible.\ngcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2024.5.0 which is incompatible.\nmlxtend 0.23.4 requires scikit-learn>=1.3.1, but you have scikit-learn 1.2.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mInstall complete. NOW: Kaggle → Restart Session, then continue.\n","output_type":"stream"}],"execution_count":2},{"id":"c9e0bb0e-0d7e-4ce7-a945-30ef0d26f13d","cell_type":"markdown","source":"## 2) Verify versions (run after restart)","metadata":{}},{"id":"9407c118-ab36-4c7a-a7ae-0be99cd40318","cell_type":"code","source":"import transformers, tokenizers, datasets, accelerate, peft, trl, huggingface_hub, safetensors\nprint(\"transformers:\", transformers.__version__)\nprint(\"tokenizers:\", tokenizers.__version__)\nprint(\"datasets:\", datasets.__version__)\nprint(\"accelerate:\", accelerate.__version__)\nprint(\"peft:\", peft.__version__)\nprint(\"trl:\", trl.__version__)\nprint(\"hf hub:\", huggingface_hub.__version__)\nprint(\"safetensors:\", safetensors.__version__)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-31T00:22:40.672698Z","iopub.execute_input":"2025-12-31T00:22:40.673318Z","iopub.status.idle":"2025-12-31T00:22:42.266189Z","shell.execute_reply.started":"2025-12-31T00:22:40.673268Z","shell.execute_reply":"2025-12-31T00:22:42.265530Z"}},"outputs":[{"name":"stdout","text":"transformers: 4.44.2\ntokenizers: 0.19.1\ndatasets: 2.20.0\naccelerate: 0.33.0\npeft: 0.12.0\ntrl: 0.9.6\nhf hub: 0.24.6\nsafetensors: 0.4.4\n","output_type":"stream"}],"execution_count":3},{"id":"4a31f879-e37b-4d1a-9813-6eae3c4c5df6","cell_type":"markdown","source":"## 3) Find JSONL dataset files\nPlace one or more `*.jsonl` files in:\n- `/kaggle/input/<dataset_name>/` (preferred), or\n- `/kaggle/working/`\n\nThen run:","metadata":{}},{"id":"83493ecf-cb59-40d9-b5fc-c9750af36f6d","cell_type":"code","source":"from glob import glob\n\njsonl_files = sorted(glob(\"/kaggle/working/*.jsonl\")) + sorted(glob(\"/kaggle/input/*/*.jsonl\"))\nassert jsonl_files, \"No .jsonl found. Add your JSONL as a Kaggle Dataset or place it in /kaggle/working.\"\n\nprint(\"Found JSONL files:\")\nfor f in jsonl_files:\n    print(\" -\", f)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-31T00:22:42.266936Z","iopub.execute_input":"2025-12-31T00:22:42.267291Z","iopub.status.idle":"2025-12-31T00:22:42.276440Z","shell.execute_reply.started":"2025-12-31T00:22:42.267272Z","shell.execute_reply":"2025-12-31T00:22:42.275707Z"}},"outputs":[{"name":"stdout","text":"Found JSONL files:\n - /kaggle/input/final-hydraulics-water-ft-dataset/FINAL_hydraulics_water_FT_dataset.jsonl\n","output_type":"stream"}],"execution_count":4},{"id":"e2720bac-0905-4872-a225-b4ef2ad7e378","cell_type":"markdown","source":"## 4) Load + merge JSONL, normalize schema, (optional) dedupe","metadata":{}},{"id":"1ebdf0ca-eb84-4756-9971-919ebebcd79b","cell_type":"code","source":"import re\nfrom datasets import load_dataset, concatenate_datasets\n\nparts = [load_dataset(\"json\", data_files=f, split=\"train\") for f in jsonl_files]\nraw = concatenate_datasets(parts) if len(parts) > 1 else parts[0]\n\nprint(raw)\nprint(\"Columns:\", raw.column_names)\n\ndef normalize_record(ex):\n    conv = ex.get(\"conversations\", [])\n    if not isinstance(conv, list):\n        return {\"_valid\": False, \"conversations\": []}\n\n    human = next((m[\"value\"].strip() for m in conv\n                  if isinstance(m, dict) and m.get(\"from\")==\"human\" and isinstance(m.get(\"value\"), str)), None)\n    gpt   = next((m[\"value\"].strip() for m in conv\n                  if isinstance(m, dict) and m.get(\"from\")==\"gpt\" and isinstance(m.get(\"value\"), str)), None)\n\n    if not human or not gpt:\n        return {\"_valid\": False, \"conversations\": []}\n\n    return {\"_valid\": True, \"conversations\":[{\"from\":\"human\",\"value\":human},{\"from\":\"gpt\",\"value\":gpt}]}\n\nds = raw.map(normalize_record).filter(lambda x: x[\"_valid\"])\nds = ds.remove_columns([c for c in ds.column_names if c not in [\"conversations\"]])\n\nprint(\"Valid records:\", len(ds))\nprint(\"Sample prompt preview:\\n\", ds[0][\"conversations\"][0][\"value\"][:200])\n\n# Optional dedupe by normalized prompt text:\nDEDUP = True\nif DEDUP:\n    def prompt_key(ex):\n        p = ex[\"conversations\"][0][\"value\"].lower()\n        p = re.sub(r\"[^\\w\\s]\", \"\", p)\n        p = re.sub(r\"\\s+\", \" \", p).strip()\n        return {\"_k\": p}\n    tmp = ds.map(prompt_key)\n    seen = set()\n    keep_idx = []\n    for i, k in enumerate(tmp[\"_k\"]):\n        if k in seen:\n            continue\n        seen.add(k)\n        keep_idx.append(i)\n    ds = ds.select(keep_idx)\n    print(\"After dedupe:\", len(ds))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-31T00:22:42.277944Z","iopub.execute_input":"2025-12-31T00:22:42.278201Z","iopub.status.idle":"2025-12-31T00:22:42.588885Z","shell.execute_reply.started":"2025-12-31T00:22:42.278183Z","shell.execute_reply":"2025-12-31T00:22:42.588258Z"}},"outputs":[{"name":"stdout","text":"Dataset({\n    features: ['conversations'],\n    num_rows: 68\n})\nColumns: ['conversations']\nValid records: 68\nSample prompt preview:\n How is water usage factored for schools that operate for 8 hours a day?\nAfter dedupe: 68\n","output_type":"stream"}],"execution_count":5},{"id":"5dfe3199-b135-4a9f-a1a3-70ccc889b07e","cell_type":"markdown","source":"## 5) Train / eval split (stratify if conclusion labels exist)","metadata":{}},{"id":"00047747-c7fe-42fe-998c-6363257f4162","cell_type":"code","source":"import re\n\ndef extract_label(ex):\n    text = ex[\"conversations\"][1][\"value\"]\n    m = re.search(r\"## Conclusion\\s*\\(Pass / Fail / Cannot verify\\)\\s*\\n\\s*(Pass|Fail|Cannot verify)\\b\", text)\n    return {\"label\": m.group(1) if m else \"Unknown\"}\n\nlabeled = ds.map(extract_label)\nlabels = set(labeled.unique(\"label\"))\nprint(\"Labels found:\", labels)\n\nEVAL_SIZE = 0.10\nSEED = 42\n\ntry:\n    usable = labeled.filter(lambda x: x[\"label\"] != \"Unknown\")\n    if labels.issuperset({\"Pass\",\"Fail\",\"Cannot verify\"}) and len(usable) >= 50:\n        splits = usable.train_test_split(test_size=EVAL_SIZE, seed=SEED, shuffle=True, stratify_by_column=\"label\")\n    else:\n        splits = labeled.train_test_split(test_size=EVAL_SIZE, seed=SEED, shuffle=True)\nexcept Exception as e:\n    print(\"Stratified split not available, using normal split. Reason:\", e)\n    splits = labeled.train_test_split(test_size=EVAL_SIZE, seed=SEED, shuffle=True)\n\ntrain_ds = splits[\"train\"]\neval_ds  = splits[\"test\"]\n\nprint(\"Train:\", len(train_ds), \"Eval:\", len(eval_ds))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-31T00:22:42.590749Z","iopub.execute_input":"2025-12-31T00:22:42.592185Z","iopub.status.idle":"2025-12-31T00:22:42.609899Z","shell.execute_reply.started":"2025-12-31T00:22:42.592161Z","shell.execute_reply":"2025-12-31T00:22:42.609071Z"}},"outputs":[{"name":"stdout","text":"Labels found: {'Cannot verify', 'Unknown'}\nTrain: 61 Eval: 7\n","output_type":"stream"}],"execution_count":6},{"id":"7af8958b-9978-4103-9051-90aba5b24c42","cell_type":"markdown","source":"## 6) Load Phi‑3.5 + build chat-template `text`","metadata":{}},{"id":"840880df-15ef-44c3-b2c6-7dc2e907954c","cell_type":"code","source":"import torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\nMODEL_NAME = \"microsoft/Phi-3.5-mini-instruct\"\n\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    MODEL_NAME,\n    torch_dtype=torch.float16,\n    device_map=\"auto\",\n)\n\ndef to_text(ex):\n    conv = ex[\"conversations\"]\n    msgs = [\n        {\"role\":\"user\", \"content\": conv[0][\"value\"]},\n        {\"role\":\"assistant\", \"content\": conv[1][\"value\"]},\n    ]\n    return {\"text\": tokenizer.apply_chat_template(msgs, tokenize=False, add_generation_prompt=False)}\n\ntrain_text = train_ds.map(to_text, remove_columns=train_ds.column_names)\neval_text  = eval_ds.map(to_text,  remove_columns=eval_ds.column_names)\n\nprint(\"Text dataset ready.\")\nprint(train_text[0][\"text\"][:450])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-31T00:22:42.610504Z","iopub.execute_input":"2025-12-31T00:22:42.610708Z","iopub.status.idle":"2025-12-31T00:23:17.106182Z","shell.execute_reply.started":"2025-12-31T00:22:42.610685Z","shell.execute_reply":"2025-12-31T00:23:17.105485Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ba2543dfd3bd4734bec6625b3911ce41"}},"metadata":{}},{"name":"stdout","text":"Text dataset ready.\n<|user|>\nWhat is the maximum distance from a siamese connection to a hydrant?<|end|>\n<|assistant|>\nThe hydrant must be located no more than 45 metres unobstructed from the siamese connection.<|end|>\n<|endoftext|>\n","output_type":"stream"}],"execution_count":7},{"id":"be7d8120-b925-4946-951f-49f91e8370df","cell_type":"markdown","source":"## 7) Train (T4‑safe LoRA settings)\nIf you see out‑of‑memory:\n- set `MAX_SEQ_LEN=768` and/or\n- increase `GRAD_ACCUM` to 24","metadata":{}},{"id":"67408050-cd80-4539-990b-e1337028fd3c","cell_type":"code","source":"from trl import SFTTrainer\nfrom transformers import TrainingArguments\nfrom peft import LoraConfig\n\nOUTPUT_DIR = \"/kaggle/working/phi35_lora_out\"\n\n# ---- T4-safe defaults ----\nMAX_SEQ_LEN = 1024\nLORA_R = 8\nGRAD_ACCUM = 16\nEPOCHS = 3\n\npeft_cfg = LoraConfig(\n    r=LORA_R,\n    lora_alpha=16,\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n    target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"],\n)\n\nargs = TrainingArguments(\n    output_dir=OUTPUT_DIR,\n    per_device_train_batch_size=1,\n    per_device_eval_batch_size=1,\n    gradient_accumulation_steps=GRAD_ACCUM,\n    learning_rate=2e-4,\n    warmup_ratio=0.03,\n    num_train_epochs=EPOCHS,\n    logging_steps=10,\n    eval_strategy=\"steps\",\n    eval_steps=100,\n    save_strategy=\"steps\",\n    save_steps=100,\n    save_total_limit=2,\n    report_to=\"none\",\n    fp16=True,\n)\n\ntrainer = SFTTrainer(\n    model=model,\n    tokenizer=tokenizer,\n    train_dataset=train_text,\n    eval_dataset=eval_text,\n    dataset_text_field=\"text\",\n    max_seq_length=MAX_SEQ_LEN,\n    packing=False,\n    peft_config=peft_cfg,\n    args=args,\n)\n\ntrainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-31T00:23:17.107055Z","iopub.execute_input":"2025-12-31T00:23:17.107991Z","iopub.status.idle":"2025-12-31T00:24:10.408455Z","shell.execute_reply.started":"2025-12-31T00:23:17.107952Z","shell.execute_reply":"2025-12-31T00:24:10.407430Z"}},"outputs":[{"name":"stderr","text":"2025-12-31 00:23:18.783653: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1767140598.957108     894 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1767140599.009032     894 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': dataset_text_field, max_seq_length. Will not be supported from version '1.0.0'.\n\nDeprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n  warnings.warn(message, FutureWarning)\n/usr/local/lib/python3.11/dist-packages/trl/trainer/sft_trainer.py:280: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/trl/trainer/sft_trainer.py:318: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/trl/trainer/sft_trainer.py:408: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/accelerate/accelerator.py:488: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\nYou are not running the flash-attention implementation, expect numerical differences.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='9' max='9' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [9/9 00:37, Epoch 2/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=9, training_loss=3.5306890275743275, metrics={'train_runtime': 43.1541, 'train_samples_per_second': 4.241, 'train_steps_per_second': 0.209, 'total_flos': 603757371291648.0, 'train_loss': 3.5306890275743275, 'epoch': 2.360655737704918})"},"metadata":{}}],"execution_count":8},{"id":"70262d65-0070-4eae-bd53-ba9da7138980","cell_type":"markdown","source":"## 8) Save adapter + tokenizer","metadata":{}},{"id":"290825f7-86a4-490e-abed-426bfd1d4da5","cell_type":"code","source":"trainer.model.save_pretrained(OUTPUT_DIR)\ntokenizer.save_pretrained(OUTPUT_DIR)\n\nprint(\"Saved to:\", OUTPUT_DIR)\n!ls -lah {OUTPUT_DIR}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-31T00:24:10.411370Z","iopub.execute_input":"2025-12-31T00:24:10.411992Z","iopub.status.idle":"2025-12-31T00:24:10.814795Z","shell.execute_reply.started":"2025-12-31T00:24:10.411970Z","shell.execute_reply":"2025-12-31T00:24:10.813826Z"}},"outputs":[{"name":"stdout","text":"Saved to: /kaggle/working/phi35_lora_out\ntotal 20M\ndrwxr-xr-x 3 root root 4.0K Dec 30 23:14 .\ndrwxr-xr-x 4 root root 4.0K Dec 30 23:13 ..\n-rw-r--r-- 1 root root  733 Dec 31 00:24 adapter_config.json\n-rw-r--r-- 1 root root  18M Dec 31 00:24 adapter_model.safetensors\n-rw-r--r-- 1 root root  293 Dec 31 00:24 added_tokens.json\ndrwxr-xr-x 2 root root 4.0K Dec 30 23:14 checkpoint-9\n-rw-r--r-- 1 root root 5.0K Dec 31 00:24 README.md\n-rw-r--r-- 1 root root  569 Dec 31 00:24 special_tokens_map.json\n-rw-r--r-- 1 root root 3.3K Dec 31 00:24 tokenizer_config.json\n-rw-r--r-- 1 root root 1.8M Dec 31 00:24 tokenizer.json\n-rw-r--r-- 1 root root 489K Dec 31 00:24 tokenizer.model\n","output_type":"stream"}],"execution_count":9},{"id":"dd990a2b-49b0-4fd6-8d69-f33d57b2e1d8","cell_type":"markdown","source":"## 9) Quick inference check","metadata":{}},{"id":"f3f79ebc-6aed-4ca8-8426-3dab946eb963","cell_type":"code","source":"from transformers import pipeline\n\npipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device_map=\"auto\")\n\nprompt = (\n    \"Water distribution review only. \"\n    \"The report provides a hydrant test but no node pressures at the proposed connection. \"\n    \"Provide required resubmission items.\"\n)\n\nout = pipe(prompt, max_new_tokens=220, do_sample=True, temperature=0.7, top_p=0.9)\nprint(out[0][\"generated_text\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-31T00:24:10.816097Z","iopub.execute_input":"2025-12-31T00:24:10.816512Z","iopub.status.idle":"2025-12-31T00:24:10.984919Z","shell.execute_reply.started":"2025-12-31T00:24:10.816482Z","shell.execute_reply":"2025-12-31T00:24:10.983859Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m_get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1602\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1603\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmodule_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1604\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/importlib/__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/pipelines/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_extraction_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPreTrainedFeatureExtractor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_processing_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBaseImageProcessor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfiguration_auto\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAutoConfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/image_processing_utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mimage_processing_base\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBatchFeature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mImageProcessingMixin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mimage_transforms\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcenter_crop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrescale\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mimage_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mChannelDimension\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/image_transforms.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m from .image_utils import (\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0mChannelDimension\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/image_utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mis_torchvision_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInterpolationMode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mextension\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_HAS_OPS\u001b[0m  \u001b[0;31m# usort:skip\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_meta_registrations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mutils\u001b[0m  \u001b[0;31m# usort:skip\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/_meta_registrations.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m \u001b[0;34m@\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlibrary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister_fake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"torchvision::nms\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmeta_nms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miou_threshold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/library.py\u001b[0m in \u001b[0;36mregister\u001b[0;34m(func)\u001b[0m\n\u001b[1;32m   1062\u001b[0m             \u001b[0muse_lib\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1063\u001b[0;31m         use_lib._register_fake(\n\u001b[0m\u001b[1;32m   1064\u001b[0m             \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_stacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_override\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mallow_override\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/library.py\u001b[0m in \u001b[0;36m_register_fake\u001b[0;34m(self, op_name, fn, _stacklevel, allow_override)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m         handle = entry.fake_impl.register(\n\u001b[0m\u001b[1;32m    212\u001b[0m             \u001b[0mfunc_to_register\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_override\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mallow_override\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_library/fake_impl.py\u001b[0m in \u001b[0;36mregister\u001b[0;34m(self, func, source, lib, allow_override)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 )\n\u001b[0;32m---> 50\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch_has_kernel_for_dispatch_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqualname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Meta\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m                 raise RuntimeError(\n","\u001b[0;31mRuntimeError\u001b[0m: operator torchvision::nms does not exist","\nThe above exception was the direct cause of the following exception:\n","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_894/2314902763.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mpipe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"text-generation\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"auto\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m prompt = (\n","\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_handle_fromlist\u001b[0;34m(module, fromlist, import_, recursive)\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1591\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1592\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1593\u001b[0;31m             \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1594\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1595\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m_get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1603\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmodule_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1604\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1605\u001b[0;31m             raise RuntimeError(\n\u001b[0m\u001b[1;32m   1606\u001b[0m                 \u001b[0;34mf\"Failed to import {self.__name__}.{module_name} because of the following error (look up to see its\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1607\u001b[0m                 \u001b[0;34mf\" traceback):\\n{e}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: Failed to import transformers.pipelines because of the following error (look up to see its traceback):\noperator torchvision::nms does not exist"],"ename":"RuntimeError","evalue":"Failed to import transformers.pipelines because of the following error (look up to see its traceback):\noperator torchvision::nms does not exist","output_type":"error"}],"execution_count":10},{"id":"c39e8cf2-5454-42e3-b599-39e2270ab3a9","cell_type":"code","source":"import torch\n\nmodel.eval()\n\nprompt = (\n    \"Water distribution review only. \"\n    \"The report provides a hydrant test but no node pressures at the proposed connection. \"\n    \"Provide required resubmission items.\"\n)\n\nmessages = [{\"role\": \"user\", \"content\": prompt}]\ninputs = tokenizer.apply_chat_template(\n    messages,\n    tokenize=True,\n    add_generation_prompt=True,\n    return_tensors=\"pt\"\n)\n\ninputs = inputs.to(model.device)\n\nwith torch.no_grad():\n    output_ids = model.generate(\n        inputs,\n        max_new_tokens=220,\n        do_sample=True,\n        temperature=0.7,\n        top_p=0.9,\n    )\n\nprint(tokenizer.decode(output_ids[0], skip_special_tokens=True))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-31T00:24:10.985403Z","iopub.status.idle":"2025-12-31T00:24:10.985674Z","shell.execute_reply.started":"2025-12-31T00:24:10.985553Z","shell.execute_reply":"2025-12-31T00:24:10.985564Z"}},"outputs":[],"execution_count":null},{"id":"b9e5d31a-56c7-4488-9e31-4966085aa5d2","cell_type":"code","source":"!pip -q install -U transformers==4.44.2 peft==0.12.0 safetensors sentencepiece huggingface_hub\n\n# llama.cpp conversion requirements\n!pip -q install -U numpy sentencepiece\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-31T00:24:10.986980Z","iopub.status.idle":"2025-12-31T00:24:10.987314Z","shell.execute_reply.started":"2025-12-31T00:24:10.987144Z","shell.execute_reply":"2025-12-31T00:24:10.987160Z"}},"outputs":[],"execution_count":null},{"id":"0829a64f-ad46-46ef-9441-179181c3a5ea","cell_type":"code","source":"!pip -q install --no-cache-dir --force-reinstall \"numpy==1.26.4\"\nprint(\"Done. Restart Session now.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-31T00:24:10.988615Z","iopub.status.idle":"2025-12-31T00:24:10.988933Z","shell.execute_reply.started":"2025-12-31T00:24:10.988770Z","shell.execute_reply":"2025-12-31T00:24:10.988784Z"}},"outputs":[],"execution_count":null},{"id":"99f1f79d-1cab-4493-9554-bebd49137165","cell_type":"code","source":"import numpy as np\nprint(np.__version__)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-31T00:24:10.989405Z","iopub.status.idle":"2025-12-31T00:24:10.989736Z","shell.execute_reply.started":"2025-12-31T00:24:10.989582Z","shell.execute_reply":"2025-12-31T00:24:10.989599Z"}},"outputs":[],"execution_count":null},{"id":"0a819a7e-0ea2-40e5-986b-af8c42ab45d0","cell_type":"code","source":"# Minimal, conversion-only environment (avoid TRL/accelerate unless you need them)\n!pip -q uninstall -y transformers tokenizers huggingface-hub safetensors peft sentencepiece numpy\n!pip -q install --no-cache-dir --force-reinstall \\\n  \"numpy==1.26.4\" \\\n  \"transformers==4.44.2\" \\\n  \"tokenizers==0.19.1\" \\\n  \"huggingface-hub==0.24.6\" \\\n  \"safetensors==0.4.4\" \\\n  \"peft==0.12.0\" \\\n  \"sentencepiece\"\n\nprint(\"Install complete. Restart Session now.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-31T00:24:10.990233Z","iopub.status.idle":"2025-12-31T00:24:10.990563Z","shell.execute_reply.started":"2025-12-31T00:24:10.990401Z","shell.execute_reply":"2025-12-31T00:24:10.990417Z"}},"outputs":[],"execution_count":null},{"id":"d2415ec3-443b-4245-a2fd-25cacdb8d939","cell_type":"code","source":"import torch, numpy as np\nprint(\"CUDA:\", torch.cuda.is_available())\nif torch.cuda.is_available():\n    print(\"GPU:\", torch.cuda.get_device_name(0))\nprint(\"NumPy:\", np.__version__)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-31T00:32:12.123544Z","iopub.execute_input":"2025-12-31T00:32:12.124155Z","iopub.status.idle":"2025-12-31T00:32:12.129393Z","shell.execute_reply.started":"2025-12-31T00:32:12.124125Z","shell.execute_reply":"2025-12-31T00:32:12.128517Z"}},"outputs":[{"name":"stdout","text":"CUDA: True\nGPU: Tesla T4\nNumPy: 1.26.4\n","output_type":"stream"}],"execution_count":11},{"id":"1e56bcf7-4b27-4a35-9d43-23165a13bdac","cell_type":"code","source":"!pip -q uninstall -y transformers tokenizers huggingface-hub safetensors peft sentencepiece numpy\n!pip -q install --no-cache-dir --force-reinstall \\\n  \"numpy==1.26.4\" \\\n  \"transformers==4.44.2\" \\\n  \"tokenizers==0.19.1\" \\\n  \"huggingface-hub==0.24.6\" \\\n  \"safetensors==0.4.4\" \\\n  \"peft==0.12.0\" \\\n  \"sentencepiece\"\n\nprint(\"Install complete. Restart Session one more time, then continue.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-31T00:32:55.303198Z","iopub.execute_input":"2025-12-31T00:32:55.303585Z","iopub.status.idle":"2025-12-31T00:35:17.360332Z","shell.execute_reply.started":"2025-12-31T00:32:55.303557Z","shell.execute_reply":"2025-12-31T00:35:17.359530Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m198.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.5/40.5 kB\u001b[0m \u001b[31m236.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.7/57.7 kB\u001b[0m \u001b[31m68.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m286.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m162.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m323.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m417.5/417.5 kB\u001b[0m \u001b[31m271.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m435.4/435.4 kB\u001b[0m \u001b[31m373.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m296.4/296.4 kB\u001b[0m \u001b[31m321.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m105.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m380.9/380.9 kB\u001b[0m \u001b[31m332.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.4/201.4 kB\u001b[0m \u001b[31m380.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.5/66.5 kB\u001b[0m \u001b[31m316.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m806.6/806.6 kB\u001b[0m \u001b[31m410.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m800.4/800.4 kB\u001b[0m \u001b[31m364.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m899.8/899.8 MB\u001b[0m \u001b[31m165.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m594.3/594.3 MB\u001b[0m \u001b[31m162.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m271.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.0/88.0 MB\u001b[0m \u001b[31m274.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m954.8/954.8 kB\u001b[0m \u001b[31m145.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m706.8/706.8 MB\u001b[0m \u001b[31m145.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.1/193.1 MB\u001b[0m \u001b[31m152.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m325.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.6/63.6 MB\u001b[0m \u001b[31m243.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m267.5/267.5 MB\u001b[0m \u001b[31m215.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m288.2/288.2 MB\u001b[0m \u001b[31m135.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m287.2/287.2 MB\u001b[0m \u001b[31m94.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.3/322.3 MB\u001b[0m \u001b[31m94.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.3/39.3 MB\u001b[0m \u001b[31m171.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.7/124.7 MB\u001b[0m \u001b[31m210.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m337.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m170.4/170.4 MB\u001b[0m \u001b[31m211.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 kB\u001b[0m \u001b[31m331.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m282.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.7/154.7 kB\u001b[0m \u001b[31m348.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m253.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m159.4/159.4 kB\u001b[0m \u001b[31m325.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m151.6/151.6 kB\u001b[0m \u001b[31m342.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.0/71.0 kB\u001b[0m \u001b[31m313.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m335.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m269.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m131.2/131.2 kB\u001b[0m \u001b[31m340.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.9/134.9 kB\u001b[0m \u001b[31m221.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m391.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\nxformers 0.0.22.post7 requires torch==2.1.0, but you have torch 2.9.1 which is incompatible.\ndatasets 2.20.0 requires fsspec[http]<=2024.5.0,>=2023.1.0, but you have fsspec 2025.12.0 which is incompatible.\ndask-cudf-cu12 25.2.2 requires pandas<2.2.4dev0,>=2.0, but you have pandas 2.3.3 which is incompatible.\npylibcudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\ncudf-cu12 25.2.2 requires pandas<2.2.4dev0,>=2.0, but you have pandas 2.3.3 which is incompatible.\ncudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\nonnx 1.18.0 requires protobuf>=4.25.1, but you have protobuf 3.20.3 which is incompatible.\ns3fs 2025.3.0 requires fsspec==2025.3.0.*, but you have fsspec 2025.12.0 which is incompatible.\na2a-sdk 0.3.10 requires protobuf>=5.29.5, but you have protobuf 3.20.3 which is incompatible.\nray 2.51.1 requires click!=8.3.0,>=7.0, but you have click 8.3.0 which is incompatible.\npreprocessing 0.1.13 requires nltk==3.2.4, but you have nltk 3.9.2 which is incompatible.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\ngoogle-colab 1.0.0 requires notebook==6.5.7, but you have notebook 6.5.4 which is incompatible.\ngoogle-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.3.3 which is incompatible.\ngoogle-colab 1.0.0 requires requests==2.32.3, but you have requests 2.32.5 which is incompatible.\ngoogle-colab 1.0.0 requires tornado==6.4.2, but you have tornado 6.5.2 which is incompatible.\ndopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\nbigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\nibis-framework 9.5.0 requires toolz<1,>=0.11, but you have toolz 1.1.0 which is incompatible.\nthinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\nopencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\ndiffusers 0.34.0 requires huggingface-hub>=0.27.0, but you have huggingface-hub 0.24.6 which is incompatible.\nlibcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\ntorchaudio 2.6.0+cu124 requires torch==2.6.0, but you have torch 2.9.1 which is incompatible.\nopencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\ngradio 5.38.1 requires huggingface-hub>=0.28.1, but you have huggingface-hub 0.24.6 which is incompatible.\ngradio 5.38.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.4 which is incompatible.\ncudf-polars-cu12 25.6.0 requires pylibcudf-cu12==25.6.*, but you have pylibcudf-cu12 25.2.2 which is incompatible.\npydrive2 1.21.3 requires cryptography<44, but you have cryptography 46.0.3 which is incompatible.\npydrive2 1.21.3 requires pyOpenSSL<=24.2.1,>=19.1.0, but you have pyopenssl 25.3.0 which is incompatible.\nimbalanced-learn 0.13.0 requires scikit-learn<2,>=1.3.2, but you have scikit-learn 1.2.2 which is incompatible.\nydf 0.13.0 requires protobuf<7.0.0,>=5.29.1, but you have protobuf 3.20.3 which is incompatible.\ntorchvision 0.21.0+cu124 requires torch==2.6.0, but you have torch 2.9.1 which is incompatible.\nopencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\nplotnine 0.14.5 requires matplotlib>=3.8.0, but you have matplotlib 3.7.2 which is incompatible.\npylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\njupyter-kernel-gateway 2.5.2 requires jupyter-client<8.0,>=5.2.0, but you have jupyter-client 8.6.3 which is incompatible.\numap-learn 0.5.9.post2 requires scikit-learn>=1.6, but you have scikit-learn 1.2.2 which is incompatible.\ngcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2025.12.0 which is incompatible.\nmlxtend 0.23.4 requires scikit-learn>=1.3.1, but you have scikit-learn 1.2.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mInstall complete. Restart Session one more time, then continue.\n","output_type":"stream"}],"execution_count":12},{"id":"3bfde526-07fb-4f24-8604-79b8fcb3cf69","cell_type":"code","source":"import numpy, transformers, peft\nprint(\"numpy:\", numpy.__version__)\nprint(\"transformers:\", transformers.__version__)\nprint(\"peft:\", peft.__version__)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-31T00:36:16.970047Z","iopub.execute_input":"2025-12-31T00:36:16.971166Z","iopub.status.idle":"2025-12-31T00:36:16.976215Z","shell.execute_reply.started":"2025-12-31T00:36:16.971117Z","shell.execute_reply":"2025-12-31T00:36:16.975511Z"}},"outputs":[{"name":"stdout","text":"numpy: 1.26.4\ntransformers: 4.44.2\npeft: 0.12.0\n","output_type":"stream"}],"execution_count":13},{"id":"7012cf3a-1f79-42b7-b703-e416b2508119","cell_type":"code","source":"import os\nLORA_DIR = \"/kaggle/working/phi35_lora_out\"\nprint(\"adapter exists:\", os.path.exists(os.path.join(LORA_DIR, \"adapter_model.safetensors\")))\nprint(\"config exists :\", os.path.exists(os.path.join(LORA_DIR, \"adapter_config.json\")))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-31T00:37:50.093401Z","iopub.execute_input":"2025-12-31T00:37:50.094058Z","iopub.status.idle":"2025-12-31T00:37:50.099063Z","shell.execute_reply.started":"2025-12-31T00:37:50.094032Z","shell.execute_reply":"2025-12-31T00:37:50.098338Z"}},"outputs":[{"name":"stdout","text":"adapter exists: True\nconfig exists : True\n","output_type":"stream"}],"execution_count":14},{"id":"025e071c-79a2-4959-8384-ba311bf65bf1","cell_type":"code","source":"import os, torch\nfrom huggingface_hub import snapshot_download\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nfrom peft import PeftModel\n\nBASE_MODEL = \"microsoft/Phi-3.5-mini-instruct\"\nBASE_DIR   = \"/kaggle/working/phi35_base_hf\"\nMERGED_DIR = \"/kaggle/working/phi35_merged_hf\"\n\n# download base once\nsnapshot_download(repo_id=BASE_MODEL, local_dir=BASE_DIR, local_dir_use_symlinks=False)\n\ntokenizer = AutoTokenizer.from_pretrained(BASE_DIR, use_fast=True)\n\nbase = AutoModelForCausalLM.from_pretrained(\n    BASE_DIR,\n    torch_dtype=torch.float32,\n    device_map=\"cpu\",\n)\n\nmodel = PeftModel.from_pretrained(base, LORA_DIR)\nmerged = model.merge_and_unload()\n\nos.makedirs(MERGED_DIR, exist_ok=True)\nmerged.save_pretrained(MERGED_DIR, safe_serialization=True)\ntokenizer.save_pretrained(MERGED_DIR)\n\nprint(\"Merged HF saved to:\", MERGED_DIR)\n!ls -lah {MERGED_DIR}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-31T00:39:13.371858Z","iopub.execute_input":"2025-12-31T00:39:13.372437Z","iopub.status.idle":"2025-12-31T00:41:06.874621Z","shell.execute_reply.started":"2025-12-31T00:39:13.372413Z","shell.execute_reply":"2025-12-31T00:41:06.873504Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:1212: UserWarning: `local_dir_use_symlinks` parameter is deprecated and will be ignored. The process to download files to a local folder has been updated and do not rely on symlinks anymore. You only need to pass a destination folder as`local_dir`.\nFor more details, check out https://huggingface.co/docs/huggingface_hub/main/en/guides/download#download-files-to-local-folder.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Fetching 20 files:   0%|          | 0/20 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"62e5da2813e145fa94d2f982155fdef3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2b37acf698df41f6975f64636e344d7d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"CODE_OF_CONDUCT.md:   0%|          | 0.00/453 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"729fc9d7e2bf4a2aae896cb180ac8a2c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"SECURITY.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5b17fb94ea74499abf282ea4ff55b0b7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"configuration_phi3.py: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"29a82ee9902d4e858e6bbd3fde39dabb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":".gitattributes: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"98b6beb378574680b46b79267f8e8108"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"NOTICE.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2ac3a3f2bec54fcd96770bcffdc0ac5e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"LICENSE: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bf9470e1a1154e5aaaf98a4432291325"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"modeling_phi3.py: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7b668034481f4724815fba93a3e38e63"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"data_summary_card.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4f2616d9cba347c1ac92fcab2fe8e13d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sample_finetune.py: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fc3aa00dc889466dabe8461b0ca00aa7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"07d716e645f44105ba0ef2a35d2a4c60"}},"metadata":{}},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mSafetensorError\u001b[0m                           Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_894/3439617205.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMERGED_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexist_ok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0mmerged\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMERGED_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msafe_serialization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMERGED_DIR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36msave_pretrained\u001b[0;34m(self, save_directory, is_main_process, state_dict, save_function, push_to_hub, max_shard_size, safe_serialization, variant, token, save_peft_format, **kwargs)\u001b[0m\n\u001b[1;32m   2791\u001b[0m                 \u001b[0;31m# At some point we will need to deal better with save_function (used for TPU and other distributed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2792\u001b[0m                 \u001b[0;31m# joyfulness), but for now this enough.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2793\u001b[0;31m                 \u001b[0msafe_save_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshard\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_directory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshard_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"format\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2794\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2795\u001b[0m                 \u001b[0msave_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshard\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_directory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshard_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/safetensors/torch.py\u001b[0m in \u001b[0;36msave_file\u001b[0;34m(tensors, filename, metadata)\u001b[0m\n\u001b[1;32m    284\u001b[0m     \u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m     \"\"\"\n\u001b[0;32m--> 286\u001b[0;31m     \u001b[0mserialize_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_flatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mSafetensorError\u001b[0m: Error while serializing: IoError(Os { code: 28, kind: StorageFull, message: \"No space left on device\" })"],"ename":"SafetensorError","evalue":"Error while serializing: IoError(Os { code: 28, kind: StorageFull, message: \"No space left on device\" })","output_type":"error"}],"execution_count":15},{"id":"0d2ad346-d805-41bd-b68f-95f306da3f80","cell_type":"code","source":"import torch, os, shutil\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nfrom peft import PeftModel\nfrom huggingface_hub import snapshot_download\n\nBASE_MODEL = \"microsoft/Phi-3.5-mini-instruct\"\nBASE_DIR   = \"/kaggle/working/phi35_base_hf\"\nLORA_DIR   = \"/kaggle/working/phi35_lora_out\"\nGGUF_OUT   = \"/kaggle/working/phi35_merged.gguf\"\n\n# download base\nsnapshot_download(\n    repo_id=BASE_MODEL,\n    local_dir=BASE_DIR,\n    local_dir_use_symlinks=False\n)\n\ntokenizer = AutoTokenizer.from_pretrained(BASE_DIR, use_fast=True)\n\nbase = AutoModelForCausalLM.from_pretrained(\n    BASE_DIR,\n    torch_dtype=torch.float32,\n    device_map=\"cpu\",\n)\n\nmodel = PeftModel.from_pretrained(base, LORA_DIR)\nmerged = model.merge_and_unload()\n\n# --- TEMP save just for conversion ---\nTMP_DIR = \"/kaggle/working/tmp_merge\"\nos.makedirs(TMP_DIR, exist_ok=True)\n\nmerged.save_pretrained(\n    TMP_DIR,\n    safe_serialization=False,     # <-- IMPORTANT\n    max_shard_size=\"2GB\"          # <-- prevents single large file\n)\ntokenizer.save_pretrained(TMP_DIR)\n\n# convert to GGUF\n!git clone -q https://github.com/ggml-org/llama.cpp.git /kaggle/working/llama.cpp\n!python /kaggle/working/llama.cpp/convert_hf_to_gguf.py {TMP_DIR} --outfile {GGUF_OUT}\n\n# cleanup immediately\nshutil.rmtree(TMP_DIR)\nshutil.rmtree(BASE_DIR)\n\nprint(\"GGUF created:\", GGUF_OUT)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-31T00:45:10.588865Z","iopub.execute_input":"2025-12-31T00:45:10.589680Z","iopub.status.idle":"2025-12-31T00:45:53.684585Z","shell.execute_reply.started":"2025-12-31T00:45:10.589650Z","shell.execute_reply":"2025-12-31T00:45:53.683521Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:1212: UserWarning: `local_dir_use_symlinks` parameter is deprecated and will be ignored. The process to download files to a local folder has been updated and do not rely on symlinks anymore. You only need to pass a destination folder as`local_dir`.\nFor more details, check out https://huggingface.co/docs/huggingface_hub/main/en/guides/download#download-files-to-local-folder.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Fetching 20 files:   0%|          | 0/20 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"65278c91d7a84615be9f5b748e80413c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"085b2aa1504543dda54626f4b993a315"}},"metadata":{}},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_1107/388021111.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;31m# --- TEMP save just for conversion ---\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0mTMP_DIR\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/kaggle/working/tmp_merge\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTMP_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexist_ok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m merged.save_pretrained(\n","\u001b[0;32m/usr/lib/python3.11/os.py\u001b[0m in \u001b[0;36mmakedirs\u001b[0;34m(name, mode, exist_ok)\u001b[0m\n","\u001b[0;31mOSError\u001b[0m: [Errno 28] No space left on device: '/kaggle/working/tmp_merge'"],"ename":"OSError","evalue":"[Errno 28] No space left on device: '/kaggle/working/tmp_merge'","output_type":"error"}],"execution_count":1},{"id":"06fbc2ff-764e-418c-9803-63aac179e13a","cell_type":"code","source":"!df -h\n!du -h --max-depth=2 /kaggle/working | sort -hr | head -n 30\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-31T00:48:57.606090Z","iopub.execute_input":"2025-12-31T00:48:57.606842Z","iopub.status.idle":"2025-12-31T00:48:58.611420Z","shell.execute_reply.started":"2025-12-31T00:48:57.606806Z","shell.execute_reply":"2025-12-31T00:48:58.610655Z"}},"outputs":[{"name":"stdout","text":"Filesystem                                                              Size  Used Avail Use% Mounted on\noverlay                                                                 7.9T  6.5T  1.5T  82% /\ntmpfs                                                                    64M     0   64M   0% /dev\nshm                                                                      14G  4.0K   14G   1% /dev/shm\n/dev/sdb1                                                               122G  112G   11G  92% /opt/bin\n/dev/loop1                                                               20G   20G     0 100% /kaggle/lib\n192.168.3.2:/data/kagglesdsdata/datasets/9161641/14348447/dfbwdah4v0fy   73T   53T   21T  73% /kaggle/input/final-hydraulics-water-ft-dataset\n/dev/mapper/snap                                                        7.9T  6.5T  1.5T  82% /etc/hosts\ntmpfs                                                                    16G     0   16G   0% /proc/acpi\ntmpfs                                                                    16G     0   16G   0% /proc/scsi\ntmpfs                                                                    16G     0   16G   0% /sys/firmware\n20G\t/kaggle/working\n13G\t/kaggle/working/phi35_merged_hf\n7.2G\t/kaggle/working/phi35_base_hf\n73M\t/kaggle/working/phi35_lora_out\n54M\t/kaggle/working/phi35_lora_out/checkpoint-9\n96K\t/kaggle/working/phi35_base_hf/.cache\n16K\t/kaggle/working/.virtual_documents\n","output_type":"stream"}],"execution_count":2},{"id":"f4c9a655-ecb5-4143-a73e-9cbb2dd316cc","cell_type":"code","source":"# Remove the huge folders filling /kaggle/working\n!rm -rf /kaggle/working/phi35_merged_hf\n!rm -rf /kaggle/working/phi35_base_hf\n\n# Optional: remove training checkpoints (keep adapter only)\n!rm -rf /kaggle/working/phi35_lora_out/checkpoint-*\n\n# Verify space is freed\n!df -h\n!du -h --max-depth=1 /kaggle/working | sort -hr\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-31T00:57:52.276755Z","iopub.execute_input":"2025-12-31T00:57:52.277505Z","iopub.status.idle":"2025-12-31T00:57:55.503912Z","shell.execute_reply.started":"2025-12-31T00:57:52.277448Z","shell.execute_reply":"2025-12-31T00:57:55.503167Z"}},"outputs":[{"name":"stdout","text":"Filesystem                                                              Size  Used Avail Use% Mounted on\noverlay                                                                 7.9T  6.5T  1.5T  82% /\ntmpfs                                                                    64M     0   64M   0% /dev\nshm                                                                      14G  4.0K   14G   1% /dev/shm\n/dev/sdb1                                                               122G  112G   11G  92% /opt/bin\n/dev/loop1                                                               20G   20M   20G   1% /kaggle/lib\n192.168.3.2:/data/kagglesdsdata/datasets/9161641/14348447/dfbwdah4v0fy   73T   53T   21T  73% /kaggle/input/final-hydraulics-water-ft-dataset\n/dev/mapper/snap                                                        7.9T  6.5T  1.5T  82% /etc/hosts\ntmpfs                                                                    16G     0   16G   0% /proc/acpi\ntmpfs                                                                    16G     0   16G   0% /proc/scsi\ntmpfs                                                                    16G     0   16G   0% /sys/firmware\n20M\t/kaggle/working/phi35_lora_out\n20M\t/kaggle/working\n16K\t/kaggle/working/.virtual_documents\n","output_type":"stream"}],"execution_count":3},{"id":"8379317c-16b3-41c2-af0f-5ce9faca3f20","cell_type":"code","source":"import os, shutil, torch\nfrom huggingface_hub import snapshot_download\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nfrom peft import PeftModel\n\n# ---- Paths ----\nLORA_DIR = \"/kaggle/working/phi35_lora_out\"\nBASE_MODEL = \"microsoft/Phi-3.5-mini-instruct\"\n\nBASE_DIR = \"/kaggle/temp/phi35_base_hf\"\nTMP_DIR  = \"/kaggle/temp/tmp_merge\"\nGGUF_OUT = \"/kaggle/working/phi35_merged.gguf\"\n\n# Keep caches out of /kaggle/working\nos.environ[\"HF_HOME\"] = \"/kaggle/temp/hf\"\nos.environ[\"TRANSFORMERS_CACHE\"] = \"/kaggle/temp/hf/transformers\"\n\n# Clean temp dirs\nshutil.rmtree(BASE_DIR, ignore_errors=True)\nshutil.rmtree(TMP_DIR, ignore_errors=True)\n\n# ---- Download base model (to temp) ----\nsnapshot_download(repo_id=BASE_MODEL, local_dir=BASE_DIR)\n\ntokenizer = AutoTokenizer.from_pretrained(BASE_DIR, use_fast=True)\n\nbase = AutoModelForCausalLM.from_pretrained(\n    BASE_DIR,\n    torch_dtype=torch.float32,\n    device_map=\"cpu\",\n)\n\n# ---- Merge LoRA ----\nmodel = PeftModel.from_pretrained(base, LORA_DIR)\nmerged = model.merge_and_unload()\n\n# ---- Temporary HF save (sharded, non-safetensors) ----\nos.makedirs(TMP_DIR, exist_ok=True)\nmerged.save_pretrained(\n    TMP_DIR,\n    safe_serialization=False,\n    max_shard_size=\"2GB\",\n)\ntokenizer.save_pretrained(TMP_DIR)\n\n# ---- Convert to GGUF ----\nLLAMA_DIR = \"/kaggle/temp/llama.cpp\"\nshutil.rmtree(LLAMA_DIR, ignore_errors=True)\n!git clone -q https://github.com/ggml-org/llama.cpp.git {LLAMA_DIR}\n\n!python {LLAMA_DIR}/convert_hf_to_gguf.py {TMP_DIR} --outfile {GGUF_OUT}\n\nprint(\"✅ GGUF created:\", GGUF_OUT)\n!ls -lah /kaggle/working\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-31T13:36:12.642976Z","iopub.execute_input":"2025-12-31T13:36:12.643565Z","iopub.status.idle":"2025-12-31T13:37:40.599861Z","shell.execute_reply.started":"2025-12-31T13:36:12.643530Z","shell.execute_reply":"2025-12-31T13:37:40.598849Z"}},"outputs":[{"name":"stderr","text":"2025-12-31 13:36:25.424858: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1767188185.577320      55 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1767188185.625307      55 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1767188186.000970      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1767188186.001023      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1767188186.001026      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1767188186.001028      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Fetching 20 files:   0%|          | 0/20 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bd38c8d4ce9e45e7a98af50c391d16cc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"NOTICE.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eaf05fc632c347f0b8f709b4960335c1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"76d55e3e256f4753914f9164cf15ab73"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json:   0%|          | 0.00/306 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"487706901d69433db05f82641bd15912"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"LICENSE: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"458ac19a90d64a90a656d34b2eb05a41"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"CODE_OF_CONDUCT.md:   0%|          | 0.00/453 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0a58060a3fb24f74a45c4ec9ef27704b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"data_summary_card.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2007c8a27de54f22a365259c04a14dac"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":".gitattributes: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0acb403b97f54941a86eba1dc79fb377"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"SECURITY.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e036c7c100b5404baac396c0409d8563"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e7697ae2681f492d9dac70e0bbbb72bc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/195 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eb9b19a86676411d82c8f318432764fb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"configuration_phi3.py: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"decc1f03fce64c058f064fd812fdfa43"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sample_finetune.py: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c305439b92134113aa0a574d52eb6345"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3188294f18af467996ab5836ea76a470"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"modeling_phi3.py: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4f08d959dfd248c89cbdccb8a266a0f7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"78281743829843f2bc2a600a84426d57"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/2.67G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"166499dbf8944d73a28b3a5a63a5e423"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"62bd29a5f81e4a438fbf98156d732eab"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/665 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"856a934bd0c74339a3f12eb2792ebe65"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e57cdf8dba7045a5b8d531b4863d94e9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1bb2fad53c05479cae0700130687f4ae"}},"metadata":{}},{"name":"stderr","text":"`torch_dtype` is deprecated! Use `dtype` instead!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"23ba1ce9222b48278e39f7a99a54483f"}},"metadata":{}},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mHFValidationError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/peft/config.py\u001b[0m in \u001b[0;36m_get_peft_type\u001b[0;34m(cls, model_id, **hf_hub_download_kwargs)\u001b[0m\n\u001b[1;32m    261\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m                 config_file = hf_hub_download(\n\u001b[0m\u001b[1;32m    263\u001b[0m                     \u001b[0mmodel_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    105\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0marg_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"repo_id\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"from_id\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"to_id\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m                 \u001b[0mvalidate_repo_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36mvalidate_repo_id\u001b[0;34m(repo_id)\u001b[0m\n\u001b[1;32m    153\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mrepo_id\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m         raise HFValidationError(\n\u001b[0m\u001b[1;32m    155\u001b[0m             \u001b[0;34m\"Repo id must be in the form 'repo_name' or 'namespace/repo_name':\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mHFValidationError\u001b[0m: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/kaggle/working/phi35_lora_out'. Use `repo_type` argument if needed.","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_55/926322940.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;31m# ---- Merge LoRA ----\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPeftModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLORA_DIR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0mmerged\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge_and_unload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/peft/peft_model.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, model, model_id, adapter_name, is_trainable, config, autocast_adapter_dtype, ephemeral_gpu_offload, low_cpu_mem_usage, key_mapping, **kwargs)\u001b[0m\n\u001b[1;32m    438\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mconfig\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m             config = PEFT_TYPE_TO_CONFIG_MAPPING[\n\u001b[0;32m--> 440\u001b[0;31m                 PeftConfig._get_peft_type(\n\u001b[0m\u001b[1;32m    441\u001b[0m                     \u001b[0mmodel_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m                     \u001b[0msubfolder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"subfolder\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/peft/config.py\u001b[0m in \u001b[0;36m_get_peft_type\u001b[0;34m(cls, model_id, **hf_hub_download_kwargs)\u001b[0m\n\u001b[1;32m    266\u001b[0m                 )\n\u001b[1;32m    267\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 268\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Can't find '{CONFIG_NAME}' at '{model_id}'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    269\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m         \u001b[0mloaded_attributes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_json_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: Can't find 'adapter_config.json' at '/kaggle/working/phi35_lora_out'"],"ename":"ValueError","evalue":"Can't find 'adapter_config.json' at '/kaggle/working/phi35_lora_out'","output_type":"error"}],"execution_count":1},{"id":"389134d4-27ec-4904-b9b5-ca985ffd664c","cell_type":"code","source":"!ls -lah /kaggle/working\n!ls -lah /kaggle/working/phi35_lora_out || true\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-31T13:52:39.106834Z","iopub.execute_input":"2025-12-31T13:52:39.107594Z","iopub.status.idle":"2025-12-31T13:52:40.223758Z","shell.execute_reply.started":"2025-12-31T13:52:39.107564Z","shell.execute_reply":"2025-12-31T13:52:40.222997Z"}},"outputs":[{"name":"stdout","text":"total 12K\ndrwxr-xr-x 3 root root 4.0K Dec 31 13:36 .\ndrwxr-xr-x 6 root root 4.0K Dec 31 13:36 ..\ndrwxr-xr-x 2 root root 4.0K Dec 31 13:36 .virtual_documents\nls: cannot access '/kaggle/working/phi35_lora_out': No such file or directory\n","output_type":"stream"}],"execution_count":2},{"id":"bad458cc-bc59-4a90-999b-a32ddcf01ee2","cell_type":"code","source":"!find /kaggle/input -maxdepth 3 -type f -name \"adapter_model.safetensors\" -o -name \"adapter_config.json\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-31T13:55:48.297242Z","iopub.execute_input":"2025-12-31T13:55:48.297873Z","iopub.status.idle":"2025-12-31T13:55:48.812984Z","shell.execute_reply.started":"2025-12-31T13:55:48.297845Z","shell.execute_reply":"2025-12-31T13:55:48.812016Z"}},"outputs":[],"execution_count":7},{"id":"22a83114-8dfd-4efa-b559-382fbbee1e14","cell_type":"code","source":"LORA_DIR = \"/kaggle/input/<something>/phi35_lora_out\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-31T13:56:21.524014Z","iopub.execute_input":"2025-12-31T13:56:21.524338Z","iopub.status.idle":"2025-12-31T13:56:21.527981Z","shell.execute_reply.started":"2025-12-31T13:56:21.524283Z","shell.execute_reply":"2025-12-31T13:56:21.527220Z"}},"outputs":[],"execution_count":11},{"id":"74be6f08-c362-4e01-909f-61b9ed4ea7f9","cell_type":"code","source":"! find /kaggle -maxdepth 4 -type f -name \"adapter_config.json\"\n! find /kaggle -maxdepth 4 -type f -name \"adapter_model.safetensors\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-31T13:57:47.507011Z","iopub.execute_input":"2025-12-31T13:57:47.507339Z","iopub.status.idle":"2025-12-31T13:57:48.540753Z","shell.execute_reply.started":"2025-12-31T13:57:47.507310Z","shell.execute_reply":"2025-12-31T13:57:48.539757Z"}},"outputs":[],"execution_count":14},{"id":"6f96d4b8-7f7b-4519-adda-b6bffc035d4f","cell_type":"code","source":"! kaggle/input/phi35-lora-adapter\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-31T14:04:22.642260Z","iopub.execute_input":"2025-12-31T14:04:22.643066Z","iopub.status.idle":"2025-12-31T14:04:23.160082Z","shell.execute_reply.started":"2025-12-31T14:04:22.643031Z","shell.execute_reply":"2025-12-31T14:04:23.159184Z"}},"outputs":[{"name":"stdout","text":"/bin/bash: line 1: kaggle/input/phi35-lora-adapter: No such file or directory\n","output_type":"stream"}],"execution_count":18},{"id":"30792dd7-1e74-4518-9445-aaef33eb3eb0","cell_type":"code","source":"! LORA_DIR = \"/kaggle/input/phi35-lora-adapter\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-31T14:06:14.582470Z","iopub.execute_input":"2025-12-31T14:06:14.582764Z","iopub.status.idle":"2025-12-31T14:06:15.096838Z","shell.execute_reply.started":"2025-12-31T14:06:14.582737Z","shell.execute_reply":"2025-12-31T14:06:15.096108Z"}},"outputs":[{"name":"stdout","text":"/bin/bash: line 1: LORA_DIR: command not found\n","output_type":"stream"}],"execution_count":22},{"id":"80f35272-1b72-45f4-803c-476f16585a52","cell_type":"code","source":"import os\nprint(os.listdir(LORA_DIR))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-31T14:06:30.045803Z","iopub.execute_input":"2025-12-31T14:06:30.046544Z","iopub.status.idle":"2025-12-31T14:06:30.053532Z","shell.execute_reply.started":"2025-12-31T14:06:30.046512Z","shell.execute_reply":"2025-12-31T14:06:30.052875Z"}},"outputs":[{"name":"stdout","text":"['adapter_model.safetensors', 'adapter_config.json', 'tokenizer.json']\n","output_type":"stream"}],"execution_count":23},{"id":"714c87b4-16e1-4c5a-8bcb-b2df1d35e7e5","cell_type":"code","source":"import os, json\nprint(\"LORA_DIR =\", LORA_DIR)\nprint(\"Exists:\", os.path.isdir(LORA_DIR))\nprint(\"Has adapter_config:\", os.path.exists(os.path.join(LORA_DIR, \"adapter_config.json\")))\n\nwith open(os.path.join(LORA_DIR, \"adapter_config.json\"), \"r\") as f:\n    cfg = json.load(f)\nprint(\"Base model in adapter_config:\", cfg.get(\"base_model_name_or_path\"))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-31T14:11:09.927503Z","iopub.execute_input":"2025-12-31T14:11:09.927835Z","iopub.status.idle":"2025-12-31T14:11:09.936365Z","shell.execute_reply.started":"2025-12-31T14:11:09.927804Z","shell.execute_reply":"2025-12-31T14:11:09.935589Z"}},"outputs":[{"name":"stdout","text":"LORA_DIR = /kaggle/input/phi35-lora-adapter\nExists: True\nHas adapter_config: True\nBase model in adapter_config: microsoft/Phi-3.5-mini-instruct\n","output_type":"stream"}],"execution_count":25},{"id":"24ee7ecc-effe-4a37-b3a2-d358ad393c1f","cell_type":"code","source":"# Example:\nLORA_DIR = \"/kaggle/input/phi35-lora-adapter\"\n\nimport os\nprint(\"LORA_DIR:\", LORA_DIR)\nprint(os.listdir(LORA_DIR))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-31T14:33:51.595528Z","iopub.execute_input":"2025-12-31T14:33:51.595718Z","iopub.status.idle":"2025-12-31T14:33:51.610223Z","shell.execute_reply.started":"2025-12-31T14:33:51.595696Z","shell.execute_reply":"2025-12-31T14:33:51.609520Z"}},"outputs":[{"name":"stdout","text":"LORA_DIR: /kaggle/input/phi35-lora-adapter\n['adapter_model.safetensors', 'adapter_config.json', 'tokenizer.json']\n","output_type":"stream"}],"execution_count":1},{"id":"fa33dd9b-068f-4b35-bc17-36dc53a42da5","cell_type":"code","source":"import os, shutil, torch\nfrom huggingface_hub import snapshot_download\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nfrom peft import PeftModel, PeftConfig\n\nBASE_MODEL = \"microsoft/Phi-3.5-mini-instruct\"\n\n# Keep big downloads + caches out of /kaggle/working\nos.environ[\"HF_HOME\"] = \"/kaggle/temp/hf\"\nos.environ[\"TRANSFORMERS_CACHE\"] = \"/kaggle/temp/hf/transformers\"\n\nBASE_DIR = \"/kaggle/temp/phi35_base_hf\"\nTMP_DIR  = \"/kaggle/temp/tmp_merge\"\nLLAMA_DIR= \"/kaggle/temp/llama.cpp\"\nGGUF_OUT = \"/kaggle/working/phi35_merged.gguf\"\n\n# Clean temp\nshutil.rmtree(BASE_DIR, ignore_errors=True)\nshutil.rmtree(TMP_DIR, ignore_errors=True)\nshutil.rmtree(LLAMA_DIR, ignore_errors=True)\n\n# Download base to temp\nsnapshot_download(repo_id=BASE_MODEL, local_dir=BASE_DIR)\n\ntokenizer = AutoTokenizer.from_pretrained(BASE_DIR, use_fast=True)\n\n# Load base (CPU to minimize GPU RAM risk)\nbase = AutoModelForCausalLM.from_pretrained(\n    BASE_DIR,\n    torch_dtype=torch.float32,\n    device_map=\"cpu\",\n)\n\n# Load adapter config explicitly (prevents PEFT treating path as HF repo)\npeft_cfg = PeftConfig.from_pretrained(LORA_DIR)\n\n# Merge LoRA\nmodel = PeftModel.from_pretrained(base, LORA_DIR, config=peft_cfg, is_trainable=False)\nmerged = model.merge_and_unload()\n\n# Save merged temporarily (sharded, no safetensors to reduce failure risk)\nos.makedirs(TMP_DIR, exist_ok=True)\nmerged.save_pretrained(\n    TMP_DIR,\n    safe_serialization=False,\n    max_shard_size=\"2GB\",\n)\ntokenizer.save_pretrained(TMP_DIR)\n\n# Convert to GGUF\n!git clone -q https://github.com/ggml-org/llama.cpp.git {LLAMA_DIR}\n!python {LLAMA_DIR}/convert_hf_to_gguf.py {TMP_DIR} --outfile {GGUF_OUT}\n\nprint(\"✅ GGUF created:\", GGUF_OUT)\n!ls -lah /kaggle/working/*.gguf\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-31T14:34:00.066448Z","iopub.execute_input":"2025-12-31T14:34:00.067219Z","iopub.status.idle":"2025-12-31T14:38:15.404300Z","shell.execute_reply.started":"2025-12-31T14:34:00.067188Z","shell.execute_reply":"2025-12-31T14:38:15.403157Z"}},"outputs":[{"name":"stderr","text":"2025-12-31 14:34:13.575624: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1767191653.764551      55 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1767191653.822667      55 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1767191654.279338      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1767191654.279387      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1767191654.279392      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1767191654.279396      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Fetching 20 files:   0%|          | 0/20 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b40c4df9d2a64c03953775b0ac8dac50"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"70aa4762322945579e7f6a3f8f6d3f93"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json:   0%|          | 0.00/306 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"70b757d4a8424ca0a23dc94df5691555"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"configuration_phi3.py: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2af782f6f6864941bc851782ab4047a8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"81daf30b8a8747c9b5d700ace3e72a25"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"SECURITY.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e39380950ba14ff093acd369831ac922"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"CODE_OF_CONDUCT.md:   0%|          | 0.00/453 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"955837bc350843348c16ad2909e40b74"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"LICENSE: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"70b83bee09484e7db369026f6db7c694"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":".gitattributes: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3be6a737b7e7431abe8a45abdfde7d79"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"NOTICE.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"785b76ef855c4ee4b9b9e6ec96ad7ff8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/195 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"65ab57283d3a4271b38754171b44c1f0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"data_summary_card.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1401412be75f4ee197c8b26fc65cd504"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/665 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aacabe5f77c8489fa37a6151e8e74822"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"99128d03d41b4515b2665d140a60fe9e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"modeling_phi3.py: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a941dbf42eec424a8067b0154919d8bc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5312cfac1b38475694d0f5abfdc45d8d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/2.67G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eb830248676942fc86a2e1049fd86385"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0a5a1cd904c643598237f4e293ec5803"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sample_finetune.py: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b256a873bbb94017b645cb2d6e381dd3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c8e10f248f2a4d1cb9b1ad4e2bdae436"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"19cea8446b1a4152bcfddbd43992f8d4"}},"metadata":{}},{"name":"stderr","text":"`torch_dtype` is deprecated! Use `dtype` instead!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eced3d1f5c2e43af809a1cf6004b65c1"}},"metadata":{}},{"name":"stdout","text":"/usr/local/lib/python3.12/dist-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n  warnings.warn(\nINFO:hf-to-gguf:Loading model: tmp_merge\nINFO:hf-to-gguf:Model architecture: Phi3ForCausalLM\nINFO:hf-to-gguf:gguf: loading model weight map from 'pytorch_model.bin.index.json'\nINFO:hf-to-gguf:gguf: indexing model part 'pytorch_model-00001-of-00009.bin'\nINFO:hf-to-gguf:gguf: indexing model part 'pytorch_model-00002-of-00009.bin'\nINFO:hf-to-gguf:gguf: indexing model part 'pytorch_model-00003-of-00009.bin'\nINFO:hf-to-gguf:gguf: indexing model part 'pytorch_model-00004-of-00009.bin'\nINFO:hf-to-gguf:gguf: indexing model part 'pytorch_model-00005-of-00009.bin'\nINFO:hf-to-gguf:gguf: indexing model part 'pytorch_model-00006-of-00009.bin'\nINFO:hf-to-gguf:gguf: indexing model part 'pytorch_model-00007-of-00009.bin'\nINFO:hf-to-gguf:gguf: indexing model part 'pytorch_model-00008-of-00009.bin'\nINFO:hf-to-gguf:gguf: indexing model part 'pytorch_model-00009-of-00009.bin'\nINFO:hf-to-gguf:heuristics unable to detect tensor dtype, defaulting to --outtype f16\nINFO:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only\nINFO:hf-to-gguf:Exporting model...\nINFO:hf-to-gguf:rope_factors_long.weight,  torch.float32 --> F32, shape = {48}\nINFO:hf-to-gguf:rope_factors_short.weight, torch.float32 --> F32, shape = {48}\nINFO:hf-to-gguf:token_embd.weight,         torch.float32 --> F16, shape = {3072, 32064}\nINFO:hf-to-gguf:blk.0.attn_output.weight,  torch.float32 --> F16, shape = {3072, 3072}\nINFO:hf-to-gguf:blk.0.attn_qkv.weight,     torch.float32 --> F16, shape = {3072, 9216}\nINFO:hf-to-gguf:blk.0.ffn_up.weight,       torch.float32 --> F16, shape = {3072, 16384}\nINFO:hf-to-gguf:blk.0.ffn_down.weight,     torch.float32 --> F16, shape = {8192, 3072}\nINFO:hf-to-gguf:blk.0.attn_norm.weight,    torch.float32 --> F32, shape = {3072}\nINFO:hf-to-gguf:blk.0.ffn_norm.weight,     torch.float32 --> F32, shape = {3072}\nINFO:hf-to-gguf:blk.1.attn_output.weight,  torch.float32 --> F16, shape = {3072, 3072}\nINFO:hf-to-gguf:blk.1.attn_qkv.weight,     torch.float32 --> F16, shape = {3072, 9216}\nINFO:hf-to-gguf:blk.1.ffn_up.weight,       torch.float32 --> F16, shape = {3072, 16384}\nINFO:hf-to-gguf:blk.1.ffn_down.weight,     torch.float32 --> F16, shape = {8192, 3072}\nINFO:hf-to-gguf:blk.1.attn_norm.weight,    torch.float32 --> F32, shape = {3072}\nINFO:hf-to-gguf:blk.1.ffn_norm.weight,     torch.float32 --> F32, shape = {3072}\nINFO:hf-to-gguf:blk.2.attn_output.weight,  torch.float32 --> F16, shape = {3072, 3072}\nINFO:hf-to-gguf:blk.2.attn_qkv.weight,     torch.float32 --> F16, shape = {3072, 9216}\nINFO:hf-to-gguf:blk.2.ffn_up.weight,       torch.float32 --> F16, shape = {3072, 16384}\nINFO:hf-to-gguf:blk.2.ffn_down.weight,     torch.float32 --> F16, shape = {8192, 3072}\nINFO:hf-to-gguf:blk.2.attn_norm.weight,    torch.float32 --> F32, shape = {3072}\nINFO:hf-to-gguf:blk.2.ffn_norm.weight,     torch.float32 --> F32, shape = {3072}\nINFO:hf-to-gguf:blk.3.attn_output.weight,  torch.float32 --> F16, shape = {3072, 3072}\nINFO:hf-to-gguf:blk.3.attn_qkv.weight,     torch.float32 --> F16, shape = {3072, 9216}\nINFO:hf-to-gguf:blk.3.ffn_up.weight,       torch.float32 --> F16, shape = {3072, 16384}\nINFO:hf-to-gguf:blk.3.ffn_down.weight,     torch.float32 --> F16, shape = {8192, 3072}\nINFO:hf-to-gguf:blk.3.attn_norm.weight,    torch.float32 --> F32, shape = {3072}\nINFO:hf-to-gguf:blk.3.ffn_norm.weight,     torch.float32 --> F32, shape = {3072}\nINFO:hf-to-gguf:blk.4.attn_output.weight,  torch.float32 --> F16, shape = {3072, 3072}\nINFO:hf-to-gguf:blk.4.attn_qkv.weight,     torch.float32 --> F16, shape = {3072, 9216}\nINFO:hf-to-gguf:blk.4.ffn_up.weight,       torch.float32 --> F16, shape = {3072, 16384}\nINFO:hf-to-gguf:blk.4.ffn_down.weight,     torch.float32 --> F16, shape = {8192, 3072}\nINFO:hf-to-gguf:blk.4.attn_norm.weight,    torch.float32 --> F32, shape = {3072}\nINFO:hf-to-gguf:blk.4.ffn_norm.weight,     torch.float32 --> F32, shape = {3072}\nINFO:hf-to-gguf:blk.5.attn_output.weight,  torch.float32 --> F16, shape = {3072, 3072}\nINFO:hf-to-gguf:blk.5.attn_qkv.weight,     torch.float32 --> F16, shape = {3072, 9216}\nINFO:hf-to-gguf:blk.5.ffn_up.weight,       torch.float32 --> F16, shape = {3072, 16384}\nINFO:hf-to-gguf:blk.5.ffn_down.weight,     torch.float32 --> F16, shape = {8192, 3072}\nINFO:hf-to-gguf:blk.5.attn_norm.weight,    torch.float32 --> F32, shape = {3072}\nINFO:hf-to-gguf:blk.5.ffn_norm.weight,     torch.float32 --> F32, shape = {3072}\nINFO:hf-to-gguf:blk.6.attn_output.weight,  torch.float32 --> F16, shape = {3072, 3072}\nINFO:hf-to-gguf:blk.6.attn_qkv.weight,     torch.float32 --> F16, shape = {3072, 9216}\nINFO:hf-to-gguf:blk.6.ffn_up.weight,       torch.float32 --> F16, shape = {3072, 16384}\nINFO:hf-to-gguf:blk.6.ffn_down.weight,     torch.float32 --> F16, shape = {8192, 3072}\nINFO:hf-to-gguf:blk.6.attn_norm.weight,    torch.float32 --> F32, shape = {3072}\nINFO:hf-to-gguf:blk.6.ffn_norm.weight,     torch.float32 --> F32, shape = {3072}\nINFO:hf-to-gguf:blk.7.attn_output.weight,  torch.float32 --> F16, shape = {3072, 3072}\nINFO:hf-to-gguf:blk.7.attn_qkv.weight,     torch.float32 --> F16, shape = {3072, 9216}\nINFO:hf-to-gguf:blk.7.ffn_up.weight,       torch.float32 --> F16, shape = {3072, 16384}\nINFO:hf-to-gguf:blk.7.ffn_down.weight,     torch.float32 --> F16, shape = {8192, 3072}\nINFO:hf-to-gguf:blk.7.attn_norm.weight,    torch.float32 --> F32, shape = {3072}\nINFO:hf-to-gguf:blk.7.ffn_norm.weight,     torch.float32 --> F32, shape = {3072}\nINFO:hf-to-gguf:blk.8.attn_output.weight,  torch.float32 --> F16, shape = {3072, 3072}\nINFO:hf-to-gguf:blk.8.attn_qkv.weight,     torch.float32 --> F16, shape = {3072, 9216}\nINFO:hf-to-gguf:blk.8.ffn_up.weight,       torch.float32 --> F16, shape = {3072, 16384}\nINFO:hf-to-gguf:blk.8.ffn_down.weight,     torch.float32 --> F16, shape = {8192, 3072}\nINFO:hf-to-gguf:blk.8.attn_norm.weight,    torch.float32 --> F32, shape = {3072}\nINFO:hf-to-gguf:blk.8.ffn_norm.weight,     torch.float32 --> F32, shape = {3072}\nINFO:hf-to-gguf:blk.9.attn_output.weight,  torch.float32 --> F16, shape = {3072, 3072}\nINFO:hf-to-gguf:blk.9.attn_qkv.weight,     torch.float32 --> F16, shape = {3072, 9216}\nINFO:hf-to-gguf:blk.9.ffn_up.weight,       torch.float32 --> F16, shape = {3072, 16384}\nINFO:hf-to-gguf:blk.9.ffn_down.weight,     torch.float32 --> F16, shape = {8192, 3072}\nINFO:hf-to-gguf:blk.9.attn_norm.weight,    torch.float32 --> F32, shape = {3072}\nINFO:hf-to-gguf:blk.9.ffn_norm.weight,     torch.float32 --> F32, shape = {3072}\nINFO:hf-to-gguf:blk.10.attn_output.weight, torch.float32 --> F16, shape = {3072, 3072}\nINFO:hf-to-gguf:blk.10.attn_qkv.weight,    torch.float32 --> F16, shape = {3072, 9216}\nINFO:hf-to-gguf:blk.10.ffn_up.weight,      torch.float32 --> F16, shape = {3072, 16384}\nINFO:hf-to-gguf:blk.10.ffn_down.weight,    torch.float32 --> F16, shape = {8192, 3072}\nINFO:hf-to-gguf:blk.10.attn_norm.weight,   torch.float32 --> F32, shape = {3072}\nINFO:hf-to-gguf:blk.10.ffn_norm.weight,    torch.float32 --> F32, shape = {3072}\nINFO:hf-to-gguf:blk.11.attn_output.weight, torch.float32 --> F16, shape = {3072, 3072}\nINFO:hf-to-gguf:blk.11.attn_qkv.weight,    torch.float32 --> F16, shape = {3072, 9216}\nINFO:hf-to-gguf:blk.11.ffn_up.weight,      torch.float32 --> F16, shape = {3072, 16384}\nINFO:hf-to-gguf:blk.11.ffn_down.weight,    torch.float32 --> F16, shape = {8192, 3072}\nINFO:hf-to-gguf:blk.11.attn_norm.weight,   torch.float32 --> F32, shape = {3072}\nINFO:hf-to-gguf:blk.11.ffn_norm.weight,    torch.float32 --> F32, shape = {3072}\nINFO:hf-to-gguf:blk.12.attn_output.weight, torch.float32 --> F16, shape = {3072, 3072}\nINFO:hf-to-gguf:blk.12.attn_qkv.weight,    torch.float32 --> F16, shape = {3072, 9216}\nINFO:hf-to-gguf:blk.12.ffn_up.weight,      torch.float32 --> F16, shape = {3072, 16384}\nINFO:hf-to-gguf:blk.12.ffn_down.weight,    torch.float32 --> F16, shape = {8192, 3072}\nINFO:hf-to-gguf:blk.12.attn_norm.weight,   torch.float32 --> F32, shape = {3072}\nINFO:hf-to-gguf:blk.12.ffn_norm.weight,    torch.float32 --> F32, shape = {3072}\nINFO:hf-to-gguf:blk.13.attn_output.weight, torch.float32 --> F16, shape = {3072, 3072}\nINFO:hf-to-gguf:blk.13.attn_qkv.weight,    torch.float32 --> F16, shape = {3072, 9216}\nINFO:hf-to-gguf:blk.13.ffn_up.weight,      torch.float32 --> F16, shape = {3072, 16384}\nINFO:hf-to-gguf:blk.13.ffn_down.weight,    torch.float32 --> F16, shape = {8192, 3072}\nINFO:hf-to-gguf:blk.13.attn_norm.weight,   torch.float32 --> F32, shape = {3072}\nINFO:hf-to-gguf:blk.13.ffn_norm.weight,    torch.float32 --> F32, shape = {3072}\nINFO:hf-to-gguf:blk.14.attn_output.weight, torch.float32 --> F16, shape = {3072, 3072}\nINFO:hf-to-gguf:blk.14.attn_qkv.weight,    torch.float32 --> F16, shape = {3072, 9216}\nINFO:hf-to-gguf:blk.14.ffn_up.weight,      torch.float32 --> F16, shape = {3072, 16384}\nINFO:hf-to-gguf:blk.14.ffn_down.weight,    torch.float32 --> F16, shape = {8192, 3072}\nINFO:hf-to-gguf:blk.14.attn_norm.weight,   torch.float32 --> F32, shape = {3072}\nINFO:hf-to-gguf:blk.14.ffn_norm.weight,    torch.float32 --> F32, shape = {3072}\nINFO:hf-to-gguf:blk.15.attn_output.weight, torch.float32 --> F16, shape = {3072, 3072}\nINFO:hf-to-gguf:blk.15.attn_qkv.weight,    torch.float32 --> F16, shape = {3072, 9216}\nINFO:hf-to-gguf:blk.15.ffn_up.weight,      torch.float32 --> F16, shape = {3072, 16384}\nINFO:hf-to-gguf:blk.15.ffn_down.weight,    torch.float32 --> F16, shape = {8192, 3072}\nINFO:hf-to-gguf:blk.15.attn_norm.weight,   torch.float32 --> F32, shape = {3072}\nINFO:hf-to-gguf:blk.15.ffn_norm.weight,    torch.float32 --> F32, shape = {3072}\nINFO:hf-to-gguf:blk.16.attn_output.weight, torch.float32 --> F16, shape = {3072, 3072}\nINFO:hf-to-gguf:blk.16.attn_qkv.weight,    torch.float32 --> F16, shape = {3072, 9216}\nINFO:hf-to-gguf:blk.16.ffn_up.weight,      torch.float32 --> F16, shape = {3072, 16384}\nINFO:hf-to-gguf:blk.16.ffn_down.weight,    torch.float32 --> F16, shape = {8192, 3072}\nINFO:hf-to-gguf:blk.16.attn_norm.weight,   torch.float32 --> F32, shape = {3072}\nINFO:hf-to-gguf:blk.16.ffn_norm.weight,    torch.float32 --> F32, shape = {3072}\nINFO:hf-to-gguf:blk.17.attn_output.weight, torch.float32 --> F16, shape = {3072, 3072}\nINFO:hf-to-gguf:blk.17.attn_qkv.weight,    torch.float32 --> F16, shape = {3072, 9216}\nINFO:hf-to-gguf:blk.17.ffn_up.weight,      torch.float32 --> F16, shape = {3072, 16384}\nINFO:hf-to-gguf:blk.17.ffn_down.weight,    torch.float32 --> F16, shape = {8192, 3072}\nINFO:hf-to-gguf:blk.17.attn_norm.weight,   torch.float32 --> F32, shape = {3072}\nINFO:hf-to-gguf:blk.17.ffn_norm.weight,    torch.float32 --> F32, shape = {3072}\nINFO:hf-to-gguf:blk.18.attn_output.weight, torch.float32 --> F16, shape = {3072, 3072}\nINFO:hf-to-gguf:blk.18.attn_qkv.weight,    torch.float32 --> F16, shape = {3072, 9216}\nINFO:hf-to-gguf:blk.18.ffn_up.weight,      torch.float32 --> F16, shape = {3072, 16384}\nINFO:hf-to-gguf:blk.18.ffn_down.weight,    torch.float32 --> F16, shape = {8192, 3072}\nINFO:hf-to-gguf:blk.18.attn_norm.weight,   torch.float32 --> F32, shape = {3072}\nINFO:hf-to-gguf:blk.18.ffn_norm.weight,    torch.float32 --> F32, shape = {3072}\nINFO:hf-to-gguf:blk.19.attn_output.weight, torch.float32 --> F16, shape = {3072, 3072}\nINFO:hf-to-gguf:blk.19.attn_qkv.weight,    torch.float32 --> F16, shape = {3072, 9216}\nINFO:hf-to-gguf:blk.19.ffn_up.weight,      torch.float32 --> F16, shape = {3072, 16384}\nINFO:hf-to-gguf:blk.19.ffn_down.weight,    torch.float32 --> F16, shape = {8192, 3072}\nINFO:hf-to-gguf:blk.19.attn_norm.weight,   torch.float32 --> F32, shape = {3072}\nINFO:hf-to-gguf:blk.19.ffn_norm.weight,    torch.float32 --> F32, shape = {3072}\nINFO:hf-to-gguf:blk.20.attn_output.weight, torch.float32 --> F16, shape = {3072, 3072}\nINFO:hf-to-gguf:blk.20.attn_qkv.weight,    torch.float32 --> F16, shape = {3072, 9216}\nINFO:hf-to-gguf:blk.20.ffn_up.weight,      torch.float32 --> F16, shape = {3072, 16384}\nINFO:hf-to-gguf:blk.20.ffn_down.weight,    torch.float32 --> F16, shape = {8192, 3072}\nINFO:hf-to-gguf:blk.20.attn_norm.weight,   torch.float32 --> F32, shape = {3072}\nINFO:hf-to-gguf:blk.20.ffn_norm.weight,    torch.float32 --> F32, shape = {3072}\nINFO:hf-to-gguf:blk.21.attn_output.weight, torch.float32 --> F16, shape = {3072, 3072}\nINFO:hf-to-gguf:blk.21.attn_qkv.weight,    torch.float32 --> F16, shape = {3072, 9216}\nINFO:hf-to-gguf:blk.21.ffn_up.weight,      torch.float32 --> F16, shape = {3072, 16384}\nINFO:hf-to-gguf:blk.21.ffn_down.weight,    torch.float32 --> F16, shape = {8192, 3072}\nINFO:hf-to-gguf:blk.21.attn_norm.weight,   torch.float32 --> F32, shape = {3072}\nINFO:hf-to-gguf:blk.21.ffn_norm.weight,    torch.float32 --> F32, shape = {3072}\nINFO:hf-to-gguf:blk.22.attn_output.weight, torch.float32 --> F16, shape = {3072, 3072}\nINFO:hf-to-gguf:blk.22.attn_qkv.weight,    torch.float32 --> F16, shape = {3072, 9216}\nINFO:hf-to-gguf:blk.22.ffn_up.weight,      torch.float32 --> F16, shape = {3072, 16384}\nINFO:hf-to-gguf:blk.22.ffn_down.weight,    torch.float32 --> F16, shape = {8192, 3072}\nINFO:hf-to-gguf:blk.22.attn_norm.weight,   torch.float32 --> F32, shape = {3072}\nINFO:hf-to-gguf:blk.22.ffn_norm.weight,    torch.float32 --> F32, shape = {3072}\nINFO:hf-to-gguf:blk.23.attn_output.weight, torch.float32 --> F16, shape = {3072, 3072}\nINFO:hf-to-gguf:blk.23.attn_qkv.weight,    torch.float32 --> F16, shape = {3072, 9216}\nINFO:hf-to-gguf:blk.23.ffn_up.weight,      torch.float32 --> F16, shape = {3072, 16384}\nINFO:hf-to-gguf:blk.23.ffn_down.weight,    torch.float32 --> F16, shape = {8192, 3072}\nINFO:hf-to-gguf:blk.23.attn_norm.weight,   torch.float32 --> F32, shape = {3072}\nINFO:hf-to-gguf:blk.23.ffn_norm.weight,    torch.float32 --> F32, shape = {3072}\nINFO:hf-to-gguf:blk.24.attn_output.weight, torch.float32 --> F16, shape = {3072, 3072}\nINFO:hf-to-gguf:blk.24.attn_qkv.weight,    torch.float32 --> F16, shape = {3072, 9216}\nINFO:hf-to-gguf:blk.24.ffn_up.weight,      torch.float32 --> F16, shape = {3072, 16384}\nINFO:hf-to-gguf:blk.24.ffn_down.weight,    torch.float32 --> F16, shape = {8192, 3072}\nINFO:hf-to-gguf:blk.24.attn_norm.weight,   torch.float32 --> F32, shape = {3072}\nINFO:hf-to-gguf:blk.24.ffn_norm.weight,    torch.float32 --> F32, shape = {3072}\nINFO:hf-to-gguf:blk.25.attn_output.weight, torch.float32 --> F16, shape = {3072, 3072}\nINFO:hf-to-gguf:blk.25.attn_qkv.weight,    torch.float32 --> F16, shape = {3072, 9216}\nINFO:hf-to-gguf:blk.25.ffn_up.weight,      torch.float32 --> F16, shape = {3072, 16384}\nINFO:hf-to-gguf:blk.25.ffn_down.weight,    torch.float32 --> F16, shape = {8192, 3072}\nINFO:hf-to-gguf:blk.25.attn_norm.weight,   torch.float32 --> F32, shape = {3072}\nINFO:hf-to-gguf:blk.25.ffn_norm.weight,    torch.float32 --> F32, shape = {3072}\nINFO:hf-to-gguf:blk.26.attn_output.weight, torch.float32 --> F16, shape = {3072, 3072}\nINFO:hf-to-gguf:blk.26.attn_qkv.weight,    torch.float32 --> F16, shape = {3072, 9216}\nINFO:hf-to-gguf:blk.26.ffn_up.weight,      torch.float32 --> F16, shape = {3072, 16384}\nINFO:hf-to-gguf:blk.26.ffn_down.weight,    torch.float32 --> F16, shape = {8192, 3072}\nINFO:hf-to-gguf:blk.26.attn_norm.weight,   torch.float32 --> F32, shape = {3072}\nINFO:hf-to-gguf:blk.26.ffn_norm.weight,    torch.float32 --> F32, shape = {3072}\nINFO:hf-to-gguf:blk.27.attn_output.weight, torch.float32 --> F16, shape = {3072, 3072}\nINFO:hf-to-gguf:blk.27.attn_qkv.weight,    torch.float32 --> F16, shape = {3072, 9216}\nINFO:hf-to-gguf:blk.27.ffn_up.weight,      torch.float32 --> F16, shape = {3072, 16384}\nINFO:hf-to-gguf:blk.27.ffn_down.weight,    torch.float32 --> F16, shape = {8192, 3072}\nINFO:hf-to-gguf:blk.27.attn_norm.weight,   torch.float32 --> F32, shape = {3072}\nINFO:hf-to-gguf:blk.27.ffn_norm.weight,    torch.float32 --> F32, shape = {3072}\nINFO:hf-to-gguf:blk.28.attn_output.weight, torch.float32 --> F16, shape = {3072, 3072}\nINFO:hf-to-gguf:blk.28.attn_qkv.weight,    torch.float32 --> F16, shape = {3072, 9216}\nINFO:hf-to-gguf:blk.28.ffn_up.weight,      torch.float32 --> F16, shape = {3072, 16384}\nINFO:hf-to-gguf:blk.28.ffn_down.weight,    torch.float32 --> F16, shape = {8192, 3072}\nINFO:hf-to-gguf:blk.28.attn_norm.weight,   torch.float32 --> F32, shape = {3072}\nINFO:hf-to-gguf:blk.28.ffn_norm.weight,    torch.float32 --> F32, shape = {3072}\nINFO:hf-to-gguf:blk.29.attn_output.weight, torch.float32 --> F16, shape = {3072, 3072}\nINFO:hf-to-gguf:blk.29.attn_qkv.weight,    torch.float32 --> F16, shape = {3072, 9216}\nINFO:hf-to-gguf:blk.29.ffn_up.weight,      torch.float32 --> F16, shape = {3072, 16384}\nINFO:hf-to-gguf:blk.29.ffn_down.weight,    torch.float32 --> F16, shape = {8192, 3072}\nINFO:hf-to-gguf:blk.29.attn_norm.weight,   torch.float32 --> F32, shape = {3072}\nINFO:hf-to-gguf:blk.29.ffn_norm.weight,    torch.float32 --> F32, shape = {3072}\nINFO:hf-to-gguf:blk.30.attn_output.weight, torch.float32 --> F16, shape = {3072, 3072}\nINFO:hf-to-gguf:blk.30.attn_qkv.weight,    torch.float32 --> F16, shape = {3072, 9216}\nINFO:hf-to-gguf:blk.30.ffn_up.weight,      torch.float32 --> F16, shape = {3072, 16384}\nINFO:hf-to-gguf:blk.30.ffn_down.weight,    torch.float32 --> F16, shape = {8192, 3072}\nINFO:hf-to-gguf:blk.30.attn_norm.weight,   torch.float32 --> F32, shape = {3072}\nINFO:hf-to-gguf:blk.30.ffn_norm.weight,    torch.float32 --> F32, shape = {3072}\nINFO:hf-to-gguf:blk.31.attn_output.weight, torch.float32 --> F16, shape = {3072, 3072}\nINFO:hf-to-gguf:blk.31.attn_qkv.weight,    torch.float32 --> F16, shape = {3072, 9216}\nINFO:hf-to-gguf:blk.31.ffn_up.weight,      torch.float32 --> F16, shape = {3072, 16384}\nINFO:hf-to-gguf:blk.31.ffn_down.weight,    torch.float32 --> F16, shape = {8192, 3072}\nINFO:hf-to-gguf:blk.31.attn_norm.weight,   torch.float32 --> F32, shape = {3072}\nINFO:hf-to-gguf:blk.31.ffn_norm.weight,    torch.float32 --> F32, shape = {3072}\nINFO:hf-to-gguf:output_norm.weight,        torch.float32 --> F32, shape = {3072}\nINFO:hf-to-gguf:output.weight,             torch.float32 --> F16, shape = {3072, 32064}\nINFO:hf-to-gguf:Set meta model\nINFO:hf-to-gguf:Set model parameters\nINFO:hf-to-gguf:Set model quantization version\nINFO:hf-to-gguf:Set model tokenizer\nINFO:gguf.vocab:Setting special token type bos to 1\nINFO:gguf.vocab:Setting special token type eos to 32000\nINFO:gguf.vocab:Setting special token type unk to 0\nINFO:gguf.vocab:Setting special token type pad to 32000\nINFO:gguf.vocab:Setting add_bos_token to False\nINFO:gguf.vocab:Setting add_eos_token to False\nINFO:gguf.vocab:Setting chat_template to {% for message in messages %}{% if message['role'] == 'system' and message['content'] %}{{'<|system|>\n' + message['content'] + '<|end|>\n'}}{% elif message['role'] == 'user' %}{{'<|user|>\n' + message['content'] + '<|end|>\n'}}{% elif message['role'] == 'assistant' %}{{'<|assistant|>\n' + message['content'] + '<|end|>\n'}}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ '<|assistant|>\n' }}{% else %}{{ eos_token }}{% endif %}\nINFO:gguf.gguf_writer:Writing the following files:\nINFO:gguf.gguf_writer:/kaggle/working/phi35_merged.gguf: n_tensors = 197, total_size = 7.6G\nWriting: 100%|███████████████████████████| 7.64G/7.64G [01:10<00:00, 109Mbyte/s]\nINFO:hf-to-gguf:Model successfully exported to /kaggle/working/phi35_merged.gguf\n✅ GGUF created: /kaggle/working/phi35_merged.gguf\n-rw-r--r-- 1 root root 7.2G Dec 31 14:38 /kaggle/working/phi35_merged.gguf\n","output_type":"stream"}],"execution_count":2},{"id":"5ce064fa-d601-4a81-a592-8456affc8fb3","cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}