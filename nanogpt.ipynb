{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":4558658,"sourceType":"datasetVersion","datasetId":2660706},{"sourceId":10753606,"sourceType":"datasetVersion","datasetId":6669576}],"dockerImageVersionId":30887,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/razaali10/nanogpt-fine-tune-shakespare?scriptVersionId=223474655\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-02-14T21:16:29.405603Z","iopub.execute_input":"2025-02-14T21:16:29.405827Z","iopub.status.idle":"2025-02-14T21:16:32.304912Z","shell.execute_reply.started":"2025-02-14T21:16:29.405802Z","shell.execute_reply":"2025-02-14T21:16:32.303922Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!git clone https://github.com/karpathy/nanoGPT.git","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-19T23:47:29.879647Z","iopub.execute_input":"2025-02-19T23:47:29.879951Z","iopub.status.idle":"2025-02-19T23:47:30.675363Z","shell.execute_reply.started":"2025-02-19T23:47:29.879921Z","shell.execute_reply":"2025-02-19T23:47:30.6743Z"}},"outputs":[{"name":"stdout","text":"Cloning into 'nanoGPT'...\nremote: Enumerating objects: 686, done.\u001b[K\nremote: Total 686 (delta 0), reused 0 (delta 0), pack-reused 686 (from 1)\u001b[K\nReceiving objects: 100% (686/686), 954.03 KiB | 19.88 MiB/s, done.\nResolving deltas: 100% (387/387), done.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"%cd /kaggle/working/nanoGPT/data/shakespeare_char","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-19T23:48:10.919638Z","iopub.execute_input":"2025-02-19T23:48:10.919986Z","iopub.status.idle":"2025-02-19T23:48:10.926126Z","shell.execute_reply.started":"2025-02-19T23:48:10.919955Z","shell.execute_reply":"2025-02-19T23:48:10.925518Z"}},"outputs":[{"name":"stdout","text":"/kaggle/working/nanoGPT/data/shakespeare_char\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"!python prepare.py","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-19T23:48:23.719138Z","iopub.execute_input":"2025-02-19T23:48:23.719433Z","iopub.status.idle":"2025-02-19T23:48:24.345341Z","shell.execute_reply.started":"2025-02-19T23:48:23.719409Z","shell.execute_reply":"2025-02-19T23:48:24.344341Z"}},"outputs":[{"name":"stdout","text":"length of dataset in characters: 1,115,394\nall the unique characters: \n !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\nvocab size: 65\ntrain has 1,003,854 tokens\nval has 111,540 tokens\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"%cd /kaggle/working/nanoGPT","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-19T23:49:20.539217Z","iopub.execute_input":"2025-02-19T23:49:20.539623Z","iopub.status.idle":"2025-02-19T23:49:20.545684Z","shell.execute_reply.started":"2025-02-19T23:49:20.539593Z","shell.execute_reply":"2025-02-19T23:49:20.545012Z"}},"outputs":[{"name":"stdout","text":"/kaggle/working/nanoGPT\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"!python train.py config/train_shakespeare_char.py --device=cpu --compile=False --eval_interval=1 --log_interval=1 --block_size=64 --batch_size=12 --n_layer=4 --n_head=4 --n_embd=128 --max_iters=2000 --lr_decay_iters=2000 --dropout=0.0","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-19T23:49:24.636226Z","iopub.execute_input":"2025-02-19T23:49:24.636534Z","iopub.status.idle":"2025-02-20T02:25:51.004153Z","shell.execute_reply.started":"2025-02-19T23:49:24.636509Z","shell.execute_reply":"2025-02-20T02:25:51.003093Z"}},"outputs":[{"name":"stdout","text":"Overriding config with config/train_shakespeare_char.py:\n# train a miniature character-level shakespeare model\n# good for debugging and playing on macbooks and such\n\nout_dir = 'out-shakespeare-char'\neval_interval = 250 # keep frequent because we'll overfit\neval_iters = 200\nlog_interval = 10 # don't print too too often\n\n# we expect to overfit on this small dataset, so only save when val improves\nalways_save_checkpoint = False\n\nwandb_log = False # override via command line if you like\nwandb_project = 'shakespeare-char'\nwandb_run_name = 'mini-gpt'\n\ndataset = 'shakespeare_char'\ngradient_accumulation_steps = 1\nbatch_size = 64\nblock_size = 256 # context of up to 256 previous characters\n\n# baby GPT model :)\nn_layer = 6\nn_head = 6\nn_embd = 384\ndropout = 0.2\n\nlearning_rate = 1e-3 # with baby networks can afford to go a bit higher\nmax_iters = 5000\nlr_decay_iters = 5000 # make equal to max_iters usually\nmin_lr = 1e-4 # learning_rate / 10 usually\nbeta2 = 0.99 # make a bit bigger because number of tokens per iter is small\n\nwarmup_iters = 100 # not super necessary potentially\n\n# on macbook also add\n# device = 'cpu'  # run on cpu only\n# compile = False # do not torch compile the model\n\nOverriding: device = cpu\nOverriding: compile = False\nOverriding: eval_interval = 1\nOverriding: log_interval = 1\nOverriding: block_size = 64\nOverriding: batch_size = 12\nOverriding: n_layer = 4\nOverriding: n_head = 4\nOverriding: n_embd = 128\nOverriding: max_iters = 2000\nOverriding: lr_decay_iters = 2000\nOverriding: dropout = 0.0\ntokens per iteration will be: 768\nfound vocab_size = 65 (inside data/shakespeare_char/meta.pkl)\nInitializing a new model from scratch\nnumber of parameters: 0.80M\n/kaggle/working/nanoGPT/train.py:196: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\nnum decayed parameter tensors: 18, with 802,944 parameters\nnum non-decayed parameter tensors: 9, with 1,152 parameters\nusing fused AdamW: False\nstep 0: train loss 4.1697, val loss 4.1662\niter 0: loss 4.1828, time 5295.06ms, mfu -100.00%\nstep 1: train loss 4.1558, val loss 4.1510\nsaving checkpoint to out-shakespeare-char\niter 1: loss 4.1455, time 5234.55ms, mfu -100.00%\nstep 2: train loss 4.1257, val loss 4.1237\nsaving checkpoint to out-shakespeare-char\niter 2: loss 4.1335, time 4550.67ms, mfu -100.00%\nstep 3: train loss 4.0837, val loss 4.0799\nsaving checkpoint to out-shakespeare-char\niter 3: loss 4.0706, time 4679.75ms, mfu -100.00%\nstep 4: train loss 4.0278, val loss 4.0261\nsaving checkpoint to out-shakespeare-char\niter 4: loss 4.0370, time 4657.75ms, mfu -100.00%\nstep 5: train loss 3.9669, val loss 3.9661\nsaving checkpoint to out-shakespeare-char\niter 5: loss 3.9445, time 4611.62ms, mfu 0.00%\nstep 6: train loss 3.9051, val loss 3.9104\nsaving checkpoint to out-shakespeare-char\niter 6: loss 3.9046, time 4703.38ms, mfu 0.00%\nstep 7: train loss 3.8496, val loss 3.8556\nsaving checkpoint to out-shakespeare-char\niter 7: loss 3.8214, time 5342.72ms, mfu 0.00%\nstep 8: train loss 3.7998, val loss 3.8098\nsaving checkpoint to out-shakespeare-char\niter 8: loss 3.7935, time 4573.34ms, mfu 0.00%\nstep 9: train loss 3.7670, val loss 3.7746\nsaving checkpoint to out-shakespeare-char\niter 9: loss 3.7732, time 4659.12ms, mfu 0.00%\nstep 10: train loss 3.7356, val loss 3.7453\nsaving checkpoint to out-shakespeare-char\niter 10: loss 3.7551, time 4681.27ms, mfu 0.00%\nstep 11: train loss 3.7129, val loss 3.7232\nsaving checkpoint to out-shakespeare-char\niter 11: loss 3.7169, time 4635.02ms, mfu 0.00%\nstep 12: train loss 3.6905, val loss 3.7006\nsaving checkpoint to out-shakespeare-char\niter 12: loss 3.6445, time 4629.21ms, mfu 0.00%\nstep 13: train loss 3.6724, val loss 3.6862\nsaving checkpoint to out-shakespeare-char\niter 13: loss 3.6745, time 4634.40ms, mfu 0.00%\nstep 14: train loss 3.6578, val loss 3.6692\nsaving checkpoint to out-shakespeare-char\niter 14: loss 3.6128, time 5410.57ms, mfu 0.00%\nstep 15: train loss 3.6440, val loss 3.6538\nsaving checkpoint to out-shakespeare-char\niter 15: loss 3.6557, time 4635.33ms, mfu 0.00%\nstep 16: train loss 3.6276, val loss 3.6402\nsaving checkpoint to out-shakespeare-char\niter 16: loss 3.6441, time 4640.09ms, mfu 0.00%\nstep 17: train loss 3.6085, val loss 3.6233\nsaving checkpoint to out-shakespeare-char\niter 17: loss 3.6133, time 4603.92ms, mfu 0.00%\nstep 18: train loss 3.5964, val loss 3.6061\nsaving checkpoint to out-shakespeare-char\niter 18: loss 3.5905, time 4647.47ms, mfu 0.00%\nstep 19: train loss 3.5764, val loss 3.5916\nsaving checkpoint to out-shakespeare-char\niter 19: loss 3.5901, time 4722.44ms, mfu 0.00%\nstep 20: train loss 3.5492, val loss 3.5671\nsaving checkpoint to out-shakespeare-char\niter 20: loss 3.5567, time 4736.60ms, mfu 0.00%\nstep 21: train loss 3.5263, val loss 3.5405\nsaving checkpoint to out-shakespeare-char\niter 21: loss 3.5237, time 5216.45ms, mfu 0.00%\nstep 22: train loss 3.5000, val loss 3.5106\nsaving checkpoint to out-shakespeare-char\niter 22: loss 3.4619, time 4586.60ms, mfu 0.00%\nstep 23: train loss 3.4590, val loss 3.4681\nsaving checkpoint to out-shakespeare-char\niter 23: loss 3.4039, time 4647.67ms, mfu 0.00%\nstep 24: train loss 3.4250, val loss 3.4430\nsaving checkpoint to out-shakespeare-char\niter 24: loss 3.4260, time 4724.64ms, mfu 0.00%\nstep 25: train loss 3.4137, val loss 3.4198\nsaving checkpoint to out-shakespeare-char\niter 25: loss 3.3636, time 4683.86ms, mfu 0.00%\nstep 26: train loss 3.3890, val loss 3.3955\nsaving checkpoint to out-shakespeare-char\niter 26: loss 3.3847, time 4673.29ms, mfu 0.00%\nstep 27: train loss 3.3506, val loss 3.3646\nsaving checkpoint to out-shakespeare-char\niter 27: loss 3.2691, time 4717.68ms, mfu 0.00%\nstep 28: train loss 3.3231, val loss 3.3363\nsaving checkpoint to out-shakespeare-char\niter 28: loss 3.3378, time 5142.38ms, mfu 0.00%\nstep 29: train loss 3.2990, val loss 3.3175\nsaving checkpoint to out-shakespeare-char\niter 29: loss 3.2871, time 4693.11ms, mfu 0.00%\nstep 30: train loss 3.2785, val loss 3.2916\nsaving checkpoint to out-shakespeare-char\niter 30: loss 3.2869, time 4596.02ms, mfu 0.00%\nstep 31: train loss 3.2559, val loss 3.2646\nsaving checkpoint to out-shakespeare-char\niter 31: loss 3.2759, time 4713.46ms, mfu 0.00%\nstep 32: train loss 3.2262, val loss 3.2382\nsaving checkpoint to out-shakespeare-char\niter 32: loss 3.2085, time 4602.82ms, mfu 0.00%\nstep 33: train loss 3.2099, val loss 3.2218\nsaving checkpoint to out-shakespeare-char\niter 33: loss 3.2599, time 4572.13ms, mfu 0.00%\nstep 34: train loss 3.1854, val loss 3.1967\nsaving checkpoint to out-shakespeare-char\niter 34: loss 3.2117, time 4989.31ms, mfu 0.00%\nstep 35: train loss 3.1619, val loss 3.1751\nsaving checkpoint to out-shakespeare-char\niter 35: loss 3.2230, time 4952.83ms, mfu 0.00%\nstep 36: train loss 3.1503, val loss 3.1638\nsaving checkpoint to out-shakespeare-char\niter 36: loss 3.1499, time 4635.52ms, mfu 0.00%\nstep 37: train loss 3.1296, val loss 3.1370\nsaving checkpoint to out-shakespeare-char\niter 37: loss 3.1359, time 4695.89ms, mfu 0.00%\nstep 38: train loss 3.1055, val loss 3.1127\nsaving checkpoint to out-shakespeare-char\niter 38: loss 3.0841, time 4603.70ms, mfu 0.00%\nstep 39: train loss 3.0839, val loss 3.0882\nsaving checkpoint to out-shakespeare-char\niter 39: loss 3.0125, time 4754.11ms, mfu 0.00%\nstep 40: train loss 3.0662, val loss 3.0693\nsaving checkpoint to out-shakespeare-char\niter 40: loss 3.0222, time 4584.78ms, mfu 0.00%\nstep 41: train loss 3.0518, val loss 3.0530\nsaving checkpoint to out-shakespeare-char\niter 41: loss 3.0251, time 5379.80ms, mfu 0.00%\nstep 42: train loss 3.0285, val loss 3.0328\nsaving checkpoint to out-shakespeare-char\niter 42: loss 2.9626, time 4660.68ms, mfu 0.00%\nstep 43: train loss 3.0091, val loss 3.0202\nsaving checkpoint to out-shakespeare-char\niter 43: loss 3.0333, time 4686.03ms, mfu 0.00%\nstep 44: train loss 2.9967, val loss 3.0030\nsaving checkpoint to out-shakespeare-char\niter 44: loss 2.9897, time 4761.07ms, mfu 0.00%\nstep 45: train loss 2.9733, val loss 2.9895\nsaving checkpoint to out-shakespeare-char\niter 45: loss 2.9791, time 4675.60ms, mfu 0.00%\nstep 46: train loss 2.9582, val loss 2.9664\nsaving checkpoint to out-shakespeare-char\niter 46: loss 2.9959, time 4631.88ms, mfu 0.00%\nstep 47: train loss 2.9427, val loss 2.9527\nsaving checkpoint to out-shakespeare-char\niter 47: loss 2.9067, time 4609.71ms, mfu 0.00%\nstep 48: train loss 2.9320, val loss 2.9405\nsaving checkpoint to out-shakespeare-char\niter 48: loss 2.9619, time 5417.92ms, mfu 0.00%\nstep 49: train loss 2.9195, val loss 2.9286\nsaving checkpoint to out-shakespeare-char\niter 49: loss 2.9168, time 4604.52ms, mfu 0.00%\nstep 50: train loss 2.9035, val loss 2.9198\nsaving checkpoint to out-shakespeare-char\niter 50: loss 2.8462, time 4599.04ms, mfu 0.00%\nstep 51: train loss 2.8996, val loss 2.9149\nsaving checkpoint to out-shakespeare-char\niter 51: loss 2.8592, time 4556.08ms, mfu 0.00%\nstep 52: train loss 2.8798, val loss 2.8995\nsaving checkpoint to out-shakespeare-char\niter 52: loss 2.8908, time 4622.06ms, mfu 0.00%\nstep 53: train loss 2.8734, val loss 2.8928\nsaving checkpoint to out-shakespeare-char\niter 53: loss 2.7950, time 4551.81ms, mfu 0.00%\nstep 54: train loss 2.8690, val loss 2.8888\nsaving checkpoint to out-shakespeare-char\niter 54: loss 2.8302, time 4649.09ms, mfu 0.00%\nstep 55: train loss 2.8558, val loss 2.8725\nsaving checkpoint to out-shakespeare-char\niter 55: loss 2.8843, time 5149.00ms, mfu 0.00%\nstep 56: train loss 2.8384, val loss 2.8563\nsaving checkpoint to out-shakespeare-char\niter 56: loss 2.7331, time 4557.45ms, mfu 0.00%\nstep 57: train loss 2.8249, val loss 2.8387\nsaving checkpoint to out-shakespeare-char\niter 57: loss 2.8523, time 4597.16ms, mfu 0.00%\nstep 58: train loss 2.8108, val loss 2.8149\nsaving checkpoint to out-shakespeare-char\niter 58: loss 2.8405, time 4558.56ms, mfu 0.00%\nstep 59: train loss 2.8136, val loss 2.8201\niter 59: loss 2.8322, time 4592.24ms, mfu 0.00%\nstep 60: train loss 2.8089, val loss 2.8146\nsaving checkpoint to out-shakespeare-char\niter 60: loss 2.7747, time 4533.64ms, mfu 0.00%\nstep 61: train loss 2.7867, val loss 2.7957\nsaving checkpoint to out-shakespeare-char\niter 61: loss 2.7930, time 4582.70ms, mfu 0.00%\nstep 62: train loss 2.7874, val loss 2.7942\nsaving checkpoint to out-shakespeare-char\niter 62: loss 2.8293, time 5284.14ms, mfu 0.00%\nstep 63: train loss 2.7852, val loss 2.7903\nsaving checkpoint to out-shakespeare-char\niter 63: loss 2.7791, time 4629.45ms, mfu 0.00%\nstep 64: train loss 2.7653, val loss 2.7833\nsaving checkpoint to out-shakespeare-char\niter 64: loss 2.7191, time 4675.96ms, mfu 0.00%\nstep 65: train loss 2.7596, val loss 2.7795\nsaving checkpoint to out-shakespeare-char\niter 65: loss 2.7813, time 4590.60ms, mfu 0.00%\nstep 66: train loss 2.7652, val loss 2.7791\nsaving checkpoint to out-shakespeare-char\niter 66: loss 2.7450, time 4626.63ms, mfu 0.00%\nstep 67: train loss 2.7540, val loss 2.7700\nsaving checkpoint to out-shakespeare-char\niter 67: loss 2.6875, time 4599.18ms, mfu 0.00%\nstep 68: train loss 2.7335, val loss 2.7492\nsaving checkpoint to out-shakespeare-char\niter 68: loss 2.8111, time 4610.97ms, mfu 0.00%\nstep 69: train loss 2.7232, val loss 2.7235\nsaving checkpoint to out-shakespeare-char\niter 69: loss 2.7291, time 5372.35ms, mfu 0.00%\nstep 70: train loss 2.7277, val loss 2.7264\niter 70: loss 2.7071, time 4553.20ms, mfu 0.00%\nstep 71: train loss 2.7167, val loss 2.7227\nsaving checkpoint to out-shakespeare-char\niter 71: loss 2.7682, time 4554.43ms, mfu 0.00%\nstep 72: train loss 2.7082, val loss 2.7036\nsaving checkpoint to out-shakespeare-char\niter 72: loss 2.6151, time 4608.05ms, mfu 0.00%\nstep 73: train loss 2.7046, val loss 2.7046\niter 73: loss 2.7146, time 4566.08ms, mfu 0.00%\nstep 74: train loss 2.7130, val loss 2.7043\niter 74: loss 2.6650, time 4514.74ms, mfu 0.00%\nstep 75: train loss 2.7004, val loss 2.7081\niter 75: loss 2.6811, time 4626.42ms, mfu 0.00%\nstep 76: train loss 2.6881, val loss 2.7007\nsaving checkpoint to out-shakespeare-char\niter 76: loss 2.7535, time 5315.57ms, mfu 0.00%\nstep 77: train loss 2.6809, val loss 2.6951\nsaving checkpoint to out-shakespeare-char\niter 77: loss 2.6363, time 4622.40ms, mfu 0.00%\nstep 78: train loss 2.6918, val loss 2.6895\nsaving checkpoint to out-shakespeare-char\niter 78: loss 2.7383, time 4467.39ms, mfu 0.00%\nstep 79: train loss 2.6905, val loss 2.6908\niter 79: loss 2.6738, time 4497.11ms, mfu 0.00%\nstep 80: train loss 2.6679, val loss 2.6870\nsaving checkpoint to out-shakespeare-char\niter 80: loss 2.7252, time 4617.99ms, mfu 0.00%\nstep 81: train loss 2.6701, val loss 2.6788\nsaving checkpoint to out-shakespeare-char\niter 81: loss 2.6339, time 4535.09ms, mfu 0.00%\nstep 82: train loss 2.6622, val loss 2.6708\nsaving checkpoint to out-shakespeare-char\niter 82: loss 2.6721, time 4738.64ms, mfu 0.00%\nstep 83: train loss 2.6701, val loss 2.6648\nsaving checkpoint to out-shakespeare-char\niter 83: loss 2.5895, time 5129.16ms, mfu 0.00%\nstep 84: train loss 2.6589, val loss 2.6596\nsaving checkpoint to out-shakespeare-char\niter 84: loss 2.6917, time 4680.43ms, mfu 0.00%\nstep 85: train loss 2.6599, val loss 2.6525\nsaving checkpoint to out-shakespeare-char\niter 85: loss 2.6682, time 4660.98ms, mfu 0.00%\nstep 86: train loss 2.6574, val loss 2.6517\nsaving checkpoint to out-shakespeare-char\niter 86: loss 2.6821, time 4710.85ms, mfu 0.00%\nstep 87: train loss 2.6588, val loss 2.6488\nsaving checkpoint to out-shakespeare-char\niter 87: loss 2.5995, time 4669.94ms, mfu 0.00%\nstep 88: train loss 2.6442, val loss 2.6495\niter 88: loss 2.6408, time 4630.41ms, mfu 0.00%\nstep 89: train loss 2.6408, val loss 2.6434\nsaving checkpoint to out-shakespeare-char\niter 89: loss 2.6595, time 4783.67ms, mfu 0.00%\nstep 90: train loss 2.6408, val loss 2.6441\niter 90: loss 2.5472, time 5024.20ms, mfu 0.00%\nstep 91: train loss 2.6461, val loss 2.6596\niter 91: loss 2.6703, time 4508.36ms, mfu 0.00%\nstep 92: train loss 2.6484, val loss 2.6574\niter 92: loss 2.6478, time 4617.90ms, mfu 0.00%\nstep 93: train loss 2.6430, val loss 2.6522\niter 93: loss 2.6163, time 4420.87ms, mfu 0.00%\nstep 94: train loss 2.6398, val loss 2.6407\nsaving checkpoint to out-shakespeare-char\niter 94: loss 2.6509, time 4525.38ms, mfu 0.00%\nstep 95: train loss 2.6301, val loss 2.6321\nsaving checkpoint to out-shakespeare-char\niter 95: loss 2.6384, time 4796.85ms, mfu 0.00%\nstep 96: train loss 2.6339, val loss 2.6379\niter 96: loss 2.6293, time 5105.11ms, mfu 0.00%\nstep 97: train loss 2.6225, val loss 2.6345\niter 97: loss 2.6523, time 4955.78ms, mfu 0.00%\nstep 98: train loss 2.6285, val loss 2.6443\niter 98: loss 2.6382, time 4606.26ms, mfu 0.00%\nstep 99: train loss 2.6393, val loss 2.6430\niter 99: loss 2.6464, time 4695.51ms, mfu 0.00%\nstep 100: train loss 2.6157, val loss 2.6237\nsaving checkpoint to out-shakespeare-char\niter 100: loss 2.6548, time 4732.81ms, mfu 0.00%\nstep 101: train loss 2.6127, val loss 2.5990\nsaving checkpoint to out-shakespeare-char\niter 101: loss 2.5952, time 4735.45ms, mfu 0.00%\nstep 102: train loss 2.6193, val loss 2.6049\niter 102: loss 2.6545, time 4696.80ms, mfu 0.00%\nstep 103: train loss 2.6093, val loss 2.6142\niter 103: loss 2.5176, time 5298.95ms, mfu 0.00%\nstep 104: train loss 2.6233, val loss 2.6309\niter 104: loss 2.5712, time 4725.58ms, mfu 0.00%\nstep 105: train loss 2.6178, val loss 2.6227\niter 105: loss 2.5512, time 4703.72ms, mfu 0.00%\nstep 106: train loss 2.6083, val loss 2.6179\niter 106: loss 2.5272, time 4616.43ms, mfu 0.00%\nstep 107: train loss 2.6234, val loss 2.6369\niter 107: loss 2.5388, time 4722.54ms, mfu 0.00%\nstep 108: train loss 2.6121, val loss 2.6240\niter 108: loss 2.5781, time 4674.00ms, mfu 0.00%\nstep 109: train loss 2.6018, val loss 2.6089\niter 109: loss 2.6221, time 4652.05ms, mfu 0.00%\nstep 110: train loss 2.6125, val loss 2.6057\niter 110: loss 2.6760, time 5454.87ms, mfu 0.00%\nstep 111: train loss 2.5977, val loss 2.5963\nsaving checkpoint to out-shakespeare-char\niter 111: loss 2.5206, time 4698.87ms, mfu 0.00%\nstep 112: train loss 2.5924, val loss 2.5938\nsaving checkpoint to out-shakespeare-char\niter 112: loss 2.5894, time 4650.87ms, mfu 0.00%\nstep 113: train loss 2.5890, val loss 2.5964\niter 113: loss 2.6199, time 4730.44ms, mfu 0.00%\nstep 114: train loss 2.5864, val loss 2.5806\nsaving checkpoint to out-shakespeare-char\niter 114: loss 2.5846, time 4666.95ms, mfu 0.00%\nstep 115: train loss 2.5807, val loss 2.5683\nsaving checkpoint to out-shakespeare-char\niter 115: loss 2.5307, time 4774.27ms, mfu 0.00%\nstep 116: train loss 2.5786, val loss 2.5631\nsaving checkpoint to out-shakespeare-char\niter 116: loss 2.4961, time 4787.99ms, mfu 0.00%\nstep 117: train loss 2.5895, val loss 2.5896\niter 117: loss 2.5758, time 5617.72ms, mfu 0.00%\nstep 118: train loss 2.5791, val loss 2.5874\niter 118: loss 2.6668, time 4654.78ms, mfu 0.00%\nstep 119: train loss 2.5768, val loss 2.5832\niter 119: loss 2.5889, time 4545.94ms, mfu 0.00%\nstep 120: train loss 2.5658, val loss 2.5704\niter 120: loss 2.5177, time 4675.17ms, mfu 0.00%\nstep 121: train loss 2.5692, val loss 2.5694\niter 121: loss 2.6103, time 4640.27ms, mfu 0.00%\nstep 122: train loss 2.5695, val loss 2.5744\niter 122: loss 2.5472, time 4695.61ms, mfu 0.00%\nstep 123: train loss 2.5651, val loss 2.5713\niter 123: loss 2.5182, time 5442.90ms, mfu 0.00%\nstep 124: train loss 2.5613, val loss 2.5710\niter 124: loss 2.5380, time 5002.05ms, mfu 0.00%\nstep 125: train loss 2.5664, val loss 2.5790\niter 125: loss 2.5222, time 4942.51ms, mfu 0.00%\nstep 126: train loss 2.5636, val loss 2.5823\niter 126: loss 2.5255, time 4760.75ms, mfu 0.00%\nstep 127: train loss 2.5656, val loss 2.5897\niter 127: loss 2.5850, time 4717.00ms, mfu 0.00%\nstep 128: train loss 2.5715, val loss 2.5942\niter 128: loss 2.6535, time 4661.68ms, mfu 0.00%\nstep 129: train loss 2.5699, val loss 2.5903\niter 129: loss 2.5311, time 4668.14ms, mfu 0.00%\nstep 130: train loss 2.5512, val loss 2.5712\niter 130: loss 2.6370, time 5226.29ms, mfu 0.00%\nstep 131: train loss 2.5575, val loss 2.5726\niter 131: loss 2.4962, time 4667.92ms, mfu 0.00%\nstep 132: train loss 2.5581, val loss 2.5670\niter 132: loss 2.6181, time 4776.86ms, mfu 0.00%\nstep 133: train loss 2.5383, val loss 2.5496\nsaving checkpoint to out-shakespeare-char\niter 133: loss 2.5395, time 4650.17ms, mfu 0.00%\nstep 134: train loss 2.5465, val loss 2.5539\niter 134: loss 2.6027, time 4724.99ms, mfu 0.00%\nstep 135: train loss 2.5468, val loss 2.5517\niter 135: loss 2.5163, time 4693.27ms, mfu 0.00%\nstep 136: train loss 2.5447, val loss 2.5501\niter 136: loss 2.5583, time 4795.81ms, mfu 0.00%\nstep 137: train loss 2.5468, val loss 2.5577\niter 137: loss 2.4955, time 5278.11ms, mfu 0.00%\nstep 138: train loss 2.5445, val loss 2.5501\niter 138: loss 2.4911, time 4767.38ms, mfu 0.00%\nstep 139: train loss 2.5339, val loss 2.5463\nsaving checkpoint to out-shakespeare-char\niter 139: loss 2.6229, time 4653.26ms, mfu 0.00%\nstep 140: train loss 2.5452, val loss 2.5424\nsaving checkpoint to out-shakespeare-char\niter 140: loss 2.4750, time 4885.86ms, mfu 0.00%\nstep 141: train loss 2.5479, val loss 2.5435\niter 141: loss 2.5597, time 4703.58ms, mfu 0.00%\nstep 142: train loss 2.5496, val loss 2.5429\niter 142: loss 2.5229, time 4790.37ms, mfu 0.00%\nstep 143: train loss 2.5472, val loss 2.5588\niter 143: loss 2.5717, time 5390.57ms, mfu 0.00%\nstep 144: train loss 2.5407, val loss 2.5525\niter 144: loss 2.5614, time 4690.03ms, mfu 0.00%\nstep 145: train loss 2.5376, val loss 2.5579\niter 145: loss 2.5714, time 4761.22ms, mfu 0.00%\nstep 146: train loss 2.5393, val loss 2.5384\nsaving checkpoint to out-shakespeare-char\niter 146: loss 2.5257, time 4675.43ms, mfu 0.00%\nstep 147: train loss 2.5298, val loss 2.5306\nsaving checkpoint to out-shakespeare-char\niter 147: loss 2.5661, time 4774.04ms, mfu 0.00%\nstep 148: train loss 2.5297, val loss 2.5275\nsaving checkpoint to out-shakespeare-char\niter 148: loss 2.5270, time 4853.36ms, mfu 0.00%\nstep 149: train loss 2.5265, val loss 2.5301\niter 149: loss 2.4821, time 5044.71ms, mfu 0.00%\nstep 150: train loss 2.5287, val loss 2.5289\niter 150: loss 2.4727, time 5709.22ms, mfu 0.00%\nstep 151: train loss 2.5164, val loss 2.5292\niter 151: loss 2.4763, time 5018.17ms, mfu 0.00%\nstep 152: train loss 2.5165, val loss 2.5248\nsaving checkpoint to out-shakespeare-char\niter 152: loss 2.4250, time 4923.71ms, mfu 0.00%\nstep 153: train loss 2.5238, val loss 2.5340\niter 153: loss 2.5771, time 4658.99ms, mfu 0.00%\nstep 154: train loss 2.5302, val loss 2.5290\niter 154: loss 2.4380, time 4667.59ms, mfu 0.00%\nstep 155: train loss 2.5461, val loss 2.5423\niter 155: loss 2.6410, time 4751.03ms, mfu 0.00%\nstep 156: train loss 2.5477, val loss 2.5452\niter 156: loss 2.5445, time 4751.93ms, mfu 0.00%\nstep 157: train loss 2.5183, val loss 2.5293\niter 157: loss 2.5405, time 5131.21ms, mfu 0.00%\nstep 158: train loss 2.5189, val loss 2.5287\niter 158: loss 2.4906, time 4615.76ms, mfu 0.00%\nstep 159: train loss 2.5291, val loss 2.5350\niter 159: loss 2.4315, time 4650.23ms, mfu 0.00%\nstep 160: train loss 2.5347, val loss 2.5489\niter 160: loss 2.4587, time 4656.10ms, mfu 0.00%\nstep 161: train loss 2.5481, val loss 2.5596\niter 161: loss 2.5619, time 4629.70ms, mfu 0.00%\nstep 162: train loss 2.5271, val loss 2.5375\niter 162: loss 2.4469, time 4651.09ms, mfu 0.00%\nstep 163: train loss 2.5240, val loss 2.5237\nsaving checkpoint to out-shakespeare-char\niter 163: loss 2.4961, time 5242.14ms, mfu 0.00%\nstep 164: train loss 2.5180, val loss 2.5242\niter 164: loss 2.5512, time 4848.29ms, mfu 0.00%\nstep 165: train loss 2.5116, val loss 2.5204\nsaving checkpoint to out-shakespeare-char\niter 165: loss 2.4679, time 4776.78ms, mfu 0.00%\nstep 166: train loss 2.5086, val loss 2.5146\nsaving checkpoint to out-shakespeare-char\niter 166: loss 2.4689, time 4688.15ms, mfu 0.00%\nstep 167: train loss 2.5043, val loss 2.5071\nsaving checkpoint to out-shakespeare-char\niter 167: loss 2.6496, time 4844.81ms, mfu 0.00%\nstep 168: train loss 2.5065, val loss 2.5167\niter 168: loss 2.4696, time 4683.37ms, mfu 0.00%\nstep 169: train loss 2.5021, val loss 2.5252\niter 169: loss 2.4383, time 4758.07ms, mfu 0.00%\nstep 170: train loss 2.5095, val loss 2.5210\niter 170: loss 2.5046, time 5535.26ms, mfu 0.00%\nstep 171: train loss 2.5007, val loss 2.5137\niter 171: loss 2.5698, time 4702.85ms, mfu 0.00%\nstep 172: train loss 2.4957, val loss 2.5134\niter 172: loss 2.3987, time 4723.98ms, mfu 0.00%\nstep 173: train loss 2.4933, val loss 2.5060\nsaving checkpoint to out-shakespeare-char\niter 173: loss 2.4362, time 4791.92ms, mfu 0.00%\nstep 174: train loss 2.5027, val loss 2.5148\niter 174: loss 2.6065, time 4843.72ms, mfu 0.00%\nstep 175: train loss 2.5033, val loss 2.5186\niter 175: loss 2.4948, time 4759.70ms, mfu 0.00%\nstep 176: train loss 2.4933, val loss 2.5090\niter 176: loss 2.4755, time 4720.49ms, mfu 0.00%\nstep 177: train loss 2.4896, val loss 2.5073\niter 177: loss 2.4476, time 5500.51ms, mfu 0.00%\nstep 178: train loss 2.4952, val loss 2.5080\niter 178: loss 2.5624, time 4654.84ms, mfu 0.00%\nstep 179: train loss 2.5019, val loss 2.5225\niter 179: loss 2.4543, time 4769.38ms, mfu 0.00%\nstep 180: train loss 2.4908, val loss 2.5073\niter 180: loss 2.4800, time 4885.59ms, mfu 0.00%\nstep 181: train loss 2.4958, val loss 2.4979\nsaving checkpoint to out-shakespeare-char\niter 181: loss 2.4691, time 4976.65ms, mfu 0.00%\nstep 182: train loss 2.4945, val loss 2.4934\nsaving checkpoint to out-shakespeare-char\niter 182: loss 2.4920, time 5006.15ms, mfu 0.00%\nstep 183: train loss 2.4991, val loss 2.4965\niter 183: loss 2.4536, time 5400.59ms, mfu 0.00%\nstep 184: train loss 2.4838, val loss 2.4934\nsaving checkpoint to out-shakespeare-char\niter 184: loss 2.4828, time 4880.97ms, mfu 0.00%\nstep 185: train loss 2.4872, val loss 2.4939\niter 185: loss 2.5211, time 4724.25ms, mfu 0.00%\nstep 186: train loss 2.5027, val loss 2.5118\niter 186: loss 2.5300, time 4688.54ms, mfu 0.00%\nstep 187: train loss 2.4969, val loss 2.5097\niter 187: loss 2.5102, time 4726.35ms, mfu 0.00%\nstep 188: train loss 2.4939, val loss 2.5009\niter 188: loss 2.4956, time 4703.57ms, mfu 0.00%\nstep 189: train loss 2.4888, val loss 2.4947\niter 189: loss 2.4740, time 4729.64ms, mfu 0.00%\nstep 190: train loss 2.4912, val loss 2.5034\niter 190: loss 2.5033, time 5431.44ms, mfu 0.00%\nstep 191: train loss 2.4916, val loss 2.4962\niter 191: loss 2.5445, time 4615.68ms, mfu 0.00%\nstep 192: train loss 2.4802, val loss 2.4932\nsaving checkpoint to out-shakespeare-char\niter 192: loss 2.4458, time 4850.06ms, mfu 0.00%\nstep 193: train loss 2.4869, val loss 2.4970\niter 193: loss 2.4497, time 4712.02ms, mfu 0.00%\nstep 194: train loss 2.4832, val loss 2.4980\niter 194: loss 2.4476, time 4784.27ms, mfu 0.00%\nstep 195: train loss 2.4843, val loss 2.4971\niter 195: loss 2.5027, time 4669.95ms, mfu 0.00%\nstep 196: train loss 2.4899, val loss 2.5127\niter 196: loss 2.5206, time 4692.30ms, mfu 0.00%\nstep 197: train loss 2.4918, val loss 2.4999\niter 197: loss 2.4552, time 5392.35ms, mfu 0.00%\nstep 198: train loss 2.4849, val loss 2.5014\niter 198: loss 2.5340, time 4727.93ms, mfu 0.00%\nstep 199: train loss 2.4870, val loss 2.4953\niter 199: loss 2.4706, time 4879.46ms, mfu 0.00%\nstep 200: train loss 2.4786, val loss 2.4884\nsaving checkpoint to out-shakespeare-char\niter 200: loss 2.4900, time 4715.96ms, mfu 0.00%\nstep 201: train loss 2.4772, val loss 2.4891\niter 201: loss 2.5390, time 4601.36ms, mfu 0.00%\nstep 202: train loss 2.4873, val loss 2.4827\nsaving checkpoint to out-shakespeare-char\niter 202: loss 2.4196, time 4704.72ms, mfu 0.00%\nstep 203: train loss 2.4792, val loss 2.4905\niter 203: loss 2.5687, time 4663.00ms, mfu 0.00%\nstep 204: train loss 2.4676, val loss 2.4896\niter 204: loss 2.5397, time 5310.51ms, mfu 0.00%\nstep 205: train loss 2.4584, val loss 2.4784\nsaving checkpoint to out-shakespeare-char\niter 205: loss 2.4480, time 4607.28ms, mfu 0.00%\nstep 206: train loss 2.4628, val loss 2.4782\nsaving checkpoint to out-shakespeare-char\niter 206: loss 2.4758, time 4788.76ms, mfu 0.00%\nstep 207: train loss 2.4628, val loss 2.4810\niter 207: loss 2.5181, time 4894.17ms, mfu 0.00%\nstep 208: train loss 2.4684, val loss 2.4738\nsaving checkpoint to out-shakespeare-char\niter 208: loss 2.5006, time 4926.26ms, mfu 0.00%\nstep 209: train loss 2.4716, val loss 2.4838\niter 209: loss 2.5342, time 4911.03ms, mfu 0.00%\nstep 210: train loss 2.4766, val loss 2.4925\niter 210: loss 2.5144, time 5445.54ms, mfu 0.00%\nstep 211: train loss 2.4812, val loss 2.5009\niter 211: loss 2.4057, time 4646.65ms, mfu 0.00%\nstep 212: train loss 2.4889, val loss 2.5094\niter 212: loss 2.4287, time 4868.73ms, mfu 0.00%\nstep 213: train loss 2.4791, val loss 2.5078\niter 213: loss 2.4299, time 4767.97ms, mfu 0.00%\nstep 214: train loss 2.4859, val loss 2.5043\niter 214: loss 2.4171, time 4877.06ms, mfu 0.00%\nstep 215: train loss 2.4820, val loss 2.5004\niter 215: loss 2.5838, time 4797.28ms, mfu 0.00%\nstep 216: train loss 2.4675, val loss 2.4935\niter 216: loss 2.4553, time 4745.86ms, mfu 0.00%\nstep 217: train loss 2.4640, val loss 2.4799\niter 217: loss 2.5024, time 5416.18ms, mfu 0.00%\nstep 218: train loss 2.4616, val loss 2.4759\niter 218: loss 2.4798, time 4728.73ms, mfu 0.00%\nstep 219: train loss 2.4701, val loss 2.4837\niter 219: loss 2.5094, time 4680.29ms, mfu 0.00%\nstep 220: train loss 2.4691, val loss 2.4774\niter 220: loss 2.4338, time 4741.89ms, mfu 0.00%\nstep 221: train loss 2.4554, val loss 2.4672\nsaving checkpoint to out-shakespeare-char\niter 221: loss 2.3961, time 4765.78ms, mfu 0.00%\nstep 222: train loss 2.4455, val loss 2.4585\nsaving checkpoint to out-shakespeare-char\niter 222: loss 2.5535, time 4708.29ms, mfu 0.00%\nstep 223: train loss 2.4646, val loss 2.4720\niter 223: loss 2.4801, time 4835.64ms, mfu 0.00%\nstep 224: train loss 2.4706, val loss 2.4803\niter 224: loss 2.4777, time 5296.77ms, mfu 0.00%\nstep 225: train loss 2.4642, val loss 2.4819\niter 225: loss 2.3947, time 4823.04ms, mfu 0.00%\nstep 226: train loss 2.4598, val loss 2.4779\niter 226: loss 2.4102, time 4702.76ms, mfu 0.00%\nstep 227: train loss 2.4586, val loss 2.4710\niter 227: loss 2.4895, time 4750.50ms, mfu 0.00%\nstep 228: train loss 2.4507, val loss 2.4630\niter 228: loss 2.5052, time 4730.23ms, mfu 0.00%\nstep 229: train loss 2.4517, val loss 2.4645\niter 229: loss 2.4611, time 4739.85ms, mfu 0.00%\nstep 230: train loss 2.4599, val loss 2.4681\niter 230: loss 2.4940, time 5233.08ms, mfu 0.00%\nstep 231: train loss 2.4593, val loss 2.4678\niter 231: loss 2.5094, time 4827.67ms, mfu 0.00%\nstep 232: train loss 2.4493, val loss 2.4495\nsaving checkpoint to out-shakespeare-char\niter 232: loss 2.4404, time 4786.98ms, mfu 0.00%\nstep 233: train loss 2.4459, val loss 2.4560\niter 233: loss 2.4863, time 5026.24ms, mfu 0.00%\nstep 234: train loss 2.4461, val loss 2.4523\niter 234: loss 2.4477, time 4964.85ms, mfu 0.00%\nstep 235: train loss 2.4448, val loss 2.4534\niter 235: loss 2.4267, time 4997.60ms, mfu 0.00%\nstep 236: train loss 2.4484, val loss 2.4554\niter 236: loss 2.3950, time 4760.58ms, mfu 0.00%\nstep 237: train loss 2.4491, val loss 2.4598\niter 237: loss 2.4743, time 5307.31ms, mfu 0.00%\nstep 238: train loss 2.4451, val loss 2.4639\niter 238: loss 2.3600, time 4737.72ms, mfu 0.00%\nstep 239: train loss 2.4438, val loss 2.4575\niter 239: loss 2.5339, time 4715.97ms, mfu 0.00%\nstep 240: train loss 2.4538, val loss 2.4599\niter 240: loss 2.4701, time 4643.95ms, mfu 0.00%\nstep 241: train loss 2.4395, val loss 2.4491\nsaving checkpoint to out-shakespeare-char\niter 241: loss 2.4486, time 4811.01ms, mfu 0.00%\nstep 242: train loss 2.4475, val loss 2.4542\niter 242: loss 2.4640, time 4587.03ms, mfu 0.00%\nstep 243: train loss 2.4448, val loss 2.4602\niter 243: loss 2.5007, time 4677.52ms, mfu 0.00%\nstep 244: train loss 2.4437, val loss 2.4553\niter 244: loss 2.3644, time 5307.17ms, mfu 0.00%\nstep 245: train loss 2.4347, val loss 2.4516\niter 245: loss 2.4013, time 4616.50ms, mfu 0.00%\nstep 246: train loss 2.4350, val loss 2.4476\nsaving checkpoint to out-shakespeare-char\niter 246: loss 2.4722, time 4701.66ms, mfu 0.00%\nstep 247: train loss 2.4318, val loss 2.4402\nsaving checkpoint to out-shakespeare-char\niter 247: loss 2.4644, time 4704.14ms, mfu 0.00%\nstep 248: train loss 2.4238, val loss 2.4305\nsaving checkpoint to out-shakespeare-char\niter 248: loss 2.4744, time 4638.00ms, mfu 0.00%\nstep 249: train loss 2.4300, val loss 2.4212\nsaving checkpoint to out-shakespeare-char\niter 249: loss 2.4327, time 4688.07ms, mfu 0.00%\nstep 250: train loss 2.4238, val loss 2.4220\niter 250: loss 2.4592, time 4967.62ms, mfu 0.00%\nstep 251: train loss 2.4182, val loss 2.4186\nsaving checkpoint to out-shakespeare-char\niter 251: loss 2.4552, time 5011.73ms, mfu 0.00%\nstep 252: train loss 2.4199, val loss 2.4193\niter 252: loss 2.3979, time 4480.24ms, mfu 0.00%\nstep 253: train loss 2.4163, val loss 2.4215\niter 253: loss 2.3794, time 4478.52ms, mfu 0.00%\nstep 254: train loss 2.4284, val loss 2.4256\niter 254: loss 2.4050, time 4589.46ms, mfu 0.00%\nstep 255: train loss 2.4199, val loss 2.4265\niter 255: loss 2.3337, time 4374.65ms, mfu 0.00%\nstep 256: train loss 2.4191, val loss 2.4155\nsaving checkpoint to out-shakespeare-char\niter 256: loss 2.3786, time 4503.35ms, mfu 0.00%\nstep 257: train loss 2.4171, val loss 2.4142\nsaving checkpoint to out-shakespeare-char\niter 257: loss 2.3712, time 4913.68ms, mfu 0.00%\nstep 258: train loss 2.4198, val loss 2.4347\niter 258: loss 2.4209, time 5144.24ms, mfu 0.00%\nstep 259: train loss 2.4252, val loss 2.4417\niter 259: loss 2.4366, time 4730.95ms, mfu 0.00%\nstep 260: train loss 2.4204, val loss 2.4361\niter 260: loss 2.3539, time 4849.28ms, mfu 0.00%\nstep 261: train loss 2.4235, val loss 2.4408\niter 261: loss 2.3664, time 4544.91ms, mfu 0.00%\nstep 262: train loss 2.4193, val loss 2.4320\niter 262: loss 2.4586, time 4513.94ms, mfu 0.00%\nstep 263: train loss 2.4142, val loss 2.4266\niter 263: loss 2.4883, time 4441.85ms, mfu 0.00%\nstep 264: train loss 2.4164, val loss 2.4227\niter 264: loss 2.3405, time 5003.20ms, mfu 0.00%\nstep 265: train loss 2.4090, val loss 2.4197\niter 265: loss 2.3747, time 4512.34ms, mfu 0.00%\nstep 266: train loss 2.4161, val loss 2.4273\niter 266: loss 2.4193, time 4394.66ms, mfu 0.00%\nstep 267: train loss 2.4161, val loss 2.4229\niter 267: loss 2.3142, time 4418.97ms, mfu 0.00%\nstep 268: train loss 2.4169, val loss 2.4182\niter 268: loss 2.4888, time 4423.09ms, mfu 0.00%\nstep 269: train loss 2.4152, val loss 2.4284\niter 269: loss 2.3606, time 4413.66ms, mfu 0.00%\nstep 270: train loss 2.4214, val loss 2.4233\niter 270: loss 2.3879, time 4352.92ms, mfu 0.00%\nstep 271: train loss 2.4214, val loss 2.4305\niter 271: loss 2.4044, time 4772.70ms, mfu 0.00%\nstep 272: train loss 2.4234, val loss 2.4374\niter 272: loss 2.4457, time 4715.42ms, mfu 0.00%\nstep 273: train loss 2.4161, val loss 2.4224\niter 273: loss 2.4947, time 4513.12ms, mfu 0.00%\nstep 274: train loss 2.4094, val loss 2.4018\nsaving checkpoint to out-shakespeare-char\niter 274: loss 2.3691, time 4464.38ms, mfu 0.00%\nstep 275: train loss 2.4116, val loss 2.4107\niter 275: loss 2.3672, time 4433.69ms, mfu 0.00%\nstep 276: train loss 2.4190, val loss 2.4183\niter 276: loss 2.3744, time 4552.89ms, mfu 0.00%\nstep 277: train loss 2.4161, val loss 2.4207\niter 277: loss 2.4075, time 4355.54ms, mfu 0.00%\nstep 278: train loss 2.4031, val loss 2.4139\niter 278: loss 2.4930, time 4479.60ms, mfu 0.00%\nstep 279: train loss 2.4015, val loss 2.4022\niter 279: loss 2.4201, time 5107.98ms, mfu 0.00%\nstep 280: train loss 2.3983, val loss 2.4125\niter 280: loss 2.4168, time 4618.20ms, mfu 0.00%\nstep 281: train loss 2.4057, val loss 2.4247\niter 281: loss 2.4243, time 4430.88ms, mfu 0.00%\nstep 282: train loss 2.4106, val loss 2.4212\niter 282: loss 2.3919, time 4567.43ms, mfu 0.00%\nstep 283: train loss 2.4049, val loss 2.4363\niter 283: loss 2.3871, time 4431.24ms, mfu 0.00%\nstep 284: train loss 2.4084, val loss 2.4351\niter 284: loss 2.4440, time 4393.70ms, mfu 0.00%\nstep 285: train loss 2.4083, val loss 2.4251\niter 285: loss 2.4060, time 4544.53ms, mfu 0.00%\nstep 286: train loss 2.4106, val loss 2.4194\niter 286: loss 2.4375, time 5171.61ms, mfu 0.00%\nstep 287: train loss 2.4090, val loss 2.4252\niter 287: loss 2.3662, time 4721.12ms, mfu 0.00%\nstep 288: train loss 2.4282, val loss 2.4329\niter 288: loss 2.4190, time 4658.10ms, mfu 0.00%\nstep 289: train loss 2.4311, val loss 2.4250\niter 289: loss 2.4165, time 4832.18ms, mfu 0.00%\nstep 290: train loss 2.4211, val loss 2.4115\niter 290: loss 2.4412, time 4805.20ms, mfu 0.00%\nstep 291: train loss 2.4210, val loss 2.4268\niter 291: loss 2.3396, time 4491.53ms, mfu 0.00%\nstep 292: train loss 2.4239, val loss 2.4273\niter 292: loss 2.4380, time 4698.37ms, mfu 0.00%\nstep 293: train loss 2.4081, val loss 2.4125\niter 293: loss 2.3953, time 4989.66ms, mfu 0.00%\nstep 294: train loss 2.4105, val loss 2.4217\niter 294: loss 2.4270, time 4430.74ms, mfu 0.00%\nstep 295: train loss 2.4050, val loss 2.4148\niter 295: loss 2.4387, time 4639.35ms, mfu 0.00%\nstep 296: train loss 2.3898, val loss 2.4067\niter 296: loss 2.3575, time 4449.47ms, mfu 0.00%\nstep 297: train loss 2.3905, val loss 2.4107\niter 297: loss 2.3905, time 4516.42ms, mfu 0.00%\nstep 298: train loss 2.3971, val loss 2.4141\niter 298: loss 2.4570, time 4464.33ms, mfu 0.00%\nstep 299: train loss 2.3972, val loss 2.4262\niter 299: loss 2.4136, time 4585.95ms, mfu 0.00%\nstep 300: train loss 2.4053, val loss 2.4316\niter 300: loss 2.4085, time 5105.47ms, mfu 0.00%\nstep 301: train loss 2.3957, val loss 2.4218\niter 301: loss 2.3920, time 4573.36ms, mfu 0.00%\nstep 302: train loss 2.3976, val loss 2.4146\niter 302: loss 2.3960, time 4482.82ms, mfu 0.00%\nstep 303: train loss 2.4044, val loss 2.4129\niter 303: loss 2.4451, time 4436.83ms, mfu 0.00%\nstep 304: train loss 2.4034, val loss 2.4201\niter 304: loss 2.4165, time 4490.14ms, mfu 0.00%\nstep 305: train loss 2.4011, val loss 2.4180\niter 305: loss 2.4079, time 4360.53ms, mfu 0.00%\nstep 306: train loss 2.3948, val loss 2.4215\niter 306: loss 2.3678, time 4543.18ms, mfu 0.00%\nstep 307: train loss 2.3984, val loss 2.4204\niter 307: loss 2.3884, time 5191.70ms, mfu 0.00%\nstep 308: train loss 2.3936, val loss 2.4033\niter 308: loss 2.5262, time 4476.39ms, mfu 0.00%\nstep 309: train loss 2.3948, val loss 2.4207\niter 309: loss 2.4120, time 4521.31ms, mfu 0.00%\nstep 310: train loss 2.3910, val loss 2.4162\niter 310: loss 2.3322, time 4472.18ms, mfu 0.00%\nstep 311: train loss 2.3836, val loss 2.3913\nsaving checkpoint to out-shakespeare-char\niter 311: loss 2.3501, time 4428.36ms, mfu 0.00%\nstep 312: train loss 2.3936, val loss 2.4038\niter 312: loss 2.4110, time 4518.02ms, mfu 0.00%\nstep 313: train loss 2.4038, val loss 2.4204\niter 313: loss 2.3310, time 4403.34ms, mfu 0.00%\nstep 314: train loss 2.3946, val loss 2.4004\niter 314: loss 2.4146, time 5087.70ms, mfu 0.00%\nstep 315: train loss 2.3931, val loss 2.3995\niter 315: loss 2.3604, time 4498.08ms, mfu 0.00%\nstep 316: train loss 2.4016, val loss 2.4039\niter 316: loss 2.4164, time 4457.31ms, mfu 0.00%\nstep 317: train loss 2.3889, val loss 2.4027\niter 317: loss 2.4273, time 4665.98ms, mfu 0.00%\nstep 318: train loss 2.3837, val loss 2.3950\niter 318: loss 2.3610, time 4724.89ms, mfu 0.00%\nstep 319: train loss 2.3794, val loss 2.3984\niter 319: loss 2.3740, time 4771.42ms, mfu 0.00%\nstep 320: train loss 2.3824, val loss 2.3964\niter 320: loss 2.3339, time 4682.65ms, mfu 0.00%\nstep 321: train loss 2.3822, val loss 2.3903\nsaving checkpoint to out-shakespeare-char\niter 321: loss 2.3462, time 5192.25ms, mfu 0.00%\nstep 322: train loss 2.3921, val loss 2.4048\niter 322: loss 2.3544, time 4433.78ms, mfu 0.00%\nstep 323: train loss 2.3862, val loss 2.3988\niter 323: loss 2.3733, time 4510.07ms, mfu 0.00%\nstep 324: train loss 2.3811, val loss 2.4001\niter 324: loss 2.3556, time 4460.49ms, mfu 0.00%\nstep 325: train loss 2.3849, val loss 2.4065\niter 325: loss 2.4627, time 4496.26ms, mfu 0.00%\nstep 326: train loss 2.3855, val loss 2.3963\niter 326: loss 2.3877, time 4535.22ms, mfu 0.00%\nstep 327: train loss 2.3808, val loss 2.4011\niter 327: loss 2.3051, time 4370.62ms, mfu 0.00%\nstep 328: train loss 2.3780, val loss 2.3935\niter 328: loss 2.3247, time 5283.03ms, mfu 0.00%\nstep 329: train loss 2.3842, val loss 2.3959\niter 329: loss 2.3571, time 4433.05ms, mfu 0.00%\nstep 330: train loss 2.3889, val loss 2.3957\niter 330: loss 2.4064, time 4506.01ms, mfu 0.00%\nstep 331: train loss 2.3835, val loss 2.3887\nsaving checkpoint to out-shakespeare-char\niter 331: loss 2.3303, time 4532.01ms, mfu 0.00%\nstep 332: train loss 2.3748, val loss 2.3915\niter 332: loss 2.3835, time 4544.81ms, mfu 0.00%\nstep 333: train loss 2.3803, val loss 2.3993\niter 333: loss 2.3712, time 4397.47ms, mfu 0.00%\nstep 334: train loss 2.3715, val loss 2.4006\niter 334: loss 2.3798, time 4546.91ms, mfu 0.00%\nstep 335: train loss 2.3773, val loss 2.3986\niter 335: loss 2.3452, time 5124.79ms, mfu 0.00%\nstep 336: train loss 2.3691, val loss 2.3835\nsaving checkpoint to out-shakespeare-char\niter 336: loss 2.2677, time 4417.81ms, mfu 0.00%\nstep 337: train loss 2.3650, val loss 2.3797\nsaving checkpoint to out-shakespeare-char\niter 337: loss 2.2991, time 4551.09ms, mfu 0.00%\nstep 338: train loss 2.3664, val loss 2.3865\niter 338: loss 2.3222, time 4482.71ms, mfu 0.00%\nstep 339: train loss 2.3707, val loss 2.3808\niter 339: loss 2.4078, time 4415.02ms, mfu 0.00%\nstep 340: train loss 2.3722, val loss 2.3874\niter 340: loss 2.4330, time 4587.37ms, mfu 0.00%\nstep 341: train loss 2.3719, val loss 2.3880\niter 341: loss 2.3181, time 4960.19ms, mfu 0.00%\nstep 342: train loss 2.3672, val loss 2.3915\niter 342: loss 2.3765, time 5762.43ms, mfu 0.00%\nstep 343: train loss 2.3693, val loss 2.3883\niter 343: loss 2.3421, time 4684.00ms, mfu 0.00%\nstep 344: train loss 2.3714, val loss 2.3891\niter 344: loss 2.3678, time 4637.15ms, mfu 0.00%\nstep 345: train loss 2.3641, val loss 2.3865\niter 345: loss 2.3057, time 4724.39ms, mfu 0.00%\nstep 346: train loss 2.3602, val loss 2.3800\niter 346: loss 2.3671, time 4683.62ms, mfu 0.00%\nstep 347: train loss 2.3687, val loss 2.3794\nsaving checkpoint to out-shakespeare-char\niter 347: loss 2.4302, time 4845.41ms, mfu 0.00%\nstep 348: train loss 2.3719, val loss 2.3743\nsaving checkpoint to out-shakespeare-char\niter 348: loss 2.4831, time 4980.36ms, mfu 0.00%\nstep 349: train loss 2.3650, val loss 2.3771\niter 349: loss 2.3431, time 4985.09ms, mfu 0.00%\nstep 350: train loss 2.3699, val loss 2.3918\niter 350: loss 2.4055, time 4670.23ms, mfu 0.00%\nstep 351: train loss 2.3839, val loss 2.3838\niter 351: loss 2.3579, time 4704.62ms, mfu 0.00%\nstep 352: train loss 2.3808, val loss 2.3813\niter 352: loss 2.4460, time 4615.73ms, mfu 0.00%\nstep 353: train loss 2.3706, val loss 2.3728\nsaving checkpoint to out-shakespeare-char\niter 353: loss 2.4366, time 4847.83ms, mfu 0.00%\nstep 354: train loss 2.3715, val loss 2.3775\niter 354: loss 2.4199, time 4624.21ms, mfu 0.00%\nstep 355: train loss 2.3752, val loss 2.3788\niter 355: loss 2.3969, time 5417.72ms, mfu 0.00%\nstep 356: train loss 2.3672, val loss 2.3925\niter 356: loss 2.4193, time 4598.30ms, mfu 0.00%\nstep 357: train loss 2.3760, val loss 2.3861\niter 357: loss 2.3955, time 4724.82ms, mfu 0.00%\nstep 358: train loss 2.3629, val loss 2.3843\niter 358: loss 2.3711, time 4637.93ms, mfu 0.00%\nstep 359: train loss 2.3631, val loss 2.3849\niter 359: loss 2.3313, time 4826.35ms, mfu 0.00%\nstep 360: train loss 2.3667, val loss 2.3871\niter 360: loss 2.3553, time 4924.28ms, mfu 0.00%\nstep 361: train loss 2.3630, val loss 2.3859\niter 361: loss 2.3894, time 4649.89ms, mfu 0.00%\nstep 362: train loss 2.3726, val loss 2.3833\niter 362: loss 2.3814, time 5333.14ms, mfu 0.00%\nstep 363: train loss 2.3713, val loss 2.3851\niter 363: loss 2.3893, time 4753.56ms, mfu 0.00%\nstep 364: train loss 2.3705, val loss 2.3778\niter 364: loss 2.3391, time 4571.34ms, mfu 0.00%\nstep 365: train loss 2.3646, val loss 2.3718\nsaving checkpoint to out-shakespeare-char\niter 365: loss 2.4306, time 4680.02ms, mfu 0.00%\nstep 366: train loss 2.3557, val loss 2.3685\nsaving checkpoint to out-shakespeare-char\niter 366: loss 2.2958, time 4765.43ms, mfu 0.00%\nstep 367: train loss 2.3552, val loss 2.3590\nsaving checkpoint to out-shakespeare-char\niter 367: loss 2.3457, time 4735.82ms, mfu 0.00%\nstep 368: train loss 2.3579, val loss 2.3694\niter 368: loss 2.3725, time 4716.05ms, mfu 0.00%\nstep 369: train loss 2.3600, val loss 2.3716\niter 369: loss 2.2695, time 5331.17ms, mfu 0.00%\nstep 370: train loss 2.3669, val loss 2.3770\niter 370: loss 2.3775, time 4676.00ms, mfu 0.00%\nstep 371: train loss 2.3595, val loss 2.3795\niter 371: loss 2.3623, time 4609.98ms, mfu 0.00%\nstep 372: train loss 2.3641, val loss 2.3765\niter 372: loss 2.3421, time 4722.40ms, mfu 0.00%\nstep 373: train loss 2.3574, val loss 2.3649\niter 373: loss 2.4190, time 4613.91ms, mfu 0.00%\nstep 374: train loss 2.3591, val loss 2.3716\niter 374: loss 2.4088, time 4933.50ms, mfu 0.00%\nstep 375: train loss 2.3757, val loss 2.3906\niter 375: loss 2.3990, time 4884.96ms, mfu 0.00%\nstep 376: train loss 2.3692, val loss 2.3816\niter 376: loss 2.3797, time 5125.51ms, mfu 0.00%\nstep 377: train loss 2.3546, val loss 2.3663\niter 377: loss 2.3053, time 4587.41ms, mfu 0.00%\nstep 378: train loss 2.3548, val loss 2.3766\niter 378: loss 2.3136, time 4691.99ms, mfu 0.00%\nstep 379: train loss 2.3710, val loss 2.3769\niter 379: loss 2.3493, time 4643.76ms, mfu 0.00%\nstep 380: train loss 2.3653, val loss 2.3885\niter 380: loss 2.3377, time 4637.35ms, mfu 0.00%\nstep 381: train loss 2.3605, val loss 2.3750\niter 381: loss 2.3967, time 4625.22ms, mfu 0.00%\nstep 382: train loss 2.3542, val loss 2.3759\niter 382: loss 2.2779, time 5169.57ms, mfu 0.00%\nstep 383: train loss 2.3452, val loss 2.3660\niter 383: loss 2.3268, time 4777.38ms, mfu 0.00%\nstep 384: train loss 2.3565, val loss 2.3680\niter 384: loss 2.3028, time 4704.36ms, mfu 0.00%\nstep 385: train loss 2.3528, val loss 2.3643\niter 385: loss 2.3480, time 4603.46ms, mfu 0.00%\nstep 386: train loss 2.3557, val loss 2.3671\niter 386: loss 2.3332, time 4669.34ms, mfu 0.00%\nstep 387: train loss 2.3527, val loss 2.3625\niter 387: loss 2.4561, time 4659.92ms, mfu 0.00%\nstep 388: train loss 2.3414, val loss 2.3588\nsaving checkpoint to out-shakespeare-char\niter 388: loss 2.3449, time 4725.70ms, mfu 0.00%\nstep 389: train loss 2.3442, val loss 2.3599\niter 389: loss 2.4244, time 5184.61ms, mfu 0.00%\nstep 390: train loss 2.3512, val loss 2.3589\niter 390: loss 2.3186, time 4662.53ms, mfu 0.00%\nstep 391: train loss 2.3440, val loss 2.3525\nsaving checkpoint to out-shakespeare-char\niter 391: loss 2.3364, time 4733.28ms, mfu 0.00%\nstep 392: train loss 2.3318, val loss 2.3373\nsaving checkpoint to out-shakespeare-char\niter 392: loss 2.3858, time 4765.90ms, mfu 0.00%\nstep 393: train loss 2.3310, val loss 2.3439\niter 393: loss 2.3420, time 4663.80ms, mfu 0.00%\nstep 394: train loss 2.3446, val loss 2.3514\niter 394: loss 2.3883, time 4628.55ms, mfu 0.00%\nstep 395: train loss 2.3460, val loss 2.3528\niter 395: loss 2.3438, time 4734.21ms, mfu 0.00%\nstep 396: train loss 2.3447, val loss 2.3554\niter 396: loss 2.3936, time 5316.61ms, mfu 0.00%\nstep 397: train loss 2.3415, val loss 2.3452\niter 397: loss 2.3282, time 4988.00ms, mfu 0.00%\nstep 398: train loss 2.3340, val loss 2.3378\niter 398: loss 2.3457, time 4950.01ms, mfu 0.00%\nstep 399: train loss 2.3414, val loss 2.3432\niter 399: loss 2.3193, time 4868.72ms, mfu 0.00%\nstep 400: train loss 2.3360, val loss 2.3487\niter 400: loss 2.3843, time 4883.30ms, mfu 0.00%\nstep 401: train loss 2.3328, val loss 2.3426\niter 401: loss 2.3124, time 4725.76ms, mfu 0.00%\nstep 402: train loss 2.3326, val loss 2.3462\niter 402: loss 2.3303, time 5117.73ms, mfu 0.00%\nstep 403: train loss 2.3421, val loss 2.3526\niter 403: loss 2.3276, time 4720.84ms, mfu 0.00%\nstep 404: train loss 2.3391, val loss 2.3555\niter 404: loss 2.4670, time 4688.68ms, mfu 0.00%\nstep 405: train loss 2.3310, val loss 2.3476\niter 405: loss 2.3596, time 4720.88ms, mfu 0.00%\nstep 406: train loss 2.3347, val loss 2.3401\niter 406: loss 2.2640, time 4622.41ms, mfu 0.00%\nstep 407: train loss 2.3329, val loss 2.3382\niter 407: loss 2.3391, time 4694.90ms, mfu 0.00%\nstep 408: train loss 2.3352, val loss 2.3367\nsaving checkpoint to out-shakespeare-char\niter 408: loss 2.3015, time 4641.61ms, mfu 0.00%\nstep 409: train loss 2.3334, val loss 2.3354\nsaving checkpoint to out-shakespeare-char\niter 409: loss 2.3599, time 5317.08ms, mfu 0.00%\nstep 410: train loss 2.3394, val loss 2.3457\niter 410: loss 2.3970, time 4642.17ms, mfu 0.00%\nstep 411: train loss 2.3413, val loss 2.3352\nsaving checkpoint to out-shakespeare-char\niter 411: loss 2.3973, time 4690.48ms, mfu 0.00%\nstep 412: train loss 2.3325, val loss 2.3378\niter 412: loss 2.3372, time 4657.18ms, mfu 0.00%\nstep 413: train loss 2.3409, val loss 2.3389\niter 413: loss 2.3252, time 4703.55ms, mfu 0.00%\nstep 414: train loss 2.3412, val loss 2.3516\niter 414: loss 2.2974, time 4682.17ms, mfu 0.00%\nstep 415: train loss 2.3406, val loss 2.3450\niter 415: loss 2.3699, time 4586.68ms, mfu 0.00%\nstep 416: train loss 2.3364, val loss 2.3423\niter 416: loss 2.2391, time 5333.24ms, mfu 0.00%\nstep 417: train loss 2.3281, val loss 2.3362\niter 417: loss 2.2284, time 4629.18ms, mfu 0.00%\nstep 418: train loss 2.3203, val loss 2.3366\niter 418: loss 2.3081, time 4677.22ms, mfu 0.00%\nstep 419: train loss 2.3284, val loss 2.3385\niter 419: loss 2.3580, time 4689.19ms, mfu 0.00%\nstep 420: train loss 2.3242, val loss 2.3376\niter 420: loss 2.3424, time 4692.93ms, mfu 0.00%\nstep 421: train loss 2.3255, val loss 2.3374\niter 421: loss 2.2665, time 4621.45ms, mfu 0.00%\nstep 422: train loss 2.3205, val loss 2.3285\nsaving checkpoint to out-shakespeare-char\niter 422: loss 2.2720, time 4755.72ms, mfu 0.00%\nstep 423: train loss 2.3219, val loss 2.3308\niter 423: loss 2.3334, time 5358.34ms, mfu 0.00%\nstep 424: train loss 2.3271, val loss 2.3312\niter 424: loss 2.2861, time 4683.21ms, mfu 0.00%\nstep 425: train loss 2.3322, val loss 2.3283\nsaving checkpoint to out-shakespeare-char\niter 425: loss 2.2409, time 4519.07ms, mfu 0.00%\nstep 426: train loss 2.3299, val loss 2.3383\niter 426: loss 2.3128, time 4612.99ms, mfu 0.00%\nstep 427: train loss 2.3302, val loss 2.3359\niter 427: loss 2.3226, time 4600.69ms, mfu 0.00%\nstep 428: train loss 2.3345, val loss 2.3442\niter 428: loss 2.3438, time 4617.52ms, mfu 0.00%\nstep 429: train loss 2.3439, val loss 2.3418\niter 429: loss 2.3818, time 4725.97ms, mfu 0.00%\nstep 430: train loss 2.3250, val loss 2.3347\niter 430: loss 2.3717, time 5113.42ms, mfu 0.00%\nstep 431: train loss 2.3082, val loss 2.3217\nsaving checkpoint to out-shakespeare-char\niter 431: loss 2.3349, time 4532.34ms, mfu 0.00%\nstep 432: train loss 2.3231, val loss 2.3313\niter 432: loss 2.4055, time 4610.60ms, mfu 0.00%\nstep 433: train loss 2.3413, val loss 2.3532\niter 433: loss 2.4038, time 4656.12ms, mfu 0.00%\nstep 434: train loss 2.3541, val loss 2.3794\niter 434: loss 2.2718, time 4667.20ms, mfu 0.00%\nstep 435: train loss 2.3411, val loss 2.3726\niter 435: loss 2.4319, time 4848.04ms, mfu 0.00%\nstep 436: train loss 2.3194, val loss 2.3496\niter 436: loss 2.3355, time 5792.03ms, mfu 0.00%\nstep 437: train loss 2.3220, val loss 2.3392\niter 437: loss 2.2982, time 4989.98ms, mfu 0.00%\nstep 438: train loss 2.3231, val loss 2.3358\niter 438: loss 2.3664, time 5119.66ms, mfu 0.00%\nstep 439: train loss 2.3173, val loss 2.3290\niter 439: loss 2.2864, time 4924.62ms, mfu 0.00%\nstep 440: train loss 2.3127, val loss 2.3171\nsaving checkpoint to out-shakespeare-char\niter 440: loss 2.3927, time 4876.30ms, mfu 0.00%\nstep 441: train loss 2.3110, val loss 2.3190\niter 441: loss 2.2831, time 4766.57ms, mfu 0.00%\nstep 442: train loss 2.3082, val loss 2.3182\niter 442: loss 2.3212, time 4835.90ms, mfu 0.00%\nstep 443: train loss 2.3043, val loss 2.3292\niter 443: loss 2.3724, time 5694.73ms, mfu 0.00%\nstep 444: train loss 2.3160, val loss 2.3318\niter 444: loss 2.2674, time 4834.97ms, mfu 0.00%\nstep 445: train loss 2.3163, val loss 2.3326\niter 445: loss 2.2897, time 4883.53ms, mfu 0.00%\nstep 446: train loss 2.3161, val loss 2.3312\niter 446: loss 2.2731, time 4632.37ms, mfu 0.00%\nstep 447: train loss 2.3017, val loss 2.3251\niter 447: loss 2.2993, time 4718.26ms, mfu 0.00%\nstep 448: train loss 2.3087, val loss 2.3193\niter 448: loss 2.2874, time 4739.56ms, mfu 0.00%\nstep 449: train loss 2.3005, val loss 2.3144\nsaving checkpoint to out-shakespeare-char\niter 449: loss 2.3529, time 5310.40ms, mfu 0.00%\nstep 450: train loss 2.3035, val loss 2.3283\niter 450: loss 2.3520, time 4952.96ms, mfu 0.00%\nstep 451: train loss 2.3071, val loss 2.3338\niter 451: loss 2.2864, time 4846.69ms, mfu 0.00%\nstep 452: train loss 2.3083, val loss 2.3276\niter 452: loss 2.2195, time 4673.14ms, mfu 0.00%\nstep 453: train loss 2.3082, val loss 2.3235\niter 453: loss 2.2556, time 4843.73ms, mfu 0.00%\nstep 454: train loss 2.3067, val loss 2.3251\niter 454: loss 2.3479, time 4716.98ms, mfu 0.00%\nstep 455: train loss 2.3039, val loss 2.3296\niter 455: loss 2.3269, time 4812.55ms, mfu 0.00%\nstep 456: train loss 2.3026, val loss 2.3341\niter 456: loss 2.3055, time 5408.47ms, mfu 0.00%\nstep 457: train loss 2.3069, val loss 2.3276\niter 457: loss 2.4399, time 4830.18ms, mfu 0.00%\nstep 458: train loss 2.3058, val loss 2.3287\niter 458: loss 2.3289, time 4627.32ms, mfu 0.00%\nstep 459: train loss 2.2931, val loss 2.3183\niter 459: loss 2.2937, time 4746.44ms, mfu 0.00%\nstep 460: train loss 2.2988, val loss 2.3206\niter 460: loss 2.2329, time 4665.71ms, mfu 0.00%\nstep 461: train loss 2.3169, val loss 2.3291\niter 461: loss 2.3118, time 4699.20ms, mfu 0.00%\nstep 462: train loss 2.3130, val loss 2.3257\niter 462: loss 2.2998, time 4592.62ms, mfu 0.00%\nstep 463: train loss 2.3012, val loss 2.3198\niter 463: loss 2.2926, time 5213.49ms, mfu 0.00%\nstep 464: train loss 2.3033, val loss 2.3207\niter 464: loss 2.3400, time 4514.60ms, mfu 0.00%\nstep 465: train loss 2.3055, val loss 2.3245\niter 465: loss 2.2117, time 4612.65ms, mfu 0.00%\nstep 466: train loss 2.3009, val loss 2.3199\niter 466: loss 2.2456, time 4578.58ms, mfu 0.00%\nstep 467: train loss 2.2967, val loss 2.3150\niter 467: loss 2.2435, time 4670.69ms, mfu 0.00%\nstep 468: train loss 2.2948, val loss 2.3170\niter 468: loss 2.3428, time 4682.78ms, mfu 0.00%\nstep 469: train loss 2.2911, val loss 2.3279\niter 469: loss 2.2633, time 4602.03ms, mfu 0.00%\nstep 470: train loss 2.2938, val loss 2.3155\niter 470: loss 2.3522, time 5555.11ms, mfu 0.00%\nstep 471: train loss 2.2902, val loss 2.3172\niter 471: loss 2.2197, time 4938.63ms, mfu 0.00%\nstep 472: train loss 2.2986, val loss 2.3307\niter 472: loss 2.2660, time 4981.62ms, mfu 0.00%\nstep 473: train loss 2.2955, val loss 2.3261\niter 473: loss 2.2552, time 4739.98ms, mfu 0.00%\nstep 474: train loss 2.3028, val loss 2.3238\niter 474: loss 2.2760, time 4696.78ms, mfu 0.00%\nstep 475: train loss 2.2926, val loss 2.3140\nsaving checkpoint to out-shakespeare-char\niter 475: loss 2.3275, time 4797.81ms, mfu 0.00%\nstep 476: train loss 2.2908, val loss 2.3143\niter 476: loss 2.3525, time 5392.77ms, mfu 0.00%\nstep 477: train loss 2.2990, val loss 2.3126\nsaving checkpoint to out-shakespeare-char\niter 477: loss 2.2786, time 4739.58ms, mfu 0.00%\nstep 478: train loss 2.2874, val loss 2.3154\niter 478: loss 2.3715, time 4806.39ms, mfu 0.00%\nstep 479: train loss 2.2894, val loss 2.3094\nsaving checkpoint to out-shakespeare-char\niter 479: loss 2.2500, time 4742.41ms, mfu 0.00%\nstep 480: train loss 2.2922, val loss 2.3162\niter 480: loss 2.2656, time 4753.86ms, mfu 0.00%\nstep 481: train loss 2.3002, val loss 2.3261\niter 481: loss 2.2699, time 4791.78ms, mfu 0.00%\nstep 482: train loss 2.2926, val loss 2.3223\niter 482: loss 2.3339, time 4807.90ms, mfu 0.00%\nstep 483: train loss 2.2814, val loss 2.3118\niter 483: loss 2.3312, time 5432.32ms, mfu 0.00%\nstep 484: train loss 2.2890, val loss 2.3076\nsaving checkpoint to out-shakespeare-char\niter 484: loss 2.2162, time 4863.91ms, mfu 0.00%\nstep 485: train loss 2.2885, val loss 2.3133\niter 485: loss 2.2838, time 4690.22ms, mfu 0.00%\nstep 486: train loss 2.2962, val loss 2.3194\niter 486: loss 2.3021, time 4762.47ms, mfu 0.00%\nstep 487: train loss 2.2925, val loss 2.3243\niter 487: loss 2.3696, time 4717.39ms, mfu 0.00%\nstep 488: train loss 2.2904, val loss 2.3129\niter 488: loss 2.2911, time 4781.06ms, mfu 0.00%\nstep 489: train loss 2.2861, val loss 2.3141\niter 489: loss 2.2432, time 5112.54ms, mfu 0.00%\nstep 490: train loss 2.2854, val loss 2.3107\niter 490: loss 2.3973, time 5091.70ms, mfu 0.00%\nstep 491: train loss 2.2916, val loss 2.3092\niter 491: loss 2.3054, time 4673.07ms, mfu 0.00%\nstep 492: train loss 2.2849, val loss 2.3161\niter 492: loss 2.3602, time 4758.22ms, mfu 0.00%\nstep 493: train loss 2.2860, val loss 2.3144\niter 493: loss 2.2466, time 4712.05ms, mfu 0.00%\nstep 494: train loss 2.2830, val loss 2.3073\nsaving checkpoint to out-shakespeare-char\niter 494: loss 2.2947, time 4749.94ms, mfu 0.00%\nstep 495: train loss 2.2834, val loss 2.3087\niter 495: loss 2.2906, time 4654.47ms, mfu 0.00%\nstep 496: train loss 2.2980, val loss 2.3188\niter 496: loss 2.3272, time 5364.03ms, mfu 0.00%\nstep 497: train loss 2.2922, val loss 2.3237\niter 497: loss 2.2997, time 4758.02ms, mfu 0.00%\nstep 498: train loss 2.2814, val loss 2.3210\niter 498: loss 2.3668, time 4670.38ms, mfu 0.00%\nstep 499: train loss 2.2744, val loss 2.3020\nsaving checkpoint to out-shakespeare-char\niter 499: loss 2.3322, time 4732.06ms, mfu 0.00%\nstep 500: train loss 2.2753, val loss 2.3049\niter 500: loss 2.2721, time 4689.55ms, mfu 0.00%\nstep 501: train loss 2.2716, val loss 2.3110\niter 501: loss 2.2011, time 4704.50ms, mfu 0.00%\nstep 502: train loss 2.2745, val loss 2.2976\nsaving checkpoint to out-shakespeare-char\niter 502: loss 2.3015, time 5049.96ms, mfu 0.00%\nstep 503: train loss 2.2751, val loss 2.3044\niter 503: loss 2.3432, time 5639.76ms, mfu 0.00%\nstep 504: train loss 2.2834, val loss 2.2960\nsaving checkpoint to out-shakespeare-char\niter 504: loss 2.2586, time 5107.61ms, mfu 0.00%\nstep 505: train loss 2.2765, val loss 2.2964\niter 505: loss 2.2354, time 4980.25ms, mfu 0.00%\nstep 506: train loss 2.2785, val loss 2.2962\niter 506: loss 2.3356, time 4784.33ms, mfu 0.00%\nstep 507: train loss 2.2751, val loss 2.3031\niter 507: loss 2.2679, time 4797.10ms, mfu 0.00%\nstep 508: train loss 2.2651, val loss 2.2933\nsaving checkpoint to out-shakespeare-char\niter 508: loss 2.3091, time 4832.90ms, mfu 0.00%\nstep 509: train loss 2.2639, val loss 2.2930\nsaving checkpoint to out-shakespeare-char\niter 509: loss 2.2133, time 5465.18ms, mfu 0.00%\nstep 510: train loss 2.2684, val loss 2.2975\niter 510: loss 2.3239, time 4832.91ms, mfu 0.00%\nstep 511: train loss 2.2720, val loss 2.3068\niter 511: loss 2.2309, time 4774.33ms, mfu 0.00%\nstep 512: train loss 2.2774, val loss 2.2973\niter 512: loss 2.2458, time 4822.79ms, mfu 0.00%\nstep 513: train loss 2.2756, val loss 2.2993\niter 513: loss 2.2370, time 4773.90ms, mfu 0.00%\nstep 514: train loss 2.2686, val loss 2.2893\nsaving checkpoint to out-shakespeare-char\niter 514: loss 2.2492, time 4814.81ms, mfu 0.00%\nstep 515: train loss 2.2656, val loss 2.2887\nsaving checkpoint to out-shakespeare-char\niter 515: loss 2.2224, time 4881.11ms, mfu 0.00%\nstep 516: train loss 2.2787, val loss 2.2969\niter 516: loss 2.2815, time 5443.46ms, mfu 0.00%\nstep 517: train loss 2.2880, val loss 2.3105\niter 517: loss 2.3535, time 4836.78ms, mfu 0.00%\nstep 518: train loss 2.2894, val loss 2.3138\niter 518: loss 2.2978, time 4843.71ms, mfu 0.00%\nstep 519: train loss 2.2736, val loss 2.2938\niter 519: loss 2.2730, time 4794.29ms, mfu 0.00%\nstep 520: train loss 2.2731, val loss 2.2856\nsaving checkpoint to out-shakespeare-char\niter 520: loss 2.2654, time 4818.48ms, mfu 0.00%\nstep 521: train loss 2.2763, val loss 2.2902\niter 521: loss 2.3270, time 4834.92ms, mfu 0.00%\nstep 522: train loss 2.2695, val loss 2.2843\nsaving checkpoint to out-shakespeare-char\niter 522: loss 2.2762, time 5388.12ms, mfu 0.00%\nstep 523: train loss 2.2662, val loss 2.2789\nsaving checkpoint to out-shakespeare-char\niter 523: loss 2.2885, time 4908.31ms, mfu 0.00%\nstep 524: train loss 2.2627, val loss 2.2771\nsaving checkpoint to out-shakespeare-char\niter 524: loss 2.2952, time 4702.23ms, mfu 0.00%\nstep 525: train loss 2.2567, val loss 2.2692\nsaving checkpoint to out-shakespeare-char\niter 525: loss 2.2119, time 4809.48ms, mfu 0.00%\nstep 526: train loss 2.2547, val loss 2.2708\niter 526: loss 2.3033, time 4608.22ms, mfu 0.00%\nstep 527: train loss 2.2611, val loss 2.2713\niter 527: loss 2.1757, time 4745.17ms, mfu 0.00%\nstep 528: train loss 2.2555, val loss 2.2782\niter 528: loss 2.2575, time 4575.41ms, mfu 0.00%\nstep 529: train loss 2.2513, val loss 2.2692\nsaving checkpoint to out-shakespeare-char\niter 529: loss 2.2195, time 5412.92ms, mfu 0.00%\nstep 530: train loss 2.2502, val loss 2.2722\niter 530: loss 2.2495, time 4724.47ms, mfu 0.00%\nstep 531: train loss 2.2531, val loss 2.2726\niter 531: loss 2.2799, time 4681.32ms, mfu 0.00%\nstep 532: train loss 2.2451, val loss 2.2739\niter 532: loss 2.1897, time 4541.12ms, mfu 0.00%\nstep 533: train loss 2.2479, val loss 2.2702\niter 533: loss 2.2419, time 4702.43ms, mfu 0.00%\nstep 534: train loss 2.2467, val loss 2.2640\nsaving checkpoint to out-shakespeare-char\niter 534: loss 2.2761, time 4764.69ms, mfu 0.00%\nstep 535: train loss 2.2505, val loss 2.2736\niter 535: loss 2.2912, time 4909.87ms, mfu 0.00%\nstep 536: train loss 2.2470, val loss 2.2729\niter 536: loss 2.1995, time 5598.80ms, mfu 0.00%\nstep 537: train loss 2.2510, val loss 2.2623\nsaving checkpoint to out-shakespeare-char\niter 537: loss 2.3032, time 4838.22ms, mfu 0.00%\nstep 538: train loss 2.2550, val loss 2.2556\nsaving checkpoint to out-shakespeare-char\niter 538: loss 2.2457, time 4704.14ms, mfu 0.00%\nstep 539: train loss 2.2503, val loss 2.2648\niter 539: loss 2.2396, time 4710.18ms, mfu 0.00%\nstep 540: train loss 2.2573, val loss 2.2637\niter 540: loss 2.2108, time 4822.63ms, mfu 0.00%\nstep 541: train loss 2.2517, val loss 2.2610\niter 541: loss 2.2424, time 4905.64ms, mfu 0.00%\nstep 542: train loss 2.2513, val loss 2.2550\nsaving checkpoint to out-shakespeare-char\niter 542: loss 2.2975, time 4983.97ms, mfu 0.00%\nstep 543: train loss 2.2484, val loss 2.2499\nsaving checkpoint to out-shakespeare-char\niter 543: loss 2.2563, time 5145.22ms, mfu 0.00%\nstep 544: train loss 2.2419, val loss 2.2619\niter 544: loss 2.2058, time 4510.78ms, mfu 0.00%\nstep 545: train loss 2.2469, val loss 2.2639\niter 545: loss 2.2511, time 4503.56ms, mfu 0.00%\nstep 546: train loss 2.2403, val loss 2.2662\niter 546: loss 2.1871, time 4508.22ms, mfu 0.00%\nstep 547: train loss 2.2570, val loss 2.2633\niter 547: loss 2.1812, time 4482.31ms, mfu 0.00%\nstep 548: train loss 2.2523, val loss 2.2636\niter 548: loss 2.1077, time 4569.81ms, mfu 0.00%\nstep 549: train loss 2.2503, val loss 2.2702\niter 549: loss 2.2594, time 4819.55ms, mfu 0.00%\nstep 550: train loss 2.2518, val loss 2.2783\niter 550: loss 2.3102, time 4863.88ms, mfu 0.00%\nstep 551: train loss 2.2562, val loss 2.2953\niter 551: loss 2.3231, time 4558.41ms, mfu 0.00%\nstep 552: train loss 2.2618, val loss 2.2978\niter 552: loss 2.2514, time 4500.16ms, mfu 0.00%\nstep 553: train loss 2.2576, val loss 2.2910\niter 553: loss 2.2808, time 4391.55ms, mfu 0.00%\nstep 554: train loss 2.2516, val loss 2.2851\niter 554: loss 2.2214, time 4539.87ms, mfu 0.00%\nstep 555: train loss 2.2550, val loss 2.2875\niter 555: loss 2.2530, time 4448.97ms, mfu 0.00%\nstep 556: train loss 2.2529, val loss 2.2664\niter 556: loss 2.3496, time 4776.13ms, mfu 0.00%\nstep 557: train loss 2.2487, val loss 2.2578\niter 557: loss 2.2805, time 4720.80ms, mfu 0.00%\nstep 558: train loss 2.2417, val loss 2.2618\niter 558: loss 2.2810, time 4450.88ms, mfu 0.00%\nstep 559: train loss 2.2361, val loss 2.2545\niter 559: loss 2.2344, time 4419.10ms, mfu 0.00%\nstep 560: train loss 2.2344, val loss 2.2498\nsaving checkpoint to out-shakespeare-char\niter 560: loss 2.2989, time 4398.99ms, mfu 0.00%\nstep 561: train loss 2.2361, val loss 2.2534\niter 561: loss 2.2337, time 4455.86ms, mfu 0.00%\nstep 562: train loss 2.2350, val loss 2.2522\niter 562: loss 2.2540, time 4457.36ms, mfu 0.00%\nstep 563: train loss 2.2370, val loss 2.2590\niter 563: loss 2.1140, time 4433.27ms, mfu 0.00%\nstep 564: train loss 2.2339, val loss 2.2649\niter 564: loss 2.2444, time 5031.39ms, mfu 0.00%\nstep 565: train loss 2.2343, val loss 2.2640\niter 565: loss 2.1965, time 4544.66ms, mfu 0.00%\nstep 566: train loss 2.2407, val loss 2.2611\niter 566: loss 2.2696, time 4315.14ms, mfu 0.00%\nstep 567: train loss 2.2373, val loss 2.2670\niter 567: loss 2.3078, time 4515.15ms, mfu 0.00%\nstep 568: train loss 2.2255, val loss 2.2582\niter 568: loss 2.2142, time 4492.56ms, mfu 0.00%\nstep 569: train loss 2.2345, val loss 2.2641\niter 569: loss 2.2095, time 4650.70ms, mfu 0.00%\nstep 570: train loss 2.2374, val loss 2.2648\niter 570: loss 2.2153, time 4757.17ms, mfu 0.00%\nstep 571: train loss 2.2311, val loss 2.2599\niter 571: loss 2.2023, time 5417.47ms, mfu 0.00%\nstep 572: train loss 2.2311, val loss 2.2559\niter 572: loss 2.2349, time 4869.09ms, mfu 0.00%\nstep 573: train loss 2.2350, val loss 2.2554\niter 573: loss 2.2536, time 4834.52ms, mfu 0.00%\nstep 574: train loss 2.2387, val loss 2.2615\niter 574: loss 2.2742, time 4543.28ms, mfu 0.00%\nstep 575: train loss 2.2376, val loss 2.2572\niter 575: loss 2.2913, time 4557.90ms, mfu 0.00%\nstep 576: train loss 2.2299, val loss 2.2492\nsaving checkpoint to out-shakespeare-char\niter 576: loss 2.2952, time 4636.13ms, mfu 0.00%\nstep 577: train loss 2.2332, val loss 2.2472\nsaving checkpoint to out-shakespeare-char\niter 577: loss 2.2852, time 5061.68ms, mfu 0.00%\nstep 578: train loss 2.2278, val loss 2.2509\niter 578: loss 2.1917, time 4715.33ms, mfu 0.00%\nstep 579: train loss 2.2343, val loss 2.2508\niter 579: loss 2.1681, time 4575.63ms, mfu 0.00%\nstep 580: train loss 2.2338, val loss 2.2509\niter 580: loss 2.1715, time 4517.88ms, mfu 0.00%\nstep 581: train loss 2.2350, val loss 2.2470\nsaving checkpoint to out-shakespeare-char\niter 581: loss 2.1638, time 4533.26ms, mfu 0.00%\nstep 582: train loss 2.2304, val loss 2.2404\nsaving checkpoint to out-shakespeare-char\niter 582: loss 2.3142, time 4591.92ms, mfu 0.00%\nstep 583: train loss 2.2281, val loss 2.2489\niter 583: loss 2.2318, time 4586.18ms, mfu 0.00%\nstep 584: train loss 2.2281, val loss 2.2614\niter 584: loss 2.3034, time 5199.22ms, mfu 0.00%\nstep 585: train loss 2.2302, val loss 2.2471\niter 585: loss 2.1659, time 4497.27ms, mfu 0.00%\nstep 586: train loss 2.2284, val loss 2.2489\niter 586: loss 2.1789, time 4709.94ms, mfu 0.00%\nstep 587: train loss 2.2188, val loss 2.2476\niter 587: loss 2.2138, time 4488.99ms, mfu 0.00%\nstep 588: train loss 2.2180, val loss 2.2440\niter 588: loss 2.2246, time 4557.73ms, mfu 0.00%\nstep 589: train loss 2.2129, val loss 2.2429\niter 589: loss 2.2775, time 4623.50ms, mfu 0.00%\nstep 590: train loss 2.2038, val loss 2.2410\niter 590: loss 2.2150, time 4479.73ms, mfu 0.00%\nstep 591: train loss 2.2032, val loss 2.2435\niter 591: loss 2.2495, time 5351.29ms, mfu 0.00%\nstep 592: train loss 2.2110, val loss 2.2461\niter 592: loss 2.1888, time 4680.43ms, mfu 0.00%\nstep 593: train loss 2.2124, val loss 2.2419\niter 593: loss 2.2048, time 4576.12ms, mfu 0.00%\nstep 594: train loss 2.2103, val loss 2.2392\nsaving checkpoint to out-shakespeare-char\niter 594: loss 2.1958, time 4587.40ms, mfu 0.00%\nstep 595: train loss 2.2068, val loss 2.2335\nsaving checkpoint to out-shakespeare-char\niter 595: loss 2.1250, time 4730.29ms, mfu 0.00%\nstep 596: train loss 2.2113, val loss 2.2326\nsaving checkpoint to out-shakespeare-char\niter 596: loss 2.1938, time 4628.96ms, mfu 0.00%\nstep 597: train loss 2.2161, val loss 2.2523\niter 597: loss 2.2203, time 4611.58ms, mfu 0.00%\nstep 598: train loss 2.2343, val loss 2.2578\niter 598: loss 2.2476, time 5079.16ms, mfu 0.00%\nstep 599: train loss 2.2274, val loss 2.2531\niter 599: loss 2.2056, time 4477.95ms, mfu 0.00%\nstep 600: train loss 2.2213, val loss 2.2508\niter 600: loss 2.1770, time 4522.45ms, mfu 0.00%\nstep 601: train loss 2.2126, val loss 2.2458\niter 601: loss 2.2842, time 4625.47ms, mfu 0.00%\nstep 602: train loss 2.2063, val loss 2.2326\niter 602: loss 2.2435, time 4524.99ms, mfu 0.00%\nstep 603: train loss 2.2108, val loss 2.2291\nsaving checkpoint to out-shakespeare-char\niter 603: loss 2.2184, time 4451.10ms, mfu 0.00%\nstep 604: train loss 2.2126, val loss 2.2302\niter 604: loss 2.2370, time 4805.12ms, mfu 0.00%\nstep 605: train loss 2.2222, val loss 2.2391\niter 605: loss 2.2211, time 5489.68ms, mfu 0.00%\nstep 606: train loss 2.2230, val loss 2.2349\niter 606: loss 2.2350, time 4902.95ms, mfu 0.00%\nstep 607: train loss 2.2091, val loss 2.2353\niter 607: loss 2.3092, time 4627.16ms, mfu 0.00%\nstep 608: train loss 2.2107, val loss 2.2344\niter 608: loss 2.2736, time 4689.93ms, mfu 0.00%\nstep 609: train loss 2.2083, val loss 2.2257\nsaving checkpoint to out-shakespeare-char\niter 609: loss 2.1953, time 4526.02ms, mfu 0.00%\nstep 610: train loss 2.2012, val loss 2.2236\nsaving checkpoint to out-shakespeare-char\niter 610: loss 2.0856, time 4672.01ms, mfu 0.00%\nstep 611: train loss 2.1994, val loss 2.2302\niter 611: loss 2.3039, time 4491.61ms, mfu 0.00%\nstep 612: train loss 2.2076, val loss 2.2251\niter 612: loss 2.1544, time 5163.10ms, mfu 0.00%\nstep 613: train loss 2.1970, val loss 2.2265\niter 613: loss 2.2011, time 4595.90ms, mfu 0.00%\nstep 614: train loss 2.1899, val loss 2.2218\nsaving checkpoint to out-shakespeare-char\niter 614: loss 2.1545, time 4704.27ms, mfu 0.00%\nstep 615: train loss 2.1975, val loss 2.2181\nsaving checkpoint to out-shakespeare-char\niter 615: loss 2.1129, time 4624.79ms, mfu 0.00%\nstep 616: train loss 2.1962, val loss 2.2229\niter 616: loss 2.1599, time 4617.52ms, mfu 0.00%\nstep 617: train loss 2.2000, val loss 2.2203\niter 617: loss 2.2010, time 4454.48ms, mfu 0.00%\nstep 618: train loss 2.2091, val loss 2.2407\niter 618: loss 2.2072, time 4741.31ms, mfu 0.00%\nstep 619: train loss 2.2072, val loss 2.2317\niter 619: loss 2.2426, time 5169.60ms, mfu 0.00%\nstep 620: train loss 2.2118, val loss 2.2347\niter 620: loss 2.2218, time 4533.20ms, mfu 0.00%\nstep 621: train loss 2.2063, val loss 2.2302\niter 621: loss 2.1646, time 4615.02ms, mfu 0.00%\nstep 622: train loss 2.2042, val loss 2.2323\niter 622: loss 2.1916, time 4538.61ms, mfu 0.00%\nstep 623: train loss 2.2092, val loss 2.2376\niter 623: loss 2.1892, time 4535.06ms, mfu 0.00%\nstep 624: train loss 2.2092, val loss 2.2357\niter 624: loss 2.2283, time 4459.03ms, mfu 0.00%\nstep 625: train loss 2.2061, val loss 2.2349\niter 625: loss 2.1947, time 4479.37ms, mfu 0.00%\nstep 626: train loss 2.2053, val loss 2.2298\niter 626: loss 2.1745, time 5189.17ms, mfu 0.00%\nstep 627: train loss 2.2022, val loss 2.2253\niter 627: loss 2.1805, time 4593.95ms, mfu 0.00%\nstep 628: train loss 2.1948, val loss 2.2196\niter 628: loss 2.2311, time 4474.91ms, mfu 0.00%\nstep 629: train loss 2.1940, val loss 2.2075\nsaving checkpoint to out-shakespeare-char\niter 629: loss 2.1220, time 4640.42ms, mfu 0.00%\nstep 630: train loss 2.1884, val loss 2.2148\niter 630: loss 2.1638, time 4432.62ms, mfu 0.00%\nstep 631: train loss 2.1997, val loss 2.2315\niter 631: loss 2.3158, time 4491.47ms, mfu 0.00%\nstep 632: train loss 2.2010, val loss 2.2229\niter 632: loss 2.3169, time 4517.97ms, mfu 0.00%\nstep 633: train loss 2.2062, val loss 2.2329\niter 633: loss 2.1821, time 4965.41ms, mfu 0.00%\nstep 634: train loss 2.1964, val loss 2.2308\niter 634: loss 2.1438, time 4378.79ms, mfu 0.00%\nstep 635: train loss 2.1882, val loss 2.2193\niter 635: loss 2.1887, time 4399.10ms, mfu 0.00%\nstep 636: train loss 2.2008, val loss 2.2238\niter 636: loss 2.1599, time 4544.67ms, mfu 0.00%\nstep 637: train loss 2.1869, val loss 2.2226\niter 637: loss 2.1680, time 4559.75ms, mfu 0.00%\nstep 638: train loss 2.1882, val loss 2.2098\niter 638: loss 2.1407, time 4561.92ms, mfu 0.00%\nstep 639: train loss 2.1936, val loss 2.2155\niter 639: loss 2.2015, time 4446.54ms, mfu 0.00%\nstep 640: train loss 2.1930, val loss 2.2209\niter 640: loss 2.2646, time 5138.35ms, mfu 0.00%\nstep 641: train loss 2.1925, val loss 2.2197\niter 641: loss 2.2230, time 4475.30ms, mfu 0.00%\nstep 642: train loss 2.1892, val loss 2.2147\niter 642: loss 2.0892, time 4625.21ms, mfu 0.00%\nstep 643: train loss 2.1905, val loss 2.2208\niter 643: loss 2.2020, time 4449.18ms, mfu 0.00%\nstep 644: train loss 2.2006, val loss 2.2238\niter 644: loss 2.1508, time 4482.18ms, mfu 0.00%\nstep 645: train loss 2.1898, val loss 2.2227\niter 645: loss 2.2073, time 4447.99ms, mfu 0.00%\nstep 646: train loss 2.1910, val loss 2.2295\niter 646: loss 2.1478, time 4678.29ms, mfu 0.00%\nstep 647: train loss 2.1950, val loss 2.2235\niter 647: loss 2.2142, time 5320.39ms, mfu 0.00%\nstep 648: train loss 2.1840, val loss 2.2254\niter 648: loss 2.2417, time 4539.73ms, mfu 0.00%\nstep 649: train loss 2.1912, val loss 2.2210\niter 649: loss 2.1753, time 4556.59ms, mfu 0.00%\nstep 650: train loss 2.1769, val loss 2.2151\niter 650: loss 2.1638, time 4468.34ms, mfu 0.00%\nstep 651: train loss 2.1899, val loss 2.2096\niter 651: loss 2.0979, time 4505.78ms, mfu 0.00%\nstep 652: train loss 2.1837, val loss 2.2223\niter 652: loss 2.1483, time 4456.21ms, mfu 0.00%\nstep 653: train loss 2.1840, val loss 2.2067\nsaving checkpoint to out-shakespeare-char\niter 653: loss 2.2112, time 4422.31ms, mfu 0.00%\nstep 654: train loss 2.1838, val loss 2.2230\niter 654: loss 2.2123, time 5020.56ms, mfu 0.00%\nstep 655: train loss 2.1939, val loss 2.2260\niter 655: loss 2.1328, time 4355.27ms, mfu 0.00%\nstep 656: train loss 2.1874, val loss 2.2152\niter 656: loss 2.2429, time 4687.75ms, mfu 0.00%\nstep 657: train loss 2.1875, val loss 2.2199\niter 657: loss 2.1696, time 4517.72ms, mfu 0.00%\nstep 658: train loss 2.1870, val loss 2.2123\niter 658: loss 2.0956, time 4400.96ms, mfu 0.00%\nstep 659: train loss 2.1854, val loss 2.2142\niter 659: loss 2.1928, time 4500.88ms, mfu 0.00%\nstep 660: train loss 2.1819, val loss 2.2092\niter 660: loss 2.2012, time 4666.88ms, mfu 0.00%\nstep 661: train loss 2.1843, val loss 2.2046\nsaving checkpoint to out-shakespeare-char\niter 661: loss 2.2108, time 5162.68ms, mfu 0.00%\nstep 662: train loss 2.1743, val loss 2.2016\nsaving checkpoint to out-shakespeare-char\niter 662: loss 2.1284, time 4564.46ms, mfu 0.00%\nstep 663: train loss 2.1803, val loss 2.2124\niter 663: loss 2.2608, time 4360.30ms, mfu 0.00%\nstep 664: train loss 2.1862, val loss 2.2211\niter 664: loss 2.1086, time 4477.43ms, mfu 0.00%\nstep 665: train loss 2.1911, val loss 2.2202\niter 665: loss 2.1790, time 4447.29ms, mfu 0.00%\nstep 666: train loss 2.1858, val loss 2.2178\niter 666: loss 2.1794, time 4452.58ms, mfu 0.00%\nstep 667: train loss 2.1853, val loss 2.2130\niter 667: loss 2.1607, time 4393.59ms, mfu 0.00%\nstep 668: train loss 2.1775, val loss 2.2080\niter 668: loss 2.1338, time 5019.49ms, mfu 0.00%\nstep 669: train loss 2.1769, val loss 2.2096\niter 669: loss 2.1753, time 4372.61ms, mfu 0.00%\nstep 670: train loss 2.1752, val loss 2.2103\niter 670: loss 2.2320, time 4648.90ms, mfu 0.00%\nstep 671: train loss 2.1880, val loss 2.2160\niter 671: loss 2.1406, time 4818.55ms, mfu 0.00%\nstep 672: train loss 2.1874, val loss 2.2183\niter 672: loss 2.2091, time 4796.81ms, mfu 0.00%\nstep 673: train loss 2.1780, val loss 2.2033\niter 673: loss 2.1056, time 4834.87ms, mfu 0.00%\nstep 674: train loss 2.1732, val loss 2.1996\nsaving checkpoint to out-shakespeare-char\niter 674: loss 2.0813, time 4484.49ms, mfu 0.00%\nstep 675: train loss 2.1868, val loss 2.2141\niter 675: loss 2.1569, time 5216.40ms, mfu 0.00%\nstep 676: train loss 2.1869, val loss 2.2136\niter 676: loss 2.1970, time 4538.37ms, mfu 0.00%\nstep 677: train loss 2.1754, val loss 2.2008\niter 677: loss 2.2146, time 4627.63ms, mfu 0.00%\nstep 678: train loss 2.1602, val loss 2.2004\niter 678: loss 2.1791, time 4505.53ms, mfu 0.00%\nstep 679: train loss 2.1623, val loss 2.1928\nsaving checkpoint to out-shakespeare-char\niter 679: loss 2.1656, time 4555.60ms, mfu 0.00%\nstep 680: train loss 2.1621, val loss 2.1973\niter 680: loss 2.1875, time 4494.47ms, mfu 0.00%\nstep 681: train loss 2.1525, val loss 2.1869\nsaving checkpoint to out-shakespeare-char\niter 681: loss 2.0889, time 4586.70ms, mfu 0.00%\nstep 682: train loss 2.1546, val loss 2.1871\niter 682: loss 2.2371, time 5090.65ms, mfu 0.00%\nstep 683: train loss 2.1605, val loss 2.1876\niter 683: loss 2.2027, time 4618.27ms, mfu 0.00%\nstep 684: train loss 2.1652, val loss 2.1871\niter 684: loss 2.1713, time 4534.42ms, mfu 0.00%\nstep 685: train loss 2.1547, val loss 2.1867\nsaving checkpoint to out-shakespeare-char\niter 685: loss 2.1622, time 4556.82ms, mfu 0.00%\nstep 686: train loss 2.1589, val loss 2.1773\nsaving checkpoint to out-shakespeare-char\niter 686: loss 2.2267, time 4581.39ms, mfu 0.00%\nstep 687: train loss 2.1542, val loss 2.1824\niter 687: loss 2.1380, time 4350.06ms, mfu 0.00%\nstep 688: train loss 2.1500, val loss 2.1850\niter 688: loss 2.1716, time 4439.75ms, mfu 0.00%\nstep 689: train loss 2.1605, val loss 2.1874\niter 689: loss 2.1498, time 4981.75ms, mfu 0.00%\nstep 690: train loss 2.1489, val loss 2.1851\niter 690: loss 2.1876, time 4387.97ms, mfu 0.00%\nstep 691: train loss 2.1497, val loss 2.1805\niter 691: loss 2.1804, time 4330.72ms, mfu 0.00%\nstep 692: train loss 2.1552, val loss 2.1770\nsaving checkpoint to out-shakespeare-char\niter 692: loss 2.1397, time 4476.68ms, mfu 0.00%\nstep 693: train loss 2.1497, val loss 2.1866\niter 693: loss 2.1285, time 4405.92ms, mfu 0.00%\nstep 694: train loss 2.1483, val loss 2.1841\niter 694: loss 2.1439, time 4417.46ms, mfu 0.00%\nstep 695: train loss 2.1540, val loss 2.1760\nsaving checkpoint to out-shakespeare-char\niter 695: loss 2.1979, time 4390.82ms, mfu 0.00%\nstep 696: train loss 2.1522, val loss 2.1891\niter 696: loss 2.1947, time 5123.53ms, mfu 0.00%\nstep 697: train loss 2.1494, val loss 2.1906\niter 697: loss 2.1597, time 4504.16ms, mfu 0.00%\nstep 698: train loss 2.1466, val loss 2.1829\niter 698: loss 2.1030, time 4290.74ms, mfu 0.00%\nstep 699: train loss 2.1484, val loss 2.1821\niter 699: loss 2.1053, time 4406.18ms, mfu 0.00%\nstep 700: train loss 2.1450, val loss 2.1866\niter 700: loss 2.1081, time 4444.71ms, mfu 0.00%\nstep 701: train loss 2.1480, val loss 2.1900\niter 701: loss 2.2523, time 4429.66ms, mfu 0.00%\nstep 702: train loss 2.1465, val loss 2.1923\niter 702: loss 2.1149, time 4296.23ms, mfu 0.00%\nstep 703: train loss 2.1516, val loss 2.1901\niter 703: loss 2.1716, time 4947.16ms, mfu 0.00%\nstep 704: train loss 2.1469, val loss 2.1781\niter 704: loss 2.1186, time 4482.24ms, mfu 0.00%\nstep 705: train loss 2.1469, val loss 2.1767\niter 705: loss 2.2034, time 4766.71ms, mfu 0.00%\nstep 706: train loss 2.1427, val loss 2.1823\niter 706: loss 2.2478, time 4602.92ms, mfu 0.00%\nstep 707: train loss 2.1494, val loss 2.1792\niter 707: loss 2.1523, time 4751.31ms, mfu 0.00%\nstep 708: train loss 2.1479, val loss 2.1872\niter 708: loss 2.1508, time 4711.19ms, mfu 0.00%\nstep 709: train loss 2.1503, val loss 2.1800\niter 709: loss 2.2091, time 4437.36ms, mfu 0.00%\nstep 710: train loss 2.1429, val loss 2.1703\nsaving checkpoint to out-shakespeare-char\niter 710: loss 2.1013, time 5150.58ms, mfu 0.00%\nstep 711: train loss 2.1439, val loss 2.1788\niter 711: loss 2.1795, time 4462.37ms, mfu 0.00%\nstep 712: train loss 2.1420, val loss 2.1800\niter 712: loss 2.1378, time 4457.92ms, mfu 0.00%\nstep 713: train loss 2.1493, val loss 2.1801\niter 713: loss 2.2298, time 4477.26ms, mfu 0.00%\nstep 714: train loss 2.1549, val loss 2.1870\niter 714: loss 2.2007, time 4675.16ms, mfu 0.00%\nstep 715: train loss 2.1488, val loss 2.1847\niter 715: loss 2.1690, time 4389.45ms, mfu 0.00%\nstep 716: train loss 2.1505, val loss 2.1855\niter 716: loss 2.1363, time 4497.86ms, mfu 0.00%\nstep 717: train loss 2.1479, val loss 2.1867\niter 717: loss 2.1595, time 5231.38ms, mfu 0.00%\nstep 718: train loss 2.1467, val loss 2.1859\niter 718: loss 2.1038, time 4463.34ms, mfu 0.00%\nstep 719: train loss 2.1459, val loss 2.1800\niter 719: loss 2.2101, time 4432.43ms, mfu 0.00%\nstep 720: train loss 2.1455, val loss 2.1827\niter 720: loss 2.1863, time 4347.56ms, mfu 0.00%\nstep 721: train loss 2.1411, val loss 2.1871\niter 721: loss 2.1399, time 4497.47ms, mfu 0.00%\nstep 722: train loss 2.1397, val loss 2.1920\niter 722: loss 2.1954, time 4559.51ms, mfu 0.00%\nstep 723: train loss 2.1437, val loss 2.1841\niter 723: loss 2.0429, time 4421.63ms, mfu 0.00%\nstep 724: train loss 2.1366, val loss 2.1895\niter 724: loss 2.1546, time 5055.49ms, mfu 0.00%\nstep 725: train loss 2.1369, val loss 2.1834\niter 725: loss 2.1591, time 4512.76ms, mfu 0.00%\nstep 726: train loss 2.1331, val loss 2.1919\niter 726: loss 2.1597, time 4351.95ms, mfu 0.00%\nstep 727: train loss 2.1441, val loss 2.1929\niter 727: loss 2.1585, time 4477.87ms, mfu 0.00%\nstep 728: train loss 2.1458, val loss 2.2011\niter 728: loss 2.0433, time 4449.64ms, mfu 0.00%\nstep 729: train loss 2.1408, val loss 2.1946\niter 729: loss 2.1249, time 4442.63ms, mfu 0.00%\nstep 730: train loss 2.1352, val loss 2.1776\niter 730: loss 2.1219, time 4408.77ms, mfu 0.00%\nstep 731: train loss 2.1289, val loss 2.1774\niter 731: loss 2.1259, time 4676.52ms, mfu 0.00%\nstep 732: train loss 2.1396, val loss 2.1810\niter 732: loss 2.1358, time 4747.52ms, mfu 0.00%\nstep 733: train loss 2.1425, val loss 2.1884\niter 733: loss 2.0998, time 4345.89ms, mfu 0.00%\nstep 734: train loss 2.1455, val loss 2.1925\niter 734: loss 2.1009, time 4371.09ms, mfu 0.00%\nstep 735: train loss 2.1388, val loss 2.1882\niter 735: loss 2.1451, time 4371.85ms, mfu 0.00%\nstep 736: train loss 2.1276, val loss 2.1861\niter 736: loss 2.1530, time 4393.89ms, mfu 0.00%\nstep 737: train loss 2.1390, val loss 2.1814\niter 737: loss 2.1498, time 4267.03ms, mfu 0.00%\nstep 738: train loss 2.1393, val loss 2.1867\niter 738: loss 2.1847, time 4317.54ms, mfu 0.00%\nstep 739: train loss 2.1463, val loss 2.1824\niter 739: loss 2.1167, time 5097.30ms, mfu 0.00%\nstep 740: train loss 2.1393, val loss 2.1839\niter 740: loss 2.1798, time 4290.11ms, mfu 0.00%\nstep 741: train loss 2.1345, val loss 2.1791\niter 741: loss 2.1929, time 4502.54ms, mfu 0.00%\nstep 742: train loss 2.1367, val loss 2.1764\niter 742: loss 2.1251, time 4624.51ms, mfu 0.00%\nstep 743: train loss 2.1270, val loss 2.1693\nsaving checkpoint to out-shakespeare-char\niter 743: loss 2.2128, time 4775.61ms, mfu 0.00%\nstep 744: train loss 2.1426, val loss 2.1691\nsaving checkpoint to out-shakespeare-char\niter 744: loss 2.0833, time 4783.34ms, mfu 0.00%\nstep 745: train loss 2.1315, val loss 2.1680\nsaving checkpoint to out-shakespeare-char\niter 745: loss 2.1664, time 4814.22ms, mfu 0.00%\nstep 746: train loss 2.1396, val loss 2.1677\nsaving checkpoint to out-shakespeare-char\niter 746: loss 2.1099, time 5309.60ms, mfu 0.00%\nstep 747: train loss 2.1423, val loss 2.1681\niter 747: loss 2.1163, time 4580.02ms, mfu 0.00%\nstep 748: train loss 2.1347, val loss 2.1679\niter 748: loss 2.1149, time 4455.56ms, mfu 0.00%\nstep 749: train loss 2.1361, val loss 2.1675\nsaving checkpoint to out-shakespeare-char\niter 749: loss 2.1313, time 4617.89ms, mfu 0.00%\nstep 750: train loss 2.1282, val loss 2.1651\nsaving checkpoint to out-shakespeare-char\niter 750: loss 2.1991, time 4531.78ms, mfu 0.00%\nstep 751: train loss 2.1211, val loss 2.1594\nsaving checkpoint to out-shakespeare-char\niter 751: loss 2.1508, time 4603.21ms, mfu 0.00%\nstep 752: train loss 2.1214, val loss 2.1501\nsaving checkpoint to out-shakespeare-char\niter 752: loss 2.1293, time 4582.94ms, mfu 0.00%\nstep 753: train loss 2.1212, val loss 2.1622\niter 753: loss 2.1745, time 5027.05ms, mfu 0.00%\nstep 754: train loss 2.1280, val loss 2.1539\niter 754: loss 2.1321, time 4461.07ms, mfu 0.00%\nstep 755: train loss 2.1258, val loss 2.1565\niter 755: loss 2.1279, time 4480.73ms, mfu 0.00%\nstep 756: train loss 2.1275, val loss 2.1649\niter 756: loss 2.0958, time 4419.62ms, mfu 0.00%\nstep 757: train loss 2.1183, val loss 2.1595\niter 757: loss 2.0716, time 4397.12ms, mfu 0.00%\nstep 758: train loss 2.1136, val loss 2.1617\niter 758: loss 2.0949, time 4484.30ms, mfu 0.00%\nstep 759: train loss 2.1182, val loss 2.1499\nsaving checkpoint to out-shakespeare-char\niter 759: loss 2.0990, time 4481.35ms, mfu 0.00%\nstep 760: train loss 2.1164, val loss 2.1565\niter 760: loss 2.1218, time 5123.92ms, mfu 0.00%\nstep 761: train loss 2.1121, val loss 2.1596\niter 761: loss 2.1388, time 4425.81ms, mfu 0.00%\nstep 762: train loss 2.1139, val loss 2.1537\niter 762: loss 2.1303, time 4492.43ms, mfu 0.00%\nstep 763: train loss 2.1127, val loss 2.1563\niter 763: loss 2.1257, time 4476.68ms, mfu 0.00%\nstep 764: train loss 2.1043, val loss 2.1500\niter 764: loss 2.1237, time 4428.07ms, mfu 0.00%\nstep 765: train loss 2.1029, val loss 2.1440\nsaving checkpoint to out-shakespeare-char\niter 765: loss 2.0368, time 4391.06ms, mfu 0.00%\nstep 766: train loss 2.1007, val loss 2.1512\niter 766: loss 2.0644, time 4492.75ms, mfu 0.00%\nstep 767: train loss 2.1020, val loss 2.1496\niter 767: loss 2.0920, time 5067.15ms, mfu 0.00%\nstep 768: train loss 2.0974, val loss 2.1542\niter 768: loss 2.0428, time 4321.02ms, mfu 0.00%\nstep 769: train loss 2.1007, val loss 2.1509\niter 769: loss 2.0530, time 4437.41ms, mfu 0.00%\nstep 770: train loss 2.1029, val loss 2.1521\niter 770: loss 2.2021, time 4388.34ms, mfu 0.00%\nstep 771: train loss 2.0980, val loss 2.1449\niter 771: loss 2.1070, time 4348.46ms, mfu 0.00%\nstep 772: train loss 2.1075, val loss 2.1563\niter 772: loss 2.0007, time 4357.82ms, mfu 0.00%\nstep 773: train loss 2.1097, val loss 2.1557\niter 773: loss 2.0262, time 4434.04ms, mfu 0.00%\nstep 774: train loss 2.1116, val loss 2.1512\niter 774: loss 2.0819, time 5061.47ms, mfu 0.00%\nstep 775: train loss 2.1039, val loss 2.1500\niter 775: loss 2.0948, time 4376.92ms, mfu 0.00%\nstep 776: train loss 2.1070, val loss 2.1437\nsaving checkpoint to out-shakespeare-char\niter 776: loss 2.1561, time 4360.74ms, mfu 0.00%\nstep 777: train loss 2.1083, val loss 2.1458\niter 777: loss 2.0689, time 4336.35ms, mfu 0.00%\nstep 778: train loss 2.1110, val loss 2.1599\niter 778: loss 2.0382, time 4324.43ms, mfu 0.00%\nstep 779: train loss 2.1089, val loss 2.1582\niter 779: loss 2.0807, time 4259.56ms, mfu 0.00%\nstep 780: train loss 2.1014, val loss 2.1539\niter 780: loss 2.1438, time 4552.70ms, mfu 0.00%\nstep 781: train loss 2.1073, val loss 2.1481\niter 781: loss 2.0689, time 4728.77ms, mfu 0.00%\nstep 782: train loss 2.1065, val loss 2.1404\nsaving checkpoint to out-shakespeare-char\niter 782: loss 2.0511, time 4446.80ms, mfu 0.00%\nstep 783: train loss 2.0992, val loss 2.1432\niter 783: loss 2.1670, time 4371.88ms, mfu 0.00%\nstep 784: train loss 2.1070, val loss 2.1468\niter 784: loss 2.1800, time 4307.04ms, mfu 0.00%\nstep 785: train loss 2.1021, val loss 2.1545\niter 785: loss 2.0298, time 4378.18ms, mfu 0.00%\nstep 786: train loss 2.1066, val loss 2.1579\niter 786: loss 2.1082, time 4613.93ms, mfu 0.00%\nstep 787: train loss 2.1019, val loss 2.1542\niter 787: loss 2.0711, time 4761.14ms, mfu 0.00%\nstep 788: train loss 2.1024, val loss 2.1563\niter 788: loss 2.0667, time 5083.12ms, mfu 0.00%\nstep 789: train loss 2.1019, val loss 2.1637\niter 789: loss 2.0823, time 5063.72ms, mfu 0.00%\nstep 790: train loss 2.1059, val loss 2.1515\niter 790: loss 2.1256, time 4630.10ms, mfu 0.00%\nstep 791: train loss 2.0976, val loss 2.1394\nsaving checkpoint to out-shakespeare-char\niter 791: loss 2.1635, time 4513.47ms, mfu 0.00%\nstep 792: train loss 2.0902, val loss 2.1389\nsaving checkpoint to out-shakespeare-char\niter 792: loss 2.1443, time 4440.29ms, mfu 0.00%\nstep 793: train loss 2.0990, val loss 2.1409\niter 793: loss 2.1478, time 4512.86ms, mfu 0.00%\nstep 794: train loss 2.0984, val loss 2.1401\niter 794: loss 2.0779, time 4575.16ms, mfu 0.00%\nstep 795: train loss 2.0879, val loss 2.1418\niter 795: loss 2.0506, time 4795.92ms, mfu 0.00%\nstep 796: train loss 2.0923, val loss 2.1436\niter 796: loss 2.0986, time 4652.40ms, mfu 0.00%\nstep 797: train loss 2.1005, val loss 2.1507\niter 797: loss 2.0491, time 4464.31ms, mfu 0.00%\nstep 798: train loss 2.1036, val loss 2.1505\niter 798: loss 2.1586, time 4385.69ms, mfu 0.00%\nstep 799: train loss 2.1082, val loss 2.1571\niter 799: loss 2.2401, time 4481.14ms, mfu 0.00%\nstep 800: train loss 2.1049, val loss 2.1571\niter 800: loss 2.1353, time 4503.55ms, mfu 0.00%\nstep 801: train loss 2.1010, val loss 2.1555\niter 801: loss 2.0832, time 4511.07ms, mfu 0.00%\nstep 802: train loss 2.1033, val loss 2.1532\niter 802: loss 2.0485, time 4734.18ms, mfu 0.00%\nstep 803: train loss 2.1102, val loss 2.1538\niter 803: loss 2.1220, time 4903.04ms, mfu 0.00%\nstep 804: train loss 2.1056, val loss 2.1566\niter 804: loss 2.1112, time 4478.29ms, mfu 0.00%\nstep 805: train loss 2.1052, val loss 2.1601\niter 805: loss 2.1982, time 4508.55ms, mfu 0.00%\nstep 806: train loss 2.1132, val loss 2.1518\niter 806: loss 2.0784, time 4451.08ms, mfu 0.00%\nstep 807: train loss 2.1108, val loss 2.1540\niter 807: loss 2.0850, time 4467.36ms, mfu 0.00%\nstep 808: train loss 2.1111, val loss 2.1491\niter 808: loss 2.2238, time 4544.54ms, mfu 0.00%\nstep 809: train loss 2.1090, val loss 2.1500\niter 809: loss 2.1454, time 4606.91ms, mfu 0.00%\nstep 810: train loss 2.1063, val loss 2.1485\niter 810: loss 2.1295, time 5200.75ms, mfu 0.00%\nstep 811: train loss 2.1121, val loss 2.1448\niter 811: loss 2.1318, time 4306.04ms, mfu 0.00%\nstep 812: train loss 2.0989, val loss 2.1461\niter 812: loss 2.0872, time 4218.40ms, mfu 0.00%\nstep 813: train loss 2.1016, val loss 2.1365\nsaving checkpoint to out-shakespeare-char\niter 813: loss 2.0420, time 4509.56ms, mfu 0.00%\nstep 814: train loss 2.0874, val loss 2.1349\nsaving checkpoint to out-shakespeare-char\niter 814: loss 2.1005, time 4279.10ms, mfu 0.00%\nstep 815: train loss 2.0836, val loss 2.1342\nsaving checkpoint to out-shakespeare-char\niter 815: loss 2.0496, time 4455.19ms, mfu 0.00%\nstep 816: train loss 2.0932, val loss 2.1354\niter 816: loss 2.0404, time 4292.13ms, mfu 0.00%\nstep 817: train loss 2.0983, val loss 2.1407\niter 817: loss 2.1074, time 4884.64ms, mfu 0.00%\nstep 818: train loss 2.0870, val loss 2.1454\niter 818: loss 2.1597, time 4378.29ms, mfu 0.00%\nstep 819: train loss 2.0873, val loss 2.1381\niter 819: loss 1.9890, time 4222.14ms, mfu 0.00%\nstep 820: train loss 2.0858, val loss 2.1337\nsaving checkpoint to out-shakespeare-char\niter 820: loss 2.1036, time 4351.35ms, mfu 0.00%\nstep 821: train loss 2.0882, val loss 2.1333\nsaving checkpoint to out-shakespeare-char\niter 821: loss 2.0430, time 4366.66ms, mfu 0.00%\nstep 822: train loss 2.0859, val loss 2.1428\niter 822: loss 2.1245, time 4450.91ms, mfu 0.00%\nstep 823: train loss 2.0790, val loss 2.1386\niter 823: loss 2.0990, time 4324.87ms, mfu 0.00%\nstep 824: train loss 2.0811, val loss 2.1271\nsaving checkpoint to out-shakespeare-char\niter 824: loss 2.0456, time 4994.94ms, mfu 0.00%\nstep 825: train loss 2.0916, val loss 2.1356\niter 825: loss 2.0880, time 4326.77ms, mfu 0.00%\nstep 826: train loss 2.0926, val loss 2.1411\niter 826: loss 2.0803, time 4346.46ms, mfu 0.00%\nstep 827: train loss 2.0990, val loss 2.1376\niter 827: loss 2.1318, time 4305.52ms, mfu 0.00%\nstep 828: train loss 2.0928, val loss 2.1424\niter 828: loss 2.1021, time 4325.19ms, mfu 0.00%\nstep 829: train loss 2.0845, val loss 2.1401\niter 829: loss 2.1250, time 4401.16ms, mfu 0.00%\nstep 830: train loss 2.0922, val loss 2.1363\niter 830: loss 2.0993, time 4391.98ms, mfu 0.00%\nstep 831: train loss 2.0865, val loss 2.1390\niter 831: loss 2.1580, time 5109.24ms, mfu 0.00%\nstep 832: train loss 2.0936, val loss 2.1268\nsaving checkpoint to out-shakespeare-char\niter 832: loss 2.1201, time 4724.10ms, mfu 0.00%\nstep 833: train loss 2.0847, val loss 2.1366\niter 833: loss 2.0726, time 4769.37ms, mfu 0.00%\nstep 834: train loss 2.0782, val loss 2.1285\niter 834: loss 2.1019, time 4671.35ms, mfu 0.00%\nstep 835: train loss 2.0776, val loss 2.1193\nsaving checkpoint to out-shakespeare-char\niter 835: loss 2.1158, time 4777.29ms, mfu 0.00%\nstep 836: train loss 2.0758, val loss 2.1149\nsaving checkpoint to out-shakespeare-char\niter 836: loss 2.0227, time 4474.52ms, mfu 0.00%\nstep 837: train loss 2.0718, val loss 2.1175\niter 837: loss 2.0994, time 4461.50ms, mfu 0.00%\nstep 838: train loss 2.0687, val loss 2.1195\niter 838: loss 2.0679, time 5222.01ms, mfu 0.00%\nstep 839: train loss 2.0751, val loss 2.1177\niter 839: loss 2.0865, time 4432.53ms, mfu 0.00%\nstep 840: train loss 2.0744, val loss 2.1267\niter 840: loss 1.9920, time 4469.32ms, mfu 0.00%\nstep 841: train loss 2.0744, val loss 2.1189\niter 841: loss 1.9827, time 4435.52ms, mfu 0.00%\nstep 842: train loss 2.0787, val loss 2.1269\niter 842: loss 1.9937, time 4411.37ms, mfu 0.00%\nstep 843: train loss 2.0714, val loss 2.1229\niter 843: loss 2.1075, time 4400.56ms, mfu 0.00%\nstep 844: train loss 2.0741, val loss 2.1287\niter 844: loss 1.9556, time 4469.17ms, mfu 0.00%\nstep 845: train loss 2.0813, val loss 2.1330\niter 845: loss 2.0375, time 5135.32ms, mfu 0.00%\nstep 846: train loss 2.0802, val loss 2.1383\niter 846: loss 2.1290, time 4593.66ms, mfu 0.00%\nstep 847: train loss 2.0875, val loss 2.1452\niter 847: loss 2.1353, time 4384.86ms, mfu 0.00%\nstep 848: train loss 2.0778, val loss 2.1443\niter 848: loss 2.0005, time 4495.13ms, mfu 0.00%\nstep 849: train loss 2.0858, val loss 2.1374\niter 849: loss 2.0232, time 4468.35ms, mfu 0.00%\nstep 850: train loss 2.0758, val loss 2.1292\niter 850: loss 1.9814, time 4477.65ms, mfu 0.00%\nstep 851: train loss 2.0676, val loss 2.1206\niter 851: loss 2.0949, time 4346.08ms, mfu 0.00%\nstep 852: train loss 2.0698, val loss 2.1201\niter 852: loss 2.0776, time 5022.73ms, mfu 0.00%\nstep 853: train loss 2.0686, val loss 2.1293\niter 853: loss 2.1009, time 4630.11ms, mfu 0.00%\nstep 854: train loss 2.0700, val loss 2.1216\niter 854: loss 2.0583, time 4619.01ms, mfu 0.00%\nstep 855: train loss 2.0713, val loss 2.1226\niter 855: loss 2.1147, time 4556.98ms, mfu 0.00%\nstep 856: train loss 2.0653, val loss 2.1169\niter 856: loss 1.9153, time 4589.39ms, mfu 0.00%\nstep 857: train loss 2.0686, val loss 2.1182\niter 857: loss 2.0499, time 4723.51ms, mfu 0.00%\nstep 858: train loss 2.0683, val loss 2.1302\niter 858: loss 2.0196, time 4614.73ms, mfu 0.00%\nstep 859: train loss 2.0645, val loss 2.1198\niter 859: loss 1.9952, time 5452.12ms, mfu 0.00%\nstep 860: train loss 2.0647, val loss 2.1180\niter 860: loss 1.9918, time 4606.06ms, mfu 0.00%\nstep 861: train loss 2.0681, val loss 2.1203\niter 861: loss 2.0505, time 4546.91ms, mfu 0.00%\nstep 862: train loss 2.0836, val loss 2.1275\niter 862: loss 2.0403, time 4598.20ms, mfu 0.00%\nstep 863: train loss 2.0808, val loss 2.1286\niter 863: loss 2.0424, time 4628.11ms, mfu 0.00%\nstep 864: train loss 2.0678, val loss 2.1206\niter 864: loss 2.0709, time 4520.87ms, mfu 0.00%\nstep 865: train loss 2.0689, val loss 2.1265\niter 865: loss 1.9869, time 4621.48ms, mfu 0.00%\nstep 866: train loss 2.0675, val loss 2.1231\niter 866: loss 2.0368, time 5151.67ms, mfu 0.00%\nstep 867: train loss 2.0745, val loss 2.1250\niter 867: loss 2.1399, time 4463.15ms, mfu 0.00%\nstep 868: train loss 2.0775, val loss 2.1302\niter 868: loss 2.0516, time 4460.17ms, mfu 0.00%\nstep 869: train loss 2.0670, val loss 2.1257\niter 869: loss 2.1320, time 4599.31ms, mfu 0.00%\nstep 870: train loss 2.0720, val loss 2.1262\niter 870: loss 2.0017, time 4814.79ms, mfu 0.00%\nstep 871: train loss 2.0724, val loss 2.1266\niter 871: loss 2.0494, time 4764.48ms, mfu 0.00%\nstep 872: train loss 2.0733, val loss 2.1246\niter 872: loss 2.1410, time 4873.61ms, mfu 0.00%\nstep 873: train loss 2.0729, val loss 2.1302\niter 873: loss 2.0970, time 5308.01ms, mfu 0.00%\nstep 874: train loss 2.0690, val loss 2.1123\nsaving checkpoint to out-shakespeare-char\niter 874: loss 2.0424, time 4622.07ms, mfu 0.00%\nstep 875: train loss 2.0647, val loss 2.1144\niter 875: loss 2.1098, time 4637.64ms, mfu 0.00%\nstep 876: train loss 2.0658, val loss 2.1136\niter 876: loss 2.1065, time 4733.37ms, mfu 0.00%\nstep 877: train loss 2.0787, val loss 2.1203\niter 877: loss 2.0579, time 4575.91ms, mfu 0.00%\nstep 878: train loss 2.0758, val loss 2.1273\niter 878: loss 2.1609, time 4664.69ms, mfu 0.00%\nstep 879: train loss 2.0710, val loss 2.1237\niter 879: loss 2.0618, time 4540.23ms, mfu 0.00%\nstep 880: train loss 2.0675, val loss 2.1203\niter 880: loss 2.0956, time 5266.73ms, mfu 0.00%\nstep 881: train loss 2.0689, val loss 2.1177\niter 881: loss 2.1459, time 4572.31ms, mfu 0.00%\nstep 882: train loss 2.0624, val loss 2.1264\niter 882: loss 2.1272, time 4617.46ms, mfu 0.00%\nstep 883: train loss 2.0687, val loss 2.1240\niter 883: loss 2.0538, time 4660.31ms, mfu 0.00%\nstep 884: train loss 2.0658, val loss 2.1169\niter 884: loss 2.1075, time 4660.42ms, mfu 0.00%\nstep 885: train loss 2.0669, val loss 2.1235\niter 885: loss 2.1462, time 4574.08ms, mfu 0.00%\nstep 886: train loss 2.0660, val loss 2.1190\niter 886: loss 1.9860, time 4625.04ms, mfu 0.00%\nstep 887: train loss 2.0514, val loss 2.1144\niter 887: loss 2.0908, time 5215.37ms, mfu 0.00%\nstep 888: train loss 2.0600, val loss 2.1093\nsaving checkpoint to out-shakespeare-char\niter 888: loss 2.0014, time 4680.48ms, mfu 0.00%\nstep 889: train loss 2.0590, val loss 2.1087\nsaving checkpoint to out-shakespeare-char\niter 889: loss 2.0873, time 4669.64ms, mfu 0.00%\nstep 890: train loss 2.0605, val loss 2.1061\nsaving checkpoint to out-shakespeare-char\niter 890: loss 2.0319, time 4617.99ms, mfu 0.00%\nstep 891: train loss 2.0553, val loss 2.1092\niter 891: loss 2.0142, time 4662.70ms, mfu 0.00%\nstep 892: train loss 2.0596, val loss 2.1193\niter 892: loss 2.0358, time 4623.77ms, mfu 0.00%\nstep 893: train loss 2.0579, val loss 2.1085\niter 893: loss 2.0083, time 4987.50ms, mfu 0.00%\nstep 894: train loss 2.0636, val loss 2.1222\niter 894: loss 2.0055, time 4780.98ms, mfu 0.00%\nstep 895: train loss 2.0528, val loss 2.1181\niter 895: loss 2.1147, time 4671.23ms, mfu 0.00%\nstep 896: train loss 2.0492, val loss 2.1212\niter 896: loss 2.0832, time 4544.33ms, mfu 0.00%\nstep 897: train loss 2.0447, val loss 2.1207\niter 897: loss 2.0650, time 4730.65ms, mfu 0.00%\nstep 898: train loss 2.0513, val loss 2.1174\niter 898: loss 2.1070, time 4391.65ms, mfu 0.00%\nstep 899: train loss 2.0509, val loss 2.1203\niter 899: loss 2.0185, time 4584.10ms, mfu 0.00%\nstep 900: train loss 2.0439, val loss 2.1133\niter 900: loss 2.0931, time 5176.90ms, mfu 0.00%\nstep 901: train loss 2.0569, val loss 2.1158\niter 901: loss 2.0369, time 4619.44ms, mfu 0.00%\nstep 902: train loss 2.0500, val loss 2.1127\niter 902: loss 2.0261, time 4419.44ms, mfu 0.00%\nstep 903: train loss 2.0397, val loss 2.1124\niter 903: loss 2.0608, time 4520.98ms, mfu 0.00%\nstep 904: train loss 2.0415, val loss 2.1014\nsaving checkpoint to out-shakespeare-char\niter 904: loss 2.1055, time 4545.85ms, mfu 0.00%\nstep 905: train loss 2.0483, val loss 2.0989\nsaving checkpoint to out-shakespeare-char\niter 905: loss 2.1110, time 4575.65ms, mfu 0.00%\nstep 906: train loss 2.0429, val loss 2.1079\niter 906: loss 2.0689, time 4538.77ms, mfu 0.00%\nstep 907: train loss 2.0519, val loss 2.1116\niter 907: loss 2.1037, time 5159.92ms, mfu 0.00%\nstep 908: train loss 2.0469, val loss 2.1149\niter 908: loss 2.0254, time 4627.41ms, mfu 0.00%\nstep 909: train loss 2.0507, val loss 2.1126\niter 909: loss 2.0559, time 4603.15ms, mfu 0.00%\nstep 910: train loss 2.0527, val loss 2.1176\niter 910: loss 1.9993, time 4633.49ms, mfu 0.00%\nstep 911: train loss 2.0463, val loss 2.1142\niter 911: loss 2.0821, time 4486.36ms, mfu 0.00%\nstep 912: train loss 2.0448, val loss 2.1064\niter 912: loss 1.9039, time 4438.70ms, mfu 0.00%\nstep 913: train loss 2.0446, val loss 2.1024\niter 913: loss 2.1143, time 4612.66ms, mfu 0.00%\nstep 914: train loss 2.0378, val loss 2.0974\nsaving checkpoint to out-shakespeare-char\niter 914: loss 2.0587, time 5522.35ms, mfu 0.00%\nstep 915: train loss 2.0451, val loss 2.1086\niter 915: loss 2.0852, time 4778.02ms, mfu 0.00%\nstep 916: train loss 2.0376, val loss 2.1013\niter 916: loss 2.1209, time 4808.89ms, mfu 0.00%\nstep 917: train loss 2.0395, val loss 2.1048\niter 917: loss 2.0410, time 4651.62ms, mfu 0.00%\nstep 918: train loss 2.0465, val loss 2.1005\niter 918: loss 2.0874, time 4660.79ms, mfu 0.00%\nstep 919: train loss 2.0439, val loss 2.1044\niter 919: loss 2.0715, time 4618.74ms, mfu 0.00%\nstep 920: train loss 2.0464, val loss 2.1077\niter 920: loss 2.1183, time 4579.09ms, mfu 0.00%\nstep 921: train loss 2.0456, val loss 2.1101\niter 921: loss 2.0260, time 5267.87ms, mfu 0.00%\nstep 922: train loss 2.0356, val loss 2.1006\niter 922: loss 2.0483, time 4611.95ms, mfu 0.00%\nstep 923: train loss 2.0377, val loss 2.0976\niter 923: loss 2.0163, time 4750.11ms, mfu 0.00%\nstep 924: train loss 2.0335, val loss 2.0974\niter 924: loss 2.0545, time 4620.81ms, mfu 0.00%\nstep 925: train loss 2.0367, val loss 2.1006\niter 925: loss 2.0305, time 4602.73ms, mfu 0.00%\nstep 926: train loss 2.0302, val loss 2.0992\niter 926: loss 2.0665, time 4598.77ms, mfu 0.00%\nstep 927: train loss 2.0471, val loss 2.1012\niter 927: loss 2.0945, time 4709.78ms, mfu 0.00%\nstep 928: train loss 2.0317, val loss 2.1026\niter 928: loss 1.9666, time 5267.81ms, mfu 0.00%\nstep 929: train loss 2.0346, val loss 2.0983\niter 929: loss 2.0893, time 4755.44ms, mfu 0.00%\nstep 930: train loss 2.0451, val loss 2.1034\niter 930: loss 2.0638, time 4626.51ms, mfu 0.00%\nstep 931: train loss 2.0389, val loss 2.0959\nsaving checkpoint to out-shakespeare-char\niter 931: loss 2.0055, time 4627.96ms, mfu 0.00%\nstep 932: train loss 2.0339, val loss 2.0934\nsaving checkpoint to out-shakespeare-char\niter 932: loss 2.0807, time 4705.20ms, mfu 0.00%\nstep 933: train loss 2.0341, val loss 2.0869\nsaving checkpoint to out-shakespeare-char\niter 933: loss 1.9553, time 4645.27ms, mfu 0.00%\nstep 934: train loss 2.0304, val loss 2.0961\niter 934: loss 1.9838, time 4614.70ms, mfu 0.00%\nstep 935: train loss 2.0367, val loss 2.0895\niter 935: loss 2.1108, time 5335.01ms, mfu 0.00%\nstep 936: train loss 2.0274, val loss 2.1036\niter 936: loss 2.1504, time 4545.29ms, mfu 0.00%\nstep 937: train loss 2.0301, val loss 2.0965\niter 937: loss 1.9619, time 4650.85ms, mfu 0.00%\nstep 938: train loss 2.0437, val loss 2.0941\niter 938: loss 2.0185, time 4529.48ms, mfu 0.00%\nstep 939: train loss 2.0329, val loss 2.0980\niter 939: loss 1.9554, time 4523.13ms, mfu 0.00%\nstep 940: train loss 2.0396, val loss 2.1023\niter 940: loss 2.0728, time 4603.32ms, mfu 0.00%\nstep 941: train loss 2.0316, val loss 2.0998\niter 941: loss 2.0451, time 4653.30ms, mfu 0.00%\nstep 942: train loss 2.0304, val loss 2.0944\niter 942: loss 2.0462, time 5430.27ms, mfu 0.00%\nstep 943: train loss 2.0240, val loss 2.0972\niter 943: loss 2.0556, time 4965.74ms, mfu 0.00%\nstep 944: train loss 2.0256, val loss 2.1018\niter 944: loss 1.9812, time 4738.32ms, mfu 0.00%\nstep 945: train loss 2.0253, val loss 2.0994\niter 945: loss 2.0987, time 4623.11ms, mfu 0.00%\nstep 946: train loss 2.0311, val loss 2.1005\niter 946: loss 2.0249, time 4577.50ms, mfu 0.00%\nstep 947: train loss 2.0295, val loss 2.0995\niter 947: loss 1.9992, time 4572.52ms, mfu 0.00%\nstep 948: train loss 2.0329, val loss 2.0985\niter 948: loss 1.8815, time 5268.60ms, mfu 0.00%\nstep 949: train loss 2.0302, val loss 2.1048\niter 949: loss 2.0987, time 4552.70ms, mfu 0.00%\nstep 950: train loss 2.0287, val loss 2.0883\niter 950: loss 2.0223, time 4700.95ms, mfu 0.00%\nstep 951: train loss 2.0237, val loss 2.0963\niter 951: loss 2.0259, time 4524.37ms, mfu 0.00%\nstep 952: train loss 2.0188, val loss 2.0937\niter 952: loss 2.0402, time 4576.19ms, mfu 0.00%\nstep 953: train loss 2.0197, val loss 2.0948\niter 953: loss 1.9755, time 4599.32ms, mfu 0.00%\nstep 954: train loss 2.0238, val loss 2.0984\niter 954: loss 2.0013, time 4582.12ms, mfu 0.00%\nstep 955: train loss 2.0205, val loss 2.0886\niter 955: loss 1.9782, time 5213.33ms, mfu 0.00%\nstep 956: train loss 2.0172, val loss 2.0942\niter 956: loss 2.0580, time 4787.58ms, mfu 0.00%\nstep 957: train loss 2.0141, val loss 2.0899\niter 957: loss 2.0218, time 4554.86ms, mfu 0.00%\nstep 958: train loss 2.0248, val loss 2.0987\niter 958: loss 2.0267, time 4640.37ms, mfu 0.00%\nstep 959: train loss 2.0227, val loss 2.0971\niter 959: loss 2.1014, time 4530.16ms, mfu 0.00%\nstep 960: train loss 2.0260, val loss 2.1037\niter 960: loss 2.0049, time 4510.75ms, mfu 0.00%\nstep 961: train loss 2.0318, val loss 2.0984\niter 961: loss 2.0129, time 4606.82ms, mfu 0.00%\nstep 962: train loss 2.0210, val loss 2.1041\niter 962: loss 2.0039, time 5046.66ms, mfu 0.00%\nstep 963: train loss 2.0250, val loss 2.1000\niter 963: loss 1.9328, time 4596.51ms, mfu 0.00%\nstep 964: train loss 2.0179, val loss 2.1034\niter 964: loss 1.9791, time 4524.97ms, mfu 0.00%\nstep 965: train loss 2.0208, val loss 2.0936\niter 965: loss 1.9886, time 4560.51ms, mfu 0.00%\nstep 966: train loss 2.0169, val loss 2.0921\niter 966: loss 2.0735, time 4497.95ms, mfu 0.00%\nstep 967: train loss 2.0218, val loss 2.0868\nsaving checkpoint to out-shakespeare-char\niter 967: loss 2.0415, time 4607.04ms, mfu 0.00%\nstep 968: train loss 2.0153, val loss 2.0863\nsaving checkpoint to out-shakespeare-char\niter 968: loss 2.0193, time 4729.97ms, mfu 0.00%\nstep 969: train loss 2.0155, val loss 2.0849\nsaving checkpoint to out-shakespeare-char\niter 969: loss 2.0262, time 5562.59ms, mfu 0.00%\nstep 970: train loss 2.0198, val loss 2.1025\niter 970: loss 1.9985, time 4568.73ms, mfu 0.00%\nstep 971: train loss 2.0234, val loss 2.0987\niter 971: loss 2.1032, time 4689.55ms, mfu 0.00%\nstep 972: train loss 2.0058, val loss 2.0933\niter 972: loss 1.9380, time 4697.95ms, mfu 0.00%\nstep 973: train loss 2.0039, val loss 2.0855\niter 973: loss 2.0557, time 4603.18ms, mfu 0.00%\nstep 974: train loss 2.0083, val loss 2.0912\niter 974: loss 2.0424, time 4573.47ms, mfu 0.00%\nstep 975: train loss 2.0136, val loss 2.0797\nsaving checkpoint to out-shakespeare-char\niter 975: loss 2.2150, time 4602.19ms, mfu 0.00%\nstep 976: train loss 2.0122, val loss 2.0864\niter 976: loss 1.9916, time 5193.03ms, mfu 0.00%\nstep 977: train loss 2.0183, val loss 2.0838\niter 977: loss 2.1349, time 4597.66ms, mfu 0.00%\nstep 978: train loss 2.0263, val loss 2.0872\niter 978: loss 1.9906, time 4563.45ms, mfu 0.00%\nstep 979: train loss 2.0208, val loss 2.0844\niter 979: loss 2.0605, time 4549.19ms, mfu 0.00%\nstep 980: train loss 2.0236, val loss 2.0888\niter 980: loss 1.9783, time 4547.36ms, mfu 0.00%\nstep 981: train loss 2.0222, val loss 2.0891\niter 981: loss 2.0519, time 4434.15ms, mfu 0.00%\nstep 982: train loss 2.0126, val loss 2.0866\niter 982: loss 1.9921, time 4564.25ms, mfu 0.00%\nstep 983: train loss 2.0078, val loss 2.0741\nsaving checkpoint to out-shakespeare-char\niter 983: loss 2.0231, time 5241.61ms, mfu 0.00%\nstep 984: train loss 1.9991, val loss 2.0786\niter 984: loss 2.0622, time 4837.10ms, mfu 0.00%\nstep 985: train loss 2.0053, val loss 2.0798\niter 985: loss 2.0015, time 4864.62ms, mfu 0.00%\nstep 986: train loss 2.0102, val loss 2.0810\niter 986: loss 2.0133, time 4694.18ms, mfu 0.00%\nstep 987: train loss 2.0092, val loss 2.0819\niter 987: loss 1.9811, time 4513.63ms, mfu 0.00%\nstep 988: train loss 2.0091, val loss 2.0889\niter 988: loss 2.0923, time 4634.54ms, mfu 0.00%\nstep 989: train loss 2.0230, val loss 2.0791\niter 989: loss 2.1069, time 4659.21ms, mfu 0.00%\nstep 990: train loss 2.0268, val loss 2.0950\niter 990: loss 2.0872, time 5150.29ms, mfu 0.00%\nstep 991: train loss 2.0215, val loss 2.0939\niter 991: loss 2.0717, time 4585.40ms, mfu 0.00%\nstep 992: train loss 2.0178, val loss 2.0938\niter 992: loss 2.0064, time 4604.23ms, mfu 0.00%\nstep 993: train loss 2.0203, val loss 2.0816\niter 993: loss 1.9878, time 4801.17ms, mfu 0.00%\nstep 994: train loss 2.0131, val loss 2.0804\niter 994: loss 2.0072, time 4526.54ms, mfu 0.00%\nstep 995: train loss 2.0072, val loss 2.0894\niter 995: loss 2.0706, time 4603.13ms, mfu 0.00%\nstep 996: train loss 2.0153, val loss 2.0906\niter 996: loss 1.9921, time 4734.13ms, mfu 0.00%\nstep 997: train loss 2.0113, val loss 2.0733\nsaving checkpoint to out-shakespeare-char\niter 997: loss 1.9203, time 4926.34ms, mfu 0.00%\nstep 998: train loss 1.9918, val loss 2.0755\niter 998: loss 1.9480, time 4393.81ms, mfu 0.00%\nstep 999: train loss 1.9961, val loss 2.0755\niter 999: loss 1.9979, time 4649.19ms, mfu 0.00%\nstep 1000: train loss 2.0132, val loss 2.0766\niter 1000: loss 1.9687, time 4483.30ms, mfu 0.00%\nstep 1001: train loss 2.0045, val loss 2.0808\niter 1001: loss 2.0228, time 4520.58ms, mfu 0.00%\nstep 1002: train loss 2.0022, val loss 2.0810\niter 1002: loss 2.0628, time 4526.37ms, mfu 0.00%\nstep 1003: train loss 2.0103, val loss 2.0788\niter 1003: loss 2.0095, time 4951.90ms, mfu 0.00%\nstep 1004: train loss 2.0033, val loss 2.0678\nsaving checkpoint to out-shakespeare-char\niter 1004: loss 1.9738, time 5043.14ms, mfu 0.00%\nstep 1005: train loss 2.0064, val loss 2.0810\niter 1005: loss 2.0146, time 4681.37ms, mfu 0.00%\nstep 1006: train loss 2.0145, val loss 2.0760\niter 1006: loss 2.0346, time 4627.39ms, mfu 0.00%\nstep 1007: train loss 2.0227, val loss 2.0911\niter 1007: loss 1.9798, time 4680.74ms, mfu 0.00%\nstep 1008: train loss 2.0192, val loss 2.0868\niter 1008: loss 1.9897, time 4559.07ms, mfu 0.00%\nstep 1009: train loss 2.0135, val loss 2.0758\niter 1009: loss 1.9072, time 4664.94ms, mfu 0.00%\nstep 1010: train loss 2.0099, val loss 2.0740\niter 1010: loss 2.0641, time 5261.66ms, mfu 0.00%\nstep 1011: train loss 2.0124, val loss 2.0662\nsaving checkpoint to out-shakespeare-char\niter 1011: loss 2.0459, time 4588.12ms, mfu 0.00%\nstep 1012: train loss 2.0059, val loss 2.0738\niter 1012: loss 1.9620, time 4650.02ms, mfu 0.00%\nstep 1013: train loss 2.0069, val loss 2.0779\niter 1013: loss 2.0148, time 4608.12ms, mfu 0.00%\nstep 1014: train loss 2.0022, val loss 2.0773\niter 1014: loss 1.9569, time 4629.57ms, mfu 0.00%\nstep 1015: train loss 1.9996, val loss 2.0712\niter 1015: loss 1.9687, time 4532.72ms, mfu 0.00%\nstep 1016: train loss 1.9924, val loss 2.0662\niter 1016: loss 2.0375, time 4555.53ms, mfu 0.00%\nstep 1017: train loss 1.9961, val loss 2.0655\nsaving checkpoint to out-shakespeare-char\niter 1017: loss 1.9026, time 5091.03ms, mfu 0.00%\nstep 1018: train loss 1.9918, val loss 2.0732\niter 1018: loss 2.0189, time 4553.92ms, mfu 0.00%\nstep 1019: train loss 1.9976, val loss 2.0783\niter 1019: loss 1.9321, time 4681.95ms, mfu 0.00%\nstep 1020: train loss 2.0034, val loss 2.0770\niter 1020: loss 2.0646, time 4688.88ms, mfu 0.00%\nstep 1021: train loss 2.0144, val loss 2.0764\niter 1021: loss 2.0645, time 4595.96ms, mfu 0.00%\nstep 1022: train loss 2.0023, val loss 2.0810\niter 1022: loss 2.1159, time 4655.87ms, mfu 0.00%\nstep 1023: train loss 2.0022, val loss 2.0760\niter 1023: loss 2.0298, time 4578.59ms, mfu 0.00%\nstep 1024: train loss 2.0009, val loss 2.0774\niter 1024: loss 2.0175, time 5470.55ms, mfu 0.00%\nstep 1025: train loss 2.0123, val loss 2.0662\niter 1025: loss 1.9865, time 4840.82ms, mfu 0.00%\nstep 1026: train loss 2.0029, val loss 2.0686\niter 1026: loss 1.9132, time 4779.34ms, mfu 0.00%\nstep 1027: train loss 2.0021, val loss 2.0687\niter 1027: loss 2.0390, time 4729.45ms, mfu 0.00%\nstep 1028: train loss 1.9929, val loss 2.0688\niter 1028: loss 1.9321, time 4659.45ms, mfu 0.00%\nstep 1029: train loss 2.0001, val loss 2.0701\niter 1029: loss 1.9491, time 4621.42ms, mfu 0.00%\nstep 1030: train loss 1.9994, val loss 2.0731\niter 1030: loss 1.9809, time 4640.86ms, mfu 0.00%\nstep 1031: train loss 1.9952, val loss 2.0716\niter 1031: loss 1.9503, time 5374.27ms, mfu 0.00%\nstep 1032: train loss 1.9981, val loss 2.0736\niter 1032: loss 2.0237, time 4647.49ms, mfu 0.00%\nstep 1033: train loss 2.0004, val loss 2.0736\niter 1033: loss 1.9816, time 4672.31ms, mfu 0.00%\nstep 1034: train loss 1.9948, val loss 2.0676\niter 1034: loss 1.9776, time 4584.42ms, mfu 0.00%\nstep 1035: train loss 1.9986, val loss 2.0679\niter 1035: loss 2.0338, time 4608.92ms, mfu 0.00%\nstep 1036: train loss 1.9984, val loss 2.0680\niter 1036: loss 1.9540, time 4654.99ms, mfu 0.00%\nstep 1037: train loss 1.9947, val loss 2.0664\niter 1037: loss 2.0310, time 4666.42ms, mfu 0.00%\nstep 1038: train loss 1.9983, val loss 2.0688\niter 1038: loss 1.9924, time 5085.69ms, mfu 0.00%\nstep 1039: train loss 1.9995, val loss 2.0635\nsaving checkpoint to out-shakespeare-char\niter 1039: loss 2.0900, time 4635.75ms, mfu 0.00%\nstep 1040: train loss 1.9925, val loss 2.0579\nsaving checkpoint to out-shakespeare-char\niter 1040: loss 2.0806, time 4420.37ms, mfu 0.00%\nstep 1041: train loss 1.9841, val loss 2.0586\niter 1041: loss 1.8923, time 4464.31ms, mfu 0.00%\nstep 1042: train loss 1.9854, val loss 2.0407\nsaving checkpoint to out-shakespeare-char\niter 1042: loss 1.9980, time 4407.12ms, mfu 0.00%\nstep 1043: train loss 1.9983, val loss 2.0501\niter 1043: loss 1.9073, time 4357.33ms, mfu 0.00%\nstep 1044: train loss 1.9922, val loss 2.0566\niter 1044: loss 1.9833, time 4515.69ms, mfu 0.00%\nstep 1045: train loss 1.9926, val loss 2.0557\niter 1045: loss 1.9738, time 5179.58ms, mfu 0.00%\nstep 1046: train loss 1.9894, val loss 2.0708\niter 1046: loss 2.0959, time 4660.44ms, mfu 0.00%\nstep 1047: train loss 2.0000, val loss 2.0604\niter 1047: loss 1.9413, time 4601.52ms, mfu 0.00%\nstep 1048: train loss 1.9880, val loss 2.0620\niter 1048: loss 2.0450, time 4600.63ms, mfu 0.00%\nstep 1049: train loss 1.9838, val loss 2.0559\niter 1049: loss 2.0222, time 4601.73ms, mfu 0.00%\nstep 1050: train loss 1.9853, val loss 2.0550\niter 1050: loss 2.0064, time 4691.96ms, mfu 0.00%\nstep 1051: train loss 1.9845, val loss 2.0512\niter 1051: loss 1.9876, time 4691.05ms, mfu 0.00%\nstep 1052: train loss 1.9835, val loss 2.0565\niter 1052: loss 1.9182, time 5005.60ms, mfu 0.00%\nstep 1053: train loss 1.9975, val loss 2.0587\niter 1053: loss 1.9089, time 4479.02ms, mfu 0.00%\nstep 1054: train loss 1.9931, val loss 2.0582\niter 1054: loss 1.9598, time 4486.28ms, mfu 0.00%\nstep 1055: train loss 1.9813, val loss 2.0573\niter 1055: loss 1.9994, time 4491.29ms, mfu 0.00%\nstep 1056: train loss 1.9885, val loss 2.0535\niter 1056: loss 1.9548, time 4527.23ms, mfu 0.00%\nstep 1057: train loss 1.9873, val loss 2.0539\niter 1057: loss 1.9396, time 4553.02ms, mfu 0.00%\nstep 1058: train loss 1.9851, val loss 2.0505\niter 1058: loss 2.0112, time 5099.21ms, mfu 0.00%\nstep 1059: train loss 1.9832, val loss 2.0527\niter 1059: loss 1.9755, time 4641.32ms, mfu 0.00%\nstep 1060: train loss 1.9720, val loss 2.0524\niter 1060: loss 1.9976, time 4688.72ms, mfu 0.00%\nstep 1061: train loss 1.9785, val loss 2.0464\niter 1061: loss 1.9693, time 4718.60ms, mfu 0.00%\nstep 1062: train loss 1.9777, val loss 2.0510\niter 1062: loss 2.0162, time 4874.46ms, mfu 0.00%\nstep 1063: train loss 1.9749, val loss 2.0486\niter 1063: loss 2.1041, time 4961.97ms, mfu 0.00%\nstep 1064: train loss 1.9839, val loss 2.0432\niter 1064: loss 2.0063, time 4888.30ms, mfu 0.00%\nstep 1065: train loss 1.9734, val loss 2.0509\niter 1065: loss 1.9278, time 5283.79ms, mfu 0.00%\nstep 1066: train loss 1.9889, val loss 2.0579\niter 1066: loss 1.9293, time 4567.29ms, mfu 0.00%\nstep 1067: train loss 1.9862, val loss 2.0624\niter 1067: loss 2.0469, time 4636.95ms, mfu 0.00%\nstep 1068: train loss 1.9819, val loss 2.0626\niter 1068: loss 2.0208, time 4638.98ms, mfu 0.00%\nstep 1069: train loss 1.9815, val loss 2.0601\niter 1069: loss 1.9485, time 4650.35ms, mfu 0.00%\nstep 1070: train loss 1.9746, val loss 2.0502\niter 1070: loss 2.0941, time 4565.70ms, mfu 0.00%\nstep 1071: train loss 1.9793, val loss 2.0557\niter 1071: loss 1.9011, time 4665.70ms, mfu 0.00%\nstep 1072: train loss 1.9796, val loss 2.0492\niter 1072: loss 2.0042, time 5239.44ms, mfu 0.00%\nstep 1073: train loss 1.9845, val loss 2.0544\niter 1073: loss 2.0494, time 4607.46ms, mfu 0.00%\nstep 1074: train loss 1.9777, val loss 2.0556\niter 1074: loss 1.9531, time 4549.32ms, mfu 0.00%\nstep 1075: train loss 1.9783, val loss 2.0552\niter 1075: loss 2.0023, time 4570.66ms, mfu 0.00%\nstep 1076: train loss 1.9718, val loss 2.0551\niter 1076: loss 1.9108, time 4619.32ms, mfu 0.00%\nstep 1077: train loss 1.9716, val loss 2.0659\niter 1077: loss 1.9787, time 4488.57ms, mfu 0.00%\nstep 1078: train loss 1.9852, val loss 2.0610\niter 1078: loss 1.9495, time 4528.61ms, mfu 0.00%\nstep 1079: train loss 1.9843, val loss 2.0598\niter 1079: loss 2.0044, time 5262.98ms, mfu 0.00%\nstep 1080: train loss 1.9833, val loss 2.0551\niter 1080: loss 2.0831, time 4481.65ms, mfu 0.00%\nstep 1081: train loss 1.9771, val loss 2.0456\niter 1081: loss 2.0471, time 4533.63ms, mfu 0.00%\nstep 1082: train loss 1.9794, val loss 2.0508\niter 1082: loss 2.0768, time 4596.90ms, mfu 0.00%\nstep 1083: train loss 1.9840, val loss 2.0485\niter 1083: loss 1.8750, time 4521.89ms, mfu 0.00%\nstep 1084: train loss 1.9802, val loss 2.0380\nsaving checkpoint to out-shakespeare-char\niter 1084: loss 2.0555, time 4781.44ms, mfu 0.00%\nstep 1085: train loss 1.9701, val loss 2.0514\niter 1085: loss 2.0115, time 4536.69ms, mfu 0.00%\nstep 1086: train loss 1.9812, val loss 2.0434\niter 1086: loss 1.9972, time 5489.26ms, mfu 0.00%\nstep 1087: train loss 1.9859, val loss 2.0505\niter 1087: loss 2.0504, time 4486.07ms, mfu 0.00%\nstep 1088: train loss 1.9796, val loss 2.0446\niter 1088: loss 1.9634, time 4456.10ms, mfu 0.00%\nstep 1089: train loss 1.9821, val loss 2.0493\niter 1089: loss 2.0327, time 4489.38ms, mfu 0.00%\nstep 1090: train loss 1.9815, val loss 2.0523\niter 1090: loss 2.0056, time 4654.25ms, mfu 0.00%\nstep 1091: train loss 1.9847, val loss 2.0394\niter 1091: loss 2.0076, time 4641.02ms, mfu 0.00%\nstep 1092: train loss 1.9755, val loss 2.0354\nsaving checkpoint to out-shakespeare-char\niter 1092: loss 1.9830, time 4615.74ms, mfu 0.00%\nstep 1093: train loss 1.9817, val loss 2.0420\niter 1093: loss 1.9320, time 5090.18ms, mfu 0.00%\nstep 1094: train loss 1.9665, val loss 2.0335\nsaving checkpoint to out-shakespeare-char\niter 1094: loss 1.8896, time 4571.11ms, mfu 0.00%\nstep 1095: train loss 1.9761, val loss 2.0364\niter 1095: loss 2.0796, time 4673.17ms, mfu 0.00%\nstep 1096: train loss 1.9720, val loss 2.0478\niter 1096: loss 1.9668, time 4670.20ms, mfu 0.00%\nstep 1097: train loss 1.9760, val loss 2.0453\niter 1097: loss 1.9290, time 5014.24ms, mfu 0.00%\nstep 1098: train loss 1.9753, val loss 2.0504\niter 1098: loss 1.9174, time 4897.66ms, mfu 0.00%\nstep 1099: train loss 1.9755, val loss 2.0512\niter 1099: loss 1.8994, time 5536.07ms, mfu 0.00%\nstep 1100: train loss 1.9663, val loss 2.0432\niter 1100: loss 1.9706, time 4879.68ms, mfu 0.00%\nstep 1101: train loss 1.9684, val loss 2.0434\niter 1101: loss 1.9099, time 4733.98ms, mfu 0.00%\nstep 1102: train loss 1.9749, val loss 2.0407\niter 1102: loss 1.8412, time 4630.49ms, mfu 0.00%\nstep 1103: train loss 1.9632, val loss 2.0476\niter 1103: loss 1.9732, time 4690.08ms, mfu 0.00%\nstep 1104: train loss 1.9645, val loss 2.0526\niter 1104: loss 1.9058, time 4606.34ms, mfu 0.00%\nstep 1105: train loss 1.9671, val loss 2.0517\niter 1105: loss 1.9421, time 4655.59ms, mfu 0.00%\nstep 1106: train loss 1.9699, val loss 2.0541\niter 1106: loss 1.9878, time 5264.73ms, mfu 0.00%\nstep 1107: train loss 1.9786, val loss 2.0601\niter 1107: loss 1.9331, time 4691.06ms, mfu 0.00%\nstep 1108: train loss 1.9756, val loss 2.0564\niter 1108: loss 1.9108, time 4671.81ms, mfu 0.00%\nstep 1109: train loss 1.9707, val loss 2.0576\niter 1109: loss 2.0342, time 4743.66ms, mfu 0.00%\nstep 1110: train loss 1.9658, val loss 2.0579\niter 1110: loss 1.9465, time 5082.30ms, mfu 0.00%\nstep 1111: train loss 1.9705, val loss 2.0515\niter 1111: loss 2.1161, time 4796.11ms, mfu 0.00%\nstep 1112: train loss 1.9643, val loss 2.0483\niter 1112: loss 2.0016, time 4840.33ms, mfu 0.00%\nstep 1113: train loss 1.9655, val loss 2.0509\niter 1113: loss 1.9947, time 5522.64ms, mfu 0.00%\nstep 1114: train loss 1.9698, val loss 2.0503\niter 1114: loss 1.9046, time 4673.86ms, mfu 0.00%\nstep 1115: train loss 1.9606, val loss 2.0412\niter 1115: loss 1.9304, time 4738.92ms, mfu 0.00%\nstep 1116: train loss 1.9593, val loss 2.0414\niter 1116: loss 2.0343, time 4662.63ms, mfu 0.00%\nstep 1117: train loss 1.9687, val loss 2.0392\niter 1117: loss 1.9459, time 4809.70ms, mfu 0.00%\nstep 1118: train loss 1.9506, val loss 2.0413\niter 1118: loss 2.0685, time 4789.28ms, mfu 0.00%\nstep 1119: train loss 1.9637, val loss 2.0410\niter 1119: loss 1.9894, time 5330.45ms, mfu 0.00%\nstep 1120: train loss 1.9664, val loss 2.0376\niter 1120: loss 1.9710, time 4600.94ms, mfu 0.00%\nstep 1121: train loss 1.9673, val loss 2.0456\niter 1121: loss 2.0633, time 4622.36ms, mfu 0.00%\nstep 1122: train loss 1.9634, val loss 2.0383\niter 1122: loss 2.0609, time 4839.37ms, mfu 0.00%\nstep 1123: train loss 1.9687, val loss 2.0431\niter 1123: loss 1.9204, time 4760.26ms, mfu 0.00%\nstep 1124: train loss 1.9569, val loss 2.0351\niter 1124: loss 1.9600, time 4802.19ms, mfu 0.00%\nstep 1125: train loss 1.9568, val loss 2.0361\niter 1125: loss 2.0343, time 4672.57ms, mfu 0.00%\nstep 1126: train loss 1.9645, val loss 2.0401\niter 1126: loss 1.9095, time 5282.24ms, mfu 0.00%\nstep 1127: train loss 1.9607, val loss 2.0271\nsaving checkpoint to out-shakespeare-char\niter 1127: loss 2.0064, time 4604.17ms, mfu 0.00%\nstep 1128: train loss 1.9551, val loss 2.0366\niter 1128: loss 1.9514, time 4603.55ms, mfu 0.00%\nstep 1129: train loss 1.9605, val loss 2.0394\niter 1129: loss 1.9008, time 4683.96ms, mfu 0.00%\nstep 1130: train loss 1.9552, val loss 2.0407\niter 1130: loss 2.0591, time 4720.76ms, mfu 0.00%\nstep 1131: train loss 1.9605, val loss 2.0352\niter 1131: loss 1.8364, time 4582.89ms, mfu 0.00%\nstep 1132: train loss 1.9589, val loss 2.0363\niter 1132: loss 2.0278, time 4667.38ms, mfu 0.00%\nstep 1133: train loss 1.9520, val loss 2.0307\niter 1133: loss 2.0280, time 5263.95ms, mfu 0.00%\nstep 1134: train loss 1.9576, val loss 2.0318\niter 1134: loss 1.9591, time 4827.36ms, mfu 0.00%\nstep 1135: train loss 1.9580, val loss 2.0389\niter 1135: loss 1.8772, time 4588.69ms, mfu 0.00%\nstep 1136: train loss 1.9597, val loss 2.0460\niter 1136: loss 1.9892, time 4634.89ms, mfu 0.00%\nstep 1137: train loss 1.9650, val loss 2.0422\niter 1137: loss 1.9472, time 4781.94ms, mfu 0.00%\nstep 1138: train loss 1.9576, val loss 2.0330\niter 1138: loss 1.9251, time 5066.25ms, mfu 0.00%\nstep 1139: train loss 1.9430, val loss 2.0209\nsaving checkpoint to out-shakespeare-char\niter 1139: loss 2.0067, time 5272.73ms, mfu 0.00%\nstep 1140: train loss 1.9558, val loss 2.0380\niter 1140: loss 1.9439, time 5405.80ms, mfu 0.00%\nstep 1141: train loss 1.9579, val loss 2.0359\niter 1141: loss 1.9946, time 4544.69ms, mfu 0.00%\nstep 1142: train loss 1.9578, val loss 2.0440\niter 1142: loss 1.8954, time 4683.26ms, mfu 0.00%\nstep 1143: train loss 1.9504, val loss 2.0287\niter 1143: loss 2.0027, time 4305.73ms, mfu 0.00%\nstep 1144: train loss 1.9490, val loss 2.0288\niter 1144: loss 1.9728, time 4413.32ms, mfu 0.00%\nstep 1145: train loss 1.9592, val loss 2.0266\niter 1145: loss 1.9597, time 4443.22ms, mfu 0.00%\nstep 1146: train loss 1.9525, val loss 2.0327\niter 1146: loss 1.9995, time 4467.46ms, mfu 0.00%\nstep 1147: train loss 1.9526, val loss 2.0397\niter 1147: loss 1.9057, time 4990.72ms, mfu 0.00%\nstep 1148: train loss 1.9516, val loss 2.0287\niter 1148: loss 1.9069, time 4313.65ms, mfu 0.00%\nstep 1149: train loss 1.9538, val loss 2.0217\niter 1149: loss 2.0132, time 4485.36ms, mfu 0.00%\nstep 1150: train loss 1.9415, val loss 2.0266\niter 1150: loss 1.9711, time 4357.78ms, mfu 0.00%\nstep 1151: train loss 1.9509, val loss 2.0224\niter 1151: loss 1.9229, time 4562.17ms, mfu 0.00%\nstep 1152: train loss 1.9540, val loss 2.0223\niter 1152: loss 1.9090, time 4388.56ms, mfu 0.00%\nstep 1153: train loss 1.9527, val loss 2.0280\niter 1153: loss 1.9809, time 4374.07ms, mfu 0.00%\nstep 1154: train loss 1.9467, val loss 2.0225\niter 1154: loss 2.0045, time 4935.99ms, mfu 0.00%\nstep 1155: train loss 1.9459, val loss 2.0268\niter 1155: loss 1.9098, time 4440.42ms, mfu 0.00%\nstep 1156: train loss 1.9461, val loss 2.0277\niter 1156: loss 1.9947, time 4431.21ms, mfu 0.00%\nstep 1157: train loss 1.9457, val loss 2.0231\niter 1157: loss 1.9258, time 4397.47ms, mfu 0.00%\nstep 1158: train loss 1.9458, val loss 2.0314\niter 1158: loss 1.9142, time 4454.11ms, mfu 0.00%\nstep 1159: train loss 1.9578, val loss 2.0311\niter 1159: loss 1.9624, time 4316.10ms, mfu 0.00%\nstep 1160: train loss 1.9530, val loss 2.0361\niter 1160: loss 1.9580, time 4579.94ms, mfu 0.00%\nstep 1161: train loss 1.9484, val loss 2.0325\niter 1161: loss 1.9381, time 4771.37ms, mfu 0.00%\nstep 1162: train loss 1.9528, val loss 2.0342\niter 1162: loss 1.8442, time 4380.72ms, mfu 0.00%\nstep 1163: train loss 1.9565, val loss 2.0336\niter 1163: loss 1.8890, time 4459.69ms, mfu 0.00%\nstep 1164: train loss 1.9501, val loss 2.0230\niter 1164: loss 2.0017, time 4358.69ms, mfu 0.00%\nstep 1165: train loss 1.9523, val loss 2.0307\niter 1165: loss 1.9772, time 4461.54ms, mfu 0.00%\nstep 1166: train loss 1.9521, val loss 2.0370\niter 1166: loss 1.9566, time 4288.99ms, mfu 0.00%\nstep 1167: train loss 1.9508, val loss 2.0336\niter 1167: loss 1.9366, time 4298.08ms, mfu 0.00%\nstep 1168: train loss 1.9420, val loss 2.0275\niter 1168: loss 1.9042, time 5007.30ms, mfu 0.00%\nstep 1169: train loss 1.9398, val loss 2.0232\niter 1169: loss 1.8396, time 4355.87ms, mfu 0.00%\nstep 1170: train loss 1.9498, val loss 2.0234\niter 1170: loss 1.8732, time 4257.74ms, mfu 0.00%\nstep 1171: train loss 1.9511, val loss 2.0284\niter 1171: loss 1.9570, time 4417.87ms, mfu 0.00%\nstep 1172: train loss 1.9471, val loss 2.0351\niter 1172: loss 1.9803, time 4292.60ms, mfu 0.00%\nstep 1173: train loss 1.9480, val loss 2.0387\niter 1173: loss 1.8971, time 4658.58ms, mfu 0.00%\nstep 1174: train loss 1.9548, val loss 2.0381\niter 1174: loss 1.9692, time 4733.68ms, mfu 0.00%\nstep 1175: train loss 1.9522, val loss 2.0320\niter 1175: loss 1.8675, time 5354.83ms, mfu 0.00%\nstep 1176: train loss 1.9487, val loss 2.0333\niter 1176: loss 1.8923, time 4496.79ms, mfu 0.00%\nstep 1177: train loss 1.9507, val loss 2.0264\niter 1177: loss 1.9773, time 4425.14ms, mfu 0.00%\nstep 1178: train loss 1.9519, val loss 2.0320\niter 1178: loss 1.9497, time 4510.53ms, mfu 0.00%\nstep 1179: train loss 1.9408, val loss 2.0280\niter 1179: loss 1.9695, time 4381.56ms, mfu 0.00%\nstep 1180: train loss 1.9390, val loss 2.0315\niter 1180: loss 2.0446, time 4353.66ms, mfu 0.00%\nstep 1181: train loss 1.9402, val loss 2.0261\niter 1181: loss 1.8711, time 4412.62ms, mfu 0.00%\nstep 1182: train loss 1.9456, val loss 2.0281\niter 1182: loss 1.9768, time 4742.06ms, mfu 0.00%\nstep 1183: train loss 1.9458, val loss 2.0229\niter 1183: loss 2.0186, time 4611.01ms, mfu 0.00%\nstep 1184: train loss 1.9454, val loss 2.0387\niter 1184: loss 1.8611, time 4466.15ms, mfu 0.00%\nstep 1185: train loss 1.9593, val loss 2.0303\niter 1185: loss 2.0187, time 4386.92ms, mfu 0.00%\nstep 1186: train loss 1.9535, val loss 2.0269\niter 1186: loss 2.0069, time 4420.11ms, mfu 0.00%\nstep 1187: train loss 1.9511, val loss 2.0246\niter 1187: loss 1.9255, time 4358.99ms, mfu 0.00%\nstep 1188: train loss 1.9411, val loss 2.0253\niter 1188: loss 1.9956, time 4304.56ms, mfu 0.00%\nstep 1189: train loss 1.9341, val loss 2.0189\nsaving checkpoint to out-shakespeare-char\niter 1189: loss 1.9300, time 4440.81ms, mfu 0.00%\nstep 1190: train loss 1.9332, val loss 2.0135\nsaving checkpoint to out-shakespeare-char\niter 1190: loss 2.0130, time 5048.15ms, mfu 0.00%\nstep 1191: train loss 1.9371, val loss 2.0132\nsaving checkpoint to out-shakespeare-char\niter 1191: loss 1.8804, time 4500.71ms, mfu 0.00%\nstep 1192: train loss 1.9424, val loss 2.0211\niter 1192: loss 1.9404, time 4384.32ms, mfu 0.00%\nstep 1193: train loss 1.9504, val loss 2.0282\niter 1193: loss 2.0541, time 4360.63ms, mfu 0.00%\nstep 1194: train loss 1.9415, val loss 2.0207\niter 1194: loss 1.9156, time 4285.82ms, mfu 0.00%\nstep 1195: train loss 1.9437, val loss 2.0257\niter 1195: loss 1.9316, time 4244.69ms, mfu 0.00%\nstep 1196: train loss 1.9433, val loss 2.0208\niter 1196: loss 1.9412, time 4266.71ms, mfu 0.00%\nstep 1197: train loss 1.9422, val loss 2.0290\niter 1197: loss 1.9575, time 5007.29ms, mfu 0.00%\nstep 1198: train loss 1.9508, val loss 2.0199\niter 1198: loss 1.9895, time 4286.00ms, mfu 0.00%\nstep 1199: train loss 1.9433, val loss 2.0141\niter 1199: loss 1.9108, time 4353.05ms, mfu 0.00%\nstep 1200: train loss 1.9376, val loss 2.0170\niter 1200: loss 1.9999, time 4364.84ms, mfu 0.00%\nstep 1201: train loss 1.9361, val loss 2.0057\nsaving checkpoint to out-shakespeare-char\niter 1201: loss 1.9453, time 4224.99ms, mfu 0.00%\nstep 1202: train loss 1.9386, val loss 2.0215\niter 1202: loss 1.9015, time 4198.79ms, mfu 0.00%\nstep 1203: train loss 1.9311, val loss 2.0231\niter 1203: loss 1.9022, time 4260.91ms, mfu 0.00%\nstep 1204: train loss 1.9378, val loss 2.0277\niter 1204: loss 1.9734, time 4841.11ms, mfu 0.00%\nstep 1205: train loss 1.9374, val loss 2.0142\niter 1205: loss 1.9164, time 4400.39ms, mfu 0.00%\nstep 1206: train loss 1.9280, val loss 2.0187\niter 1206: loss 1.8596, time 4276.13ms, mfu 0.00%\nstep 1207: train loss 1.9337, val loss 2.0226\niter 1207: loss 1.9383, time 4235.83ms, mfu 0.00%\nstep 1208: train loss 1.9451, val loss 2.0314\niter 1208: loss 1.8537, time 4283.80ms, mfu 0.00%\nstep 1209: train loss 1.9368, val loss 2.0385\niter 1209: loss 1.9651, time 4629.60ms, mfu 0.00%\nstep 1210: train loss 1.9427, val loss 2.0317\niter 1210: loss 2.0016, time 4576.76ms, mfu 0.00%\nstep 1211: train loss 1.9448, val loss 2.0324\niter 1211: loss 1.9111, time 5039.22ms, mfu 0.00%\nstep 1212: train loss 1.9350, val loss 2.0297\niter 1212: loss 1.9280, time 4698.72ms, mfu 0.00%\nstep 1213: train loss 1.9445, val loss 2.0233\niter 1213: loss 1.9673, time 4559.08ms, mfu 0.00%\nstep 1214: train loss 1.9418, val loss 2.0275\niter 1214: loss 1.9558, time 4368.41ms, mfu 0.00%\nstep 1215: train loss 1.9375, val loss 2.0342\niter 1215: loss 2.0133, time 4310.46ms, mfu 0.00%\nstep 1216: train loss 1.9380, val loss 2.0279\niter 1216: loss 2.0222, time 4369.00ms, mfu 0.00%\nstep 1217: train loss 1.9346, val loss 2.0227\niter 1217: loss 1.9084, time 4213.62ms, mfu 0.00%\nstep 1218: train loss 1.9356, val loss 2.0286\niter 1218: loss 1.9957, time 4436.11ms, mfu 0.00%\nstep 1219: train loss 1.9277, val loss 2.0291\niter 1219: loss 1.8735, time 5086.27ms, mfu 0.00%\nstep 1220: train loss 1.9319, val loss 2.0216\niter 1220: loss 1.9021, time 4385.14ms, mfu 0.00%\nstep 1221: train loss 1.9349, val loss 2.0232\niter 1221: loss 1.8936, time 4358.31ms, mfu 0.00%\nstep 1222: train loss 1.9388, val loss 2.0127\niter 1222: loss 2.0473, time 4307.69ms, mfu 0.00%\nstep 1223: train loss 1.9275, val loss 2.0170\niter 1223: loss 1.9846, time 4350.06ms, mfu 0.00%\nstep 1224: train loss 1.9267, val loss 2.0186\niter 1224: loss 1.8924, time 4308.61ms, mfu 0.00%\nstep 1225: train loss 1.9265, val loss 2.0198\niter 1225: loss 1.9221, time 4331.47ms, mfu 0.00%\nstep 1226: train loss 1.9302, val loss 2.0110\niter 1226: loss 2.0200, time 5047.76ms, mfu 0.00%\nstep 1227: train loss 1.9225, val loss 2.0122\niter 1227: loss 1.9832, time 4392.74ms, mfu 0.00%\nstep 1228: train loss 1.9293, val loss 2.0015\nsaving checkpoint to out-shakespeare-char\niter 1228: loss 1.9453, time 4332.80ms, mfu 0.00%\nstep 1229: train loss 1.9273, val loss 2.0178\niter 1229: loss 1.9816, time 4401.65ms, mfu 0.00%\nstep 1230: train loss 1.9275, val loss 2.0157\niter 1230: loss 1.9133, time 4312.54ms, mfu 0.00%\nstep 1231: train loss 1.9243, val loss 2.0275\niter 1231: loss 1.9319, time 4270.22ms, mfu 0.00%\nstep 1232: train loss 1.9282, val loss 2.0147\niter 1232: loss 1.9140, time 4402.93ms, mfu 0.00%\nstep 1233: train loss 1.9308, val loss 2.0183\niter 1233: loss 2.0494, time 5014.39ms, mfu 0.00%\nstep 1234: train loss 1.9222, val loss 2.0107\niter 1234: loss 1.8637, time 4435.71ms, mfu 0.00%\nstep 1235: train loss 1.9228, val loss 2.0078\niter 1235: loss 1.8721, time 4165.46ms, mfu 0.00%\nstep 1236: train loss 1.9127, val loss 2.0047\niter 1236: loss 1.9307, time 4334.60ms, mfu 0.00%\nstep 1237: train loss 1.9221, val loss 1.9990\nsaving checkpoint to out-shakespeare-char\niter 1237: loss 2.0151, time 4440.80ms, mfu 0.00%\nstep 1238: train loss 1.9183, val loss 2.0040\niter 1238: loss 2.0104, time 4320.67ms, mfu 0.00%\nstep 1239: train loss 1.9195, val loss 2.0112\niter 1239: loss 1.9114, time 4309.90ms, mfu 0.00%\nstep 1240: train loss 1.9197, val loss 1.9994\niter 1240: loss 1.9252, time 4546.17ms, mfu 0.00%\nstep 1241: train loss 1.9169, val loss 1.9983\nsaving checkpoint to out-shakespeare-char\niter 1241: loss 1.9891, time 4659.75ms, mfu 0.00%\nstep 1242: train loss 1.9149, val loss 1.9985\niter 1242: loss 1.9640, time 4342.95ms, mfu 0.00%\nstep 1243: train loss 1.9208, val loss 2.0106\niter 1243: loss 1.9735, time 4310.83ms, mfu 0.00%\nstep 1244: train loss 1.9173, val loss 2.0029\niter 1244: loss 1.9179, time 4160.05ms, mfu 0.00%\nstep 1245: train loss 1.9146, val loss 2.0101\niter 1245: loss 1.9524, time 4317.19ms, mfu 0.00%\nstep 1246: train loss 1.9187, val loss 2.0075\niter 1246: loss 1.8562, time 4244.00ms, mfu 0.00%\nstep 1247: train loss 1.9069, val loss 2.0037\niter 1247: loss 1.9211, time 4396.86ms, mfu 0.00%\nstep 1248: train loss 1.9247, val loss 2.0088\niter 1248: loss 1.9550, time 5230.97ms, mfu 0.00%\nstep 1249: train loss 1.9221, val loss 2.0122\niter 1249: loss 2.0045, time 4566.22ms, mfu 0.00%\nstep 1250: train loss 1.9265, val loss 2.0163\niter 1250: loss 1.9945, time 4683.12ms, mfu 0.00%\nstep 1251: train loss 1.9237, val loss 2.0063\niter 1251: loss 1.9676, time 4390.59ms, mfu 0.00%\nstep 1252: train loss 1.9206, val loss 2.0061\niter 1252: loss 1.9393, time 4409.40ms, mfu 0.00%\nstep 1253: train loss 1.9224, val loss 2.0218\niter 1253: loss 1.8852, time 4343.49ms, mfu 0.00%\nstep 1254: train loss 1.9250, val loss 2.0112\niter 1254: loss 1.8804, time 4380.61ms, mfu 0.00%\nstep 1255: train loss 1.9289, val loss 2.0108\niter 1255: loss 1.9129, time 4900.36ms, mfu 0.00%\nstep 1256: train loss 1.9243, val loss 2.0080\niter 1256: loss 1.8690, time 4463.21ms, mfu 0.00%\nstep 1257: train loss 1.9222, val loss 2.0076\niter 1257: loss 1.9387, time 4337.69ms, mfu 0.00%\nstep 1258: train loss 1.9210, val loss 2.0025\niter 1258: loss 1.9230, time 4397.43ms, mfu 0.00%\nstep 1259: train loss 1.9171, val loss 1.9938\nsaving checkpoint to out-shakespeare-char\niter 1259: loss 1.8041, time 4418.70ms, mfu 0.00%\nstep 1260: train loss 1.9256, val loss 2.0023\niter 1260: loss 1.8992, time 4332.87ms, mfu 0.00%\nstep 1261: train loss 1.9160, val loss 2.0175\niter 1261: loss 1.8611, time 4416.52ms, mfu 0.00%\nstep 1262: train loss 1.9172, val loss 2.0193\niter 1262: loss 1.9059, time 4987.94ms, mfu 0.00%\nstep 1263: train loss 1.9219, val loss 2.0161\niter 1263: loss 1.8844, time 4389.50ms, mfu 0.00%\nstep 1264: train loss 1.9263, val loss 2.0135\niter 1264: loss 2.0544, time 4415.39ms, mfu 0.00%\nstep 1265: train loss 1.9199, val loss 2.0107\niter 1265: loss 1.9350, time 4294.10ms, mfu 0.00%\nstep 1266: train loss 1.9204, val loss 2.0060\niter 1266: loss 2.0199, time 4412.12ms, mfu 0.00%\nstep 1267: train loss 1.9158, val loss 2.0058\niter 1267: loss 1.9275, time 4331.06ms, mfu 0.00%\nstep 1268: train loss 1.9163, val loss 1.9996\niter 1268: loss 1.8793, time 4377.10ms, mfu 0.00%\nstep 1269: train loss 1.9204, val loss 2.0029\niter 1269: loss 1.9075, time 4961.62ms, mfu 0.00%\nstep 1270: train loss 1.9091, val loss 2.0060\niter 1270: loss 1.9627, time 4411.15ms, mfu 0.00%\nstep 1271: train loss 1.9124, val loss 2.0023\niter 1271: loss 1.8595, time 4418.89ms, mfu 0.00%\nstep 1272: train loss 1.9164, val loss 2.0015\niter 1272: loss 1.8866, time 4526.20ms, mfu 0.00%\nstep 1273: train loss 1.9159, val loss 2.0037\niter 1273: loss 1.9824, time 4306.10ms, mfu 0.00%\nstep 1274: train loss 1.9095, val loss 2.0163\niter 1274: loss 1.9988, time 4363.50ms, mfu 0.00%\nstep 1275: train loss 1.9137, val loss 2.0049\niter 1275: loss 1.9861, time 4274.20ms, mfu 0.00%\nstep 1276: train loss 1.9140, val loss 2.0003\niter 1276: loss 1.9379, time 4421.68ms, mfu 0.00%\nstep 1277: train loss 1.9021, val loss 2.0112\niter 1277: loss 1.8540, time 4901.67ms, mfu 0.00%\nstep 1278: train loss 1.9076, val loss 2.0009\niter 1278: loss 1.8404, time 4548.75ms, mfu 0.00%\nstep 1279: train loss 1.8992, val loss 2.0037\niter 1279: loss 2.0574, time 4696.56ms, mfu 0.00%\nstep 1280: train loss 1.9159, val loss 1.9993\niter 1280: loss 1.8711, time 4600.34ms, mfu 0.00%\nstep 1281: train loss 1.9052, val loss 2.0086\niter 1281: loss 1.8858, time 4609.02ms, mfu 0.00%\nstep 1282: train loss 1.9111, val loss 1.9984\niter 1282: loss 1.9179, time 4605.57ms, mfu 0.00%\nstep 1283: train loss 1.9053, val loss 2.0016\niter 1283: loss 1.8547, time 4572.07ms, mfu 0.00%\nstep 1284: train loss 1.9055, val loss 1.9989\niter 1284: loss 1.8539, time 4735.17ms, mfu 0.00%\nstep 1285: train loss 1.8993, val loss 2.0036\niter 1285: loss 1.8995, time 4660.38ms, mfu 0.00%\nstep 1286: train loss 1.9094, val loss 1.9991\niter 1286: loss 1.9759, time 4374.85ms, mfu 0.00%\nstep 1287: train loss 1.9114, val loss 1.9865\nsaving checkpoint to out-shakespeare-char\niter 1287: loss 1.8910, time 4378.19ms, mfu 0.00%\nstep 1288: train loss 1.9077, val loss 1.9995\niter 1288: loss 1.9217, time 4503.19ms, mfu 0.00%\nstep 1289: train loss 1.9065, val loss 2.0036\niter 1289: loss 1.9188, time 4573.06ms, mfu 0.00%\nstep 1290: train loss 1.9146, val loss 2.0062\niter 1290: loss 1.9552, time 4710.09ms, mfu 0.00%\nstep 1291: train loss 1.9117, val loss 2.0117\niter 1291: loss 1.9291, time 5035.93ms, mfu 0.00%\nstep 1292: train loss 1.9126, val loss 2.0119\niter 1292: loss 1.9541, time 4634.55ms, mfu 0.00%\nstep 1293: train loss 1.9172, val loss 2.0037\niter 1293: loss 1.9867, time 4457.06ms, mfu 0.00%\nstep 1294: train loss 1.9072, val loss 2.0045\niter 1294: loss 1.9799, time 4557.15ms, mfu 0.00%\nstep 1295: train loss 1.9114, val loss 2.0188\niter 1295: loss 1.9651, time 4505.07ms, mfu 0.00%\nstep 1296: train loss 1.9103, val loss 2.0034\niter 1296: loss 1.8925, time 4588.37ms, mfu 0.00%\nstep 1297: train loss 1.9039, val loss 2.0060\niter 1297: loss 1.8686, time 4835.66ms, mfu 0.00%\nstep 1298: train loss 1.9054, val loss 2.0044\niter 1298: loss 1.8167, time 4900.10ms, mfu 0.00%\nstep 1299: train loss 1.9136, val loss 2.0035\niter 1299: loss 1.9502, time 4510.12ms, mfu 0.00%\nstep 1300: train loss 1.9043, val loss 2.0153\niter 1300: loss 1.9784, time 4597.31ms, mfu 0.00%\nstep 1301: train loss 1.9162, val loss 2.0122\niter 1301: loss 1.8929, time 4428.31ms, mfu 0.00%\nstep 1302: train loss 1.9102, val loss 2.0002\niter 1302: loss 1.9051, time 4481.74ms, mfu 0.00%\nstep 1303: train loss 1.9059, val loss 1.9967\niter 1303: loss 1.8050, time 4545.04ms, mfu 0.00%\nstep 1304: train loss 1.9097, val loss 1.9931\niter 1304: loss 1.8202, time 4777.34ms, mfu 0.00%\nstep 1305: train loss 1.9052, val loss 1.9856\nsaving checkpoint to out-shakespeare-char\niter 1305: loss 1.8921, time 5056.56ms, mfu 0.00%\nstep 1306: train loss 1.9060, val loss 1.9909\niter 1306: loss 1.9743, time 4436.06ms, mfu 0.00%\nstep 1307: train loss 1.9139, val loss 1.9926\niter 1307: loss 1.9077, time 4565.02ms, mfu 0.00%\nstep 1308: train loss 1.9018, val loss 2.0028\niter 1308: loss 1.8150, time 4541.42ms, mfu 0.00%\nstep 1309: train loss 1.9018, val loss 1.9972\niter 1309: loss 1.9748, time 4816.37ms, mfu 0.00%\nstep 1310: train loss 1.8981, val loss 1.9991\niter 1310: loss 1.8481, time 4759.34ms, mfu 0.00%\nstep 1311: train loss 1.9017, val loss 1.9929\niter 1311: loss 1.8857, time 5471.09ms, mfu 0.00%\nstep 1312: train loss 1.8949, val loss 1.9940\niter 1312: loss 1.8736, time 4769.99ms, mfu 0.00%\nstep 1313: train loss 1.9011, val loss 1.9956\niter 1313: loss 1.8387, time 4622.32ms, mfu 0.00%\nstep 1314: train loss 1.8965, val loss 1.9999\niter 1314: loss 1.8835, time 4494.39ms, mfu 0.00%\nstep 1315: train loss 1.8918, val loss 2.0026\niter 1315: loss 1.9106, time 4589.91ms, mfu 0.00%\nstep 1316: train loss 1.8987, val loss 2.0013\niter 1316: loss 1.9010, time 4525.73ms, mfu 0.00%\nstep 1317: train loss 1.8960, val loss 1.9890\niter 1317: loss 1.9433, time 4522.26ms, mfu 0.00%\nstep 1318: train loss 1.8904, val loss 1.9940\niter 1318: loss 1.8019, time 5227.50ms, mfu 0.00%\nstep 1319: train loss 1.8938, val loss 1.9978\niter 1319: loss 1.8612, time 4618.98ms, mfu 0.00%\nstep 1320: train loss 1.8954, val loss 1.9963\niter 1320: loss 1.9265, time 4570.15ms, mfu 0.00%\nstep 1321: train loss 1.8908, val loss 1.9979\niter 1321: loss 1.9446, time 4509.96ms, mfu 0.00%\nstep 1322: train loss 1.8905, val loss 1.9962\niter 1322: loss 1.8340, time 4504.66ms, mfu 0.00%\nstep 1323: train loss 1.8891, val loss 1.9993\niter 1323: loss 1.7567, time 4544.06ms, mfu 0.00%\nstep 1324: train loss 1.8935, val loss 1.9866\niter 1324: loss 1.8626, time 4667.33ms, mfu 0.00%\nstep 1325: train loss 1.8944, val loss 2.0044\niter 1325: loss 1.8855, time 5100.90ms, mfu 0.00%\nstep 1326: train loss 1.8857, val loss 1.9947\niter 1326: loss 1.8706, time 4622.00ms, mfu 0.00%\nstep 1327: train loss 1.8964, val loss 2.0072\niter 1327: loss 1.9653, time 4488.41ms, mfu 0.00%\nstep 1328: train loss 1.8941, val loss 2.0043\niter 1328: loss 1.9077, time 4652.30ms, mfu 0.00%\nstep 1329: train loss 1.8882, val loss 1.9983\niter 1329: loss 1.8170, time 4502.70ms, mfu 0.00%\nstep 1330: train loss 1.8949, val loss 2.0093\niter 1330: loss 1.8650, time 4601.89ms, mfu 0.00%\nstep 1331: train loss 1.8971, val loss 2.0128\niter 1331: loss 1.9555, time 4585.94ms, mfu 0.00%\nstep 1332: train loss 1.9077, val loss 2.0112\niter 1332: loss 2.0086, time 5271.03ms, mfu 0.00%\nstep 1333: train loss 1.8929, val loss 2.0093\niter 1333: loss 1.8855, time 4559.16ms, mfu 0.00%\nstep 1334: train loss 1.8941, val loss 2.0011\niter 1334: loss 1.8376, time 4500.34ms, mfu 0.00%\nstep 1335: train loss 1.8966, val loss 1.9960\niter 1335: loss 1.7858, time 4591.02ms, mfu 0.00%\nstep 1336: train loss 1.8913, val loss 2.0015\niter 1336: loss 1.8074, time 4560.32ms, mfu 0.00%\nstep 1337: train loss 1.8965, val loss 1.9997\niter 1337: loss 1.8590, time 4457.28ms, mfu 0.00%\nstep 1338: train loss 1.8980, val loss 2.0085\niter 1338: loss 1.9035, time 4483.48ms, mfu 0.00%\nstep 1339: train loss 1.9013, val loss 1.9981\niter 1339: loss 1.8599, time 5058.18ms, mfu 0.00%\nstep 1340: train loss 1.8931, val loss 2.0020\niter 1340: loss 1.9243, time 4430.35ms, mfu 0.00%\nstep 1341: train loss 1.9002, val loss 2.0036\niter 1341: loss 1.9581, time 4544.31ms, mfu 0.00%\nstep 1342: train loss 1.8902, val loss 1.9974\niter 1342: loss 1.9530, time 4648.45ms, mfu 0.00%\nstep 1343: train loss 1.8936, val loss 1.9915\niter 1343: loss 1.8768, time 4836.99ms, mfu 0.00%\nstep 1344: train loss 1.8955, val loss 1.9913\niter 1344: loss 1.8830, time 4859.83ms, mfu 0.00%\nstep 1345: train loss 1.8923, val loss 1.9934\niter 1345: loss 1.7893, time 5032.63ms, mfu 0.00%\nstep 1346: train loss 1.8947, val loss 1.9900\niter 1346: loss 1.8668, time 5303.38ms, mfu 0.00%\nstep 1347: train loss 1.8963, val loss 1.9911\niter 1347: loss 1.8687, time 4584.96ms, mfu 0.00%\nstep 1348: train loss 1.8968, val loss 1.9904\niter 1348: loss 1.8908, time 4551.13ms, mfu 0.00%\nstep 1349: train loss 1.8913, val loss 1.9878\niter 1349: loss 1.9195, time 4614.64ms, mfu 0.00%\nstep 1350: train loss 1.8921, val loss 1.9935\niter 1350: loss 1.9288, time 4555.82ms, mfu 0.00%\nstep 1351: train loss 1.8868, val loss 1.9964\niter 1351: loss 1.8373, time 4554.19ms, mfu 0.00%\nstep 1352: train loss 1.8890, val loss 1.9902\niter 1352: loss 1.8918, time 4557.76ms, mfu 0.00%\nstep 1353: train loss 1.8949, val loss 1.9928\niter 1353: loss 1.8128, time 5199.09ms, mfu 0.00%\nstep 1354: train loss 1.8981, val loss 1.9971\niter 1354: loss 1.9015, time 4610.37ms, mfu 0.00%\nstep 1355: train loss 1.8953, val loss 1.9956\niter 1355: loss 1.9902, time 4459.70ms, mfu 0.00%\nstep 1356: train loss 1.8952, val loss 2.0060\niter 1356: loss 1.8850, time 4723.55ms, mfu 0.00%\nstep 1357: train loss 1.8879, val loss 1.9985\niter 1357: loss 1.8131, time 4551.41ms, mfu 0.00%\nstep 1358: train loss 1.8916, val loss 1.9971\niter 1358: loss 1.8698, time 4618.51ms, mfu 0.00%\nstep 1359: train loss 1.8901, val loss 1.9919\niter 1359: loss 1.8428, time 4766.04ms, mfu 0.00%\nstep 1360: train loss 1.8878, val loss 2.0047\niter 1360: loss 1.8625, time 5164.40ms, mfu 0.00%\nstep 1361: train loss 1.8949, val loss 2.0071\niter 1361: loss 1.8807, time 4620.32ms, mfu 0.00%\nstep 1362: train loss 1.8956, val loss 2.0037\niter 1362: loss 1.8544, time 4562.65ms, mfu 0.00%\nstep 1363: train loss 1.8923, val loss 2.0011\niter 1363: loss 1.8922, time 4588.01ms, mfu 0.00%\nstep 1364: train loss 1.8842, val loss 2.0073\niter 1364: loss 1.9092, time 4601.66ms, mfu 0.00%\nstep 1365: train loss 1.8895, val loss 2.0066\niter 1365: loss 1.8835, time 4524.55ms, mfu 0.00%\nstep 1366: train loss 1.8901, val loss 2.0073\niter 1366: loss 1.8716, time 4846.25ms, mfu 0.00%\nstep 1367: train loss 1.8817, val loss 1.9941\niter 1367: loss 1.9026, time 4952.24ms, mfu 0.00%\nstep 1368: train loss 1.8832, val loss 1.9989\niter 1368: loss 1.8530, time 4498.94ms, mfu 0.00%\nstep 1369: train loss 1.8754, val loss 1.9960\niter 1369: loss 1.8218, time 4534.93ms, mfu 0.00%\nstep 1370: train loss 1.8814, val loss 1.9935\niter 1370: loss 1.8492, time 4458.19ms, mfu 0.00%\nstep 1371: train loss 1.8824, val loss 1.9961\niter 1371: loss 1.9131, time 4539.48ms, mfu 0.00%\nstep 1372: train loss 1.8822, val loss 1.9884\niter 1372: loss 1.9104, time 4476.06ms, mfu 0.00%\nstep 1373: train loss 1.8885, val loss 1.9882\niter 1373: loss 2.0400, time 4809.31ms, mfu 0.00%\nstep 1374: train loss 1.8849, val loss 1.9908\niter 1374: loss 1.8756, time 5188.07ms, mfu 0.00%\nstep 1375: train loss 1.8846, val loss 1.9890\niter 1375: loss 1.8853, time 4826.51ms, mfu 0.00%\nstep 1376: train loss 1.8774, val loss 1.9866\niter 1376: loss 1.7883, time 4683.95ms, mfu 0.00%\nstep 1377: train loss 1.8863, val loss 1.9857\niter 1377: loss 1.8605, time 4630.54ms, mfu 0.00%\nstep 1378: train loss 1.8786, val loss 1.9789\nsaving checkpoint to out-shakespeare-char\niter 1378: loss 1.9397, time 4542.48ms, mfu 0.00%\nstep 1379: train loss 1.8812, val loss 1.9856\niter 1379: loss 1.8715, time 4589.11ms, mfu 0.00%\nstep 1380: train loss 1.8799, val loss 1.9854\niter 1380: loss 1.7288, time 5234.91ms, mfu 0.00%\nstep 1381: train loss 1.8745, val loss 1.9894\niter 1381: loss 1.8072, time 4631.78ms, mfu 0.00%\nstep 1382: train loss 1.8755, val loss 1.9779\nsaving checkpoint to out-shakespeare-char\niter 1382: loss 1.8894, time 4565.83ms, mfu 0.00%\nstep 1383: train loss 1.8799, val loss 1.9836\niter 1383: loss 1.8589, time 4589.57ms, mfu 0.00%\nstep 1384: train loss 1.8821, val loss 1.9770\nsaving checkpoint to out-shakespeare-char\niter 1384: loss 1.7848, time 4617.74ms, mfu 0.00%\nstep 1385: train loss 1.8807, val loss 1.9786\niter 1385: loss 1.9296, time 4548.91ms, mfu 0.00%\nstep 1386: train loss 1.8758, val loss 1.9865\niter 1386: loss 1.8316, time 4521.71ms, mfu 0.00%\nstep 1387: train loss 1.8806, val loss 1.9823\niter 1387: loss 1.8516, time 5046.55ms, mfu 0.00%\nstep 1388: train loss 1.8883, val loss 1.9799\niter 1388: loss 1.8623, time 4601.86ms, mfu 0.00%\nstep 1389: train loss 1.8765, val loss 1.9850\niter 1389: loss 1.7768, time 4494.56ms, mfu 0.00%\nstep 1390: train loss 1.8809, val loss 1.9815\niter 1390: loss 1.8844, time 4523.45ms, mfu 0.00%\nstep 1391: train loss 1.8807, val loss 1.9774\niter 1391: loss 1.9050, time 4458.35ms, mfu 0.00%\nstep 1392: train loss 1.8688, val loss 1.9857\niter 1392: loss 1.9084, time 4580.55ms, mfu 0.00%\nstep 1393: train loss 1.8838, val loss 1.9786\niter 1393: loss 1.7931, time 4729.06ms, mfu 0.00%\nstep 1394: train loss 1.8771, val loss 1.9761\nsaving checkpoint to out-shakespeare-char\niter 1394: loss 1.9390, time 5404.64ms, mfu 0.00%\nstep 1395: train loss 1.8747, val loss 1.9897\niter 1395: loss 1.9436, time 4549.94ms, mfu 0.00%\nstep 1396: train loss 1.8758, val loss 1.9753\nsaving checkpoint to out-shakespeare-char\niter 1396: loss 1.8255, time 4599.59ms, mfu 0.00%\nstep 1397: train loss 1.8758, val loss 1.9737\nsaving checkpoint to out-shakespeare-char\niter 1397: loss 1.8717, time 4649.36ms, mfu 0.00%\nstep 1398: train loss 1.8818, val loss 1.9774\niter 1398: loss 1.8603, time 4587.11ms, mfu 0.00%\nstep 1399: train loss 1.8778, val loss 1.9782\niter 1399: loss 1.9027, time 4687.62ms, mfu 0.00%\nstep 1400: train loss 1.8712, val loss 1.9770\niter 1400: loss 1.8203, time 4628.65ms, mfu 0.00%\nstep 1401: train loss 1.8788, val loss 1.9880\niter 1401: loss 1.8358, time 5212.77ms, mfu 0.00%\nstep 1402: train loss 1.8752, val loss 1.9786\niter 1402: loss 2.0269, time 4685.17ms, mfu 0.00%\nstep 1403: train loss 1.8794, val loss 1.9803\niter 1403: loss 1.7995, time 4674.86ms, mfu 0.00%\nstep 1404: train loss 1.8834, val loss 1.9881\niter 1404: loss 1.8443, time 4638.17ms, mfu 0.00%\nstep 1405: train loss 1.8896, val loss 1.9853\niter 1405: loss 1.8673, time 4691.56ms, mfu 0.00%\nstep 1406: train loss 1.8848, val loss 1.9953\niter 1406: loss 1.9772, time 4544.40ms, mfu 0.00%\nstep 1407: train loss 1.8820, val loss 1.9830\niter 1407: loss 1.9389, time 4862.65ms, mfu 0.00%\nstep 1408: train loss 1.8770, val loss 1.9794\niter 1408: loss 1.8662, time 4895.22ms, mfu 0.00%\nstep 1409: train loss 1.8790, val loss 1.9782\niter 1409: loss 1.9093, time 4653.24ms, mfu 0.00%\nstep 1410: train loss 1.8732, val loss 1.9883\niter 1410: loss 1.8488, time 4543.69ms, mfu 0.00%\nstep 1411: train loss 1.8758, val loss 1.9773\niter 1411: loss 1.7627, time 4641.10ms, mfu 0.00%\nstep 1412: train loss 1.8801, val loss 1.9805\niter 1412: loss 1.8655, time 4771.59ms, mfu 0.00%\nstep 1413: train loss 1.8738, val loss 1.9757\niter 1413: loss 1.8649, time 4734.69ms, mfu 0.00%\nstep 1414: train loss 1.8710, val loss 1.9778\niter 1414: loss 1.9009, time 5680.30ms, mfu 0.00%\nstep 1415: train loss 1.8697, val loss 1.9760\niter 1415: loss 1.8987, time 4926.73ms, mfu 0.00%\nstep 1416: train loss 1.8785, val loss 1.9759\niter 1416: loss 1.9340, time 4719.41ms, mfu 0.00%\nstep 1417: train loss 1.8802, val loss 1.9670\nsaving checkpoint to out-shakespeare-char\niter 1417: loss 1.8151, time 4752.92ms, mfu 0.00%\nstep 1418: train loss 1.8710, val loss 1.9843\niter 1418: loss 1.8481, time 4646.21ms, mfu 0.00%\nstep 1419: train loss 1.8704, val loss 1.9686\niter 1419: loss 1.9022, time 4730.93ms, mfu 0.00%\nstep 1420: train loss 1.8807, val loss 1.9711\niter 1420: loss 1.8348, time 4692.48ms, mfu 0.00%\nstep 1421: train loss 1.8658, val loss 1.9668\nsaving checkpoint to out-shakespeare-char\niter 1421: loss 1.9359, time 5193.90ms, mfu 0.00%\nstep 1422: train loss 1.8800, val loss 1.9753\niter 1422: loss 1.9697, time 4593.41ms, mfu 0.00%\nstep 1423: train loss 1.8728, val loss 1.9836\niter 1423: loss 1.7938, time 4568.09ms, mfu 0.00%\nstep 1424: train loss 1.8719, val loss 1.9867\niter 1424: loss 1.9287, time 4635.17ms, mfu 0.00%\nstep 1425: train loss 1.8726, val loss 1.9780\niter 1425: loss 1.9960, time 4561.96ms, mfu 0.00%\nstep 1426: train loss 1.8713, val loss 1.9777\niter 1426: loss 1.8784, time 4623.51ms, mfu 0.00%\nstep 1427: train loss 1.8736, val loss 1.9884\niter 1427: loss 1.8816, time 4615.16ms, mfu 0.00%\nstep 1428: train loss 1.8658, val loss 1.9781\niter 1428: loss 1.9366, time 5148.82ms, mfu 0.00%\nstep 1429: train loss 1.8703, val loss 1.9778\niter 1429: loss 1.8192, time 4561.51ms, mfu 0.00%\nstep 1430: train loss 1.8632, val loss 1.9750\niter 1430: loss 1.8443, time 4581.07ms, mfu 0.00%\nstep 1431: train loss 1.8653, val loss 1.9753\niter 1431: loss 1.8416, time 4573.11ms, mfu 0.00%\nstep 1432: train loss 1.8727, val loss 1.9791\niter 1432: loss 1.8207, time 4596.65ms, mfu 0.00%\nstep 1433: train loss 1.8703, val loss 1.9801\niter 1433: loss 1.8765, time 4523.54ms, mfu 0.00%\nstep 1434: train loss 1.8666, val loss 1.9851\niter 1434: loss 1.8467, time 4498.74ms, mfu 0.00%\nstep 1435: train loss 1.8782, val loss 1.9839\niter 1435: loss 1.8987, time 5138.96ms, mfu 0.00%\nstep 1436: train loss 1.8711, val loss 1.9758\niter 1436: loss 1.9400, time 4462.54ms, mfu 0.00%\nstep 1437: train loss 1.8744, val loss 1.9859\niter 1437: loss 1.9327, time 4594.60ms, mfu 0.00%\nstep 1438: train loss 1.8725, val loss 1.9846\niter 1438: loss 1.9229, time 4462.69ms, mfu 0.00%\nstep 1439: train loss 1.8623, val loss 1.9769\niter 1439: loss 1.8583, time 4685.20ms, mfu 0.00%\nstep 1440: train loss 1.8564, val loss 1.9700\niter 1440: loss 1.8929, time 4640.43ms, mfu 0.00%\nstep 1441: train loss 1.8613, val loss 1.9764\niter 1441: loss 1.8263, time 4857.93ms, mfu 0.00%\nstep 1442: train loss 1.8633, val loss 1.9793\niter 1442: loss 1.8098, time 5526.77ms, mfu 0.00%\nstep 1443: train loss 1.8686, val loss 1.9694\niter 1443: loss 1.8526, time 4844.21ms, mfu 0.00%\nstep 1444: train loss 1.8699, val loss 1.9754\niter 1444: loss 1.8159, time 4800.54ms, mfu 0.00%\nstep 1445: train loss 1.8671, val loss 1.9636\nsaving checkpoint to out-shakespeare-char\niter 1445: loss 1.8682, time 4772.59ms, mfu 0.00%\nstep 1446: train loss 1.8637, val loss 1.9758\niter 1446: loss 1.8756, time 4597.26ms, mfu 0.00%\nstep 1447: train loss 1.8590, val loss 1.9623\nsaving checkpoint to out-shakespeare-char\niter 1447: loss 1.8621, time 4869.60ms, mfu 0.00%\nstep 1448: train loss 1.8695, val loss 1.9704\niter 1448: loss 1.9193, time 4940.73ms, mfu 0.00%\nstep 1449: train loss 1.8708, val loss 1.9661\niter 1449: loss 1.7522, time 4839.03ms, mfu 0.00%\nstep 1450: train loss 1.8649, val loss 1.9702\niter 1450: loss 1.7985, time 4565.35ms, mfu 0.00%\nstep 1451: train loss 1.8670, val loss 1.9689\niter 1451: loss 1.8476, time 4640.52ms, mfu 0.00%\nstep 1452: train loss 1.8650, val loss 1.9555\nsaving checkpoint to out-shakespeare-char\niter 1452: loss 1.7994, time 4664.78ms, mfu 0.00%\nstep 1453: train loss 1.8568, val loss 1.9657\niter 1453: loss 1.7737, time 4692.76ms, mfu 0.00%\nstep 1454: train loss 1.8605, val loss 1.9642\niter 1454: loss 1.9078, time 4603.64ms, mfu 0.00%\nstep 1455: train loss 1.8627, val loss 1.9658\niter 1455: loss 1.8399, time 5074.48ms, mfu 0.00%\nstep 1456: train loss 1.8605, val loss 1.9628\niter 1456: loss 1.9099, time 4535.55ms, mfu 0.00%\nstep 1457: train loss 1.8631, val loss 1.9568\niter 1457: loss 1.8522, time 4504.29ms, mfu 0.00%\nstep 1458: train loss 1.8662, val loss 1.9596\niter 1458: loss 1.8200, time 4605.41ms, mfu 0.00%\nstep 1459: train loss 1.8525, val loss 1.9641\niter 1459: loss 1.8269, time 4474.70ms, mfu 0.00%\nstep 1460: train loss 1.8632, val loss 1.9641\niter 1460: loss 1.8796, time 4642.49ms, mfu 0.00%\nstep 1461: train loss 1.8566, val loss 1.9629\niter 1461: loss 1.8282, time 4477.11ms, mfu 0.00%\nstep 1462: train loss 1.8551, val loss 1.9668\niter 1462: loss 1.9044, time 5122.68ms, mfu 0.00%\nstep 1463: train loss 1.8549, val loss 1.9631\niter 1463: loss 1.7759, time 4457.19ms, mfu 0.00%\nstep 1464: train loss 1.8544, val loss 1.9688\niter 1464: loss 1.8203, time 4581.62ms, mfu 0.00%\nstep 1465: train loss 1.8652, val loss 1.9697\niter 1465: loss 1.8612, time 4561.71ms, mfu 0.00%\nstep 1466: train loss 1.8566, val loss 1.9708\niter 1466: loss 1.8127, time 4512.09ms, mfu 0.00%\nstep 1467: train loss 1.8573, val loss 1.9737\niter 1467: loss 1.7789, time 4517.79ms, mfu 0.00%\nstep 1468: train loss 1.8658, val loss 1.9745\niter 1468: loss 1.9324, time 4517.96ms, mfu 0.00%\nstep 1469: train loss 1.8581, val loss 1.9786\niter 1469: loss 1.7308, time 5260.63ms, mfu 0.00%\nstep 1470: train loss 1.8543, val loss 1.9680\niter 1470: loss 1.8201, time 4512.89ms, mfu 0.00%\nstep 1471: train loss 1.8558, val loss 1.9700\niter 1471: loss 1.9050, time 4554.74ms, mfu 0.00%\nstep 1472: train loss 1.8481, val loss 1.9613\niter 1472: loss 1.7168, time 4516.24ms, mfu 0.00%\nstep 1473: train loss 1.8545, val loss 1.9653\niter 1473: loss 1.8698, time 4613.91ms, mfu 0.00%\nstep 1474: train loss 1.8569, val loss 1.9565\niter 1474: loss 1.8467, time 4556.53ms, mfu 0.00%\nstep 1475: train loss 1.8523, val loss 1.9674\niter 1475: loss 1.9231, time 4691.95ms, mfu 0.00%\nstep 1476: train loss 1.8560, val loss 1.9688\niter 1476: loss 1.8246, time 5280.53ms, mfu 0.00%\nstep 1477: train loss 1.8556, val loss 1.9623\niter 1477: loss 1.7842, time 5046.35ms, mfu 0.00%\nstep 1478: train loss 1.8548, val loss 1.9686\niter 1478: loss 1.9320, time 4899.27ms, mfu 0.00%\nstep 1479: train loss 1.8553, val loss 1.9633\niter 1479: loss 1.8447, time 4838.28ms, mfu 0.00%\nstep 1480: train loss 1.8494, val loss 1.9681\niter 1480: loss 1.8931, time 4817.81ms, mfu 0.00%\nstep 1481: train loss 1.8449, val loss 1.9775\niter 1481: loss 1.8269, time 4954.88ms, mfu 0.00%\nstep 1482: train loss 1.8585, val loss 1.9712\niter 1482: loss 1.7680, time 4694.31ms, mfu 0.00%\nstep 1483: train loss 1.8565, val loss 1.9657\niter 1483: loss 1.8736, time 5167.72ms, mfu 0.00%\nstep 1484: train loss 1.8610, val loss 1.9684\niter 1484: loss 1.7155, time 4554.04ms, mfu 0.00%\nstep 1485: train loss 1.8563, val loss 1.9692\niter 1485: loss 1.9263, time 4682.59ms, mfu 0.00%\nstep 1486: train loss 1.8655, val loss 1.9763\niter 1486: loss 1.9377, time 4568.04ms, mfu 0.00%\nstep 1487: train loss 1.8651, val loss 1.9716\niter 1487: loss 1.9340, time 4673.40ms, mfu 0.00%\nstep 1488: train loss 1.8542, val loss 1.9601\niter 1488: loss 1.7834, time 4556.74ms, mfu 0.00%\nstep 1489: train loss 1.8545, val loss 1.9675\niter 1489: loss 1.8499, time 4778.27ms, mfu 0.00%\nstep 1490: train loss 1.8570, val loss 1.9643\niter 1490: loss 1.9087, time 5121.23ms, mfu 0.00%\nstep 1491: train loss 1.8552, val loss 1.9631\niter 1491: loss 1.9179, time 4563.40ms, mfu 0.00%\nstep 1492: train loss 1.8533, val loss 1.9637\niter 1492: loss 1.7955, time 4637.22ms, mfu 0.00%\nstep 1493: train loss 1.8517, val loss 1.9608\niter 1493: loss 1.8512, time 4524.26ms, mfu 0.00%\nstep 1494: train loss 1.8417, val loss 1.9601\niter 1494: loss 1.7539, time 4612.15ms, mfu 0.00%\nstep 1495: train loss 1.8533, val loss 1.9610\niter 1495: loss 1.8233, time 4590.35ms, mfu 0.00%\nstep 1496: train loss 1.8536, val loss 1.9686\niter 1496: loss 1.9404, time 5142.90ms, mfu 0.00%\nstep 1497: train loss 1.8522, val loss 1.9571\niter 1497: loss 1.7924, time 4904.58ms, mfu 0.00%\nstep 1498: train loss 1.8536, val loss 1.9543\nsaving checkpoint to out-shakespeare-char\niter 1498: loss 1.9228, time 4664.00ms, mfu 0.00%\nstep 1499: train loss 1.8448, val loss 1.9546\niter 1499: loss 1.8117, time 4552.93ms, mfu 0.00%\nstep 1500: train loss 1.8444, val loss 1.9608\niter 1500: loss 1.7957, time 4693.13ms, mfu 0.00%\nstep 1501: train loss 1.8459, val loss 1.9566\niter 1501: loss 1.7881, time 4590.02ms, mfu 0.00%\nstep 1502: train loss 1.8554, val loss 1.9586\niter 1502: loss 1.8861, time 4675.56ms, mfu 0.00%\nstep 1503: train loss 1.8454, val loss 1.9570\niter 1503: loss 1.8902, time 5394.84ms, mfu 0.00%\nstep 1504: train loss 1.8594, val loss 1.9651\niter 1504: loss 1.9029, time 4650.86ms, mfu 0.00%\nstep 1505: train loss 1.8571, val loss 1.9674\niter 1505: loss 1.8517, time 4709.08ms, mfu 0.00%\nstep 1506: train loss 1.8563, val loss 1.9618\niter 1506: loss 1.8681, time 4635.36ms, mfu 0.00%\nstep 1507: train loss 1.8563, val loss 1.9660\niter 1507: loss 1.9119, time 4542.68ms, mfu 0.00%\nstep 1508: train loss 1.8536, val loss 1.9633\niter 1508: loss 1.8495, time 4499.01ms, mfu 0.00%\nstep 1509: train loss 1.8494, val loss 1.9645\niter 1509: loss 1.9093, time 4453.44ms, mfu 0.00%\nstep 1510: train loss 1.8512, val loss 1.9598\niter 1510: loss 1.9285, time 5121.28ms, mfu 0.00%\nstep 1511: train loss 1.8505, val loss 1.9607\niter 1511: loss 1.9212, time 4409.20ms, mfu 0.00%\nstep 1512: train loss 1.8470, val loss 1.9717\niter 1512: loss 1.8396, time 4352.36ms, mfu 0.00%\nstep 1513: train loss 1.8539, val loss 1.9620\niter 1513: loss 1.9148, time 4481.87ms, mfu 0.00%\nstep 1514: train loss 1.8532, val loss 1.9597\niter 1514: loss 1.7818, time 4412.38ms, mfu 0.00%\nstep 1515: train loss 1.8520, val loss 1.9645\niter 1515: loss 1.9366, time 4572.47ms, mfu 0.00%\nstep 1516: train loss 1.8544, val loss 1.9625\niter 1516: loss 1.8971, time 4787.18ms, mfu 0.00%\nstep 1517: train loss 1.8460, val loss 1.9616\niter 1517: loss 1.7295, time 5331.16ms, mfu 0.00%\nstep 1518: train loss 1.8506, val loss 1.9567\niter 1518: loss 1.7852, time 4745.64ms, mfu 0.00%\nstep 1519: train loss 1.8482, val loss 1.9550\niter 1519: loss 1.9075, time 4587.51ms, mfu 0.00%\nstep 1520: train loss 1.8411, val loss 1.9642\niter 1520: loss 1.8132, time 4507.50ms, mfu 0.00%\nstep 1521: train loss 1.8431, val loss 1.9578\niter 1521: loss 1.9470, time 4510.34ms, mfu 0.00%\nstep 1522: train loss 1.8484, val loss 1.9476\nsaving checkpoint to out-shakespeare-char\niter 1522: loss 1.8805, time 4564.78ms, mfu 0.00%\nstep 1523: train loss 1.8412, val loss 1.9575\niter 1523: loss 1.8660, time 4550.06ms, mfu 0.00%\nstep 1524: train loss 1.8453, val loss 1.9644\niter 1524: loss 1.7651, time 5209.04ms, mfu 0.00%\nstep 1525: train loss 1.8467, val loss 1.9570\niter 1525: loss 1.8756, time 4532.55ms, mfu 0.00%\nstep 1526: train loss 1.8562, val loss 1.9627\niter 1526: loss 1.8617, time 4574.05ms, mfu 0.00%\nstep 1527: train loss 1.8489, val loss 1.9648\niter 1527: loss 1.7304, time 4507.60ms, mfu 0.00%\nstep 1528: train loss 1.8472, val loss 1.9637\niter 1528: loss 1.8671, time 4658.28ms, mfu 0.00%\nstep 1529: train loss 1.8458, val loss 1.9721\niter 1529: loss 1.8534, time 4476.51ms, mfu 0.00%\nstep 1530: train loss 1.8432, val loss 1.9637\niter 1530: loss 1.8107, time 4566.22ms, mfu 0.00%\nstep 1531: train loss 1.8418, val loss 1.9577\niter 1531: loss 1.8918, time 5255.49ms, mfu 0.00%\nstep 1532: train loss 1.8417, val loss 1.9561\niter 1532: loss 1.7784, time 4568.87ms, mfu 0.00%\nstep 1533: train loss 1.8411, val loss 1.9539\niter 1533: loss 1.8485, time 4536.41ms, mfu 0.00%\nstep 1534: train loss 1.8477, val loss 1.9582\niter 1534: loss 1.9214, time 4515.47ms, mfu 0.00%\nstep 1535: train loss 1.8564, val loss 1.9611\niter 1535: loss 1.8900, time 4495.28ms, mfu 0.00%\nstep 1536: train loss 1.8536, val loss 1.9572\niter 1536: loss 1.6667, time 4501.81ms, mfu 0.00%\nstep 1537: train loss 1.8415, val loss 1.9637\niter 1537: loss 1.8385, time 4515.74ms, mfu 0.00%\nstep 1538: train loss 1.8406, val loss 1.9532\niter 1538: loss 1.8086, time 5244.87ms, mfu 0.00%\nstep 1539: train loss 1.8392, val loss 1.9507\niter 1539: loss 1.7834, time 4520.80ms, mfu 0.00%\nstep 1540: train loss 1.8403, val loss 1.9552\niter 1540: loss 1.9318, time 4452.41ms, mfu 0.00%\nstep 1541: train loss 1.8380, val loss 1.9434\nsaving checkpoint to out-shakespeare-char\niter 1541: loss 1.7995, time 4678.07ms, mfu 0.00%\nstep 1542: train loss 1.8438, val loss 1.9565\niter 1542: loss 1.8071, time 4462.42ms, mfu 0.00%\nstep 1543: train loss 1.8303, val loss 1.9518\niter 1543: loss 1.8999, time 4610.05ms, mfu 0.00%\nstep 1544: train loss 1.8370, val loss 1.9416\nsaving checkpoint to out-shakespeare-char\niter 1544: loss 1.7976, time 4579.75ms, mfu 0.00%\nstep 1545: train loss 1.8306, val loss 1.9529\niter 1545: loss 1.7722, time 5108.23ms, mfu 0.00%\nstep 1546: train loss 1.8354, val loss 1.9609\niter 1546: loss 1.7760, time 4458.21ms, mfu 0.00%\nstep 1547: train loss 1.8319, val loss 1.9515\niter 1547: loss 1.8530, time 4441.78ms, mfu 0.00%\nstep 1548: train loss 1.8364, val loss 1.9520\niter 1548: loss 1.8766, time 4423.33ms, mfu 0.00%\nstep 1549: train loss 1.8351, val loss 1.9473\niter 1549: loss 1.8122, time 4467.27ms, mfu 0.00%\nstep 1550: train loss 1.8359, val loss 1.9678\niter 1550: loss 1.7673, time 4397.77ms, mfu 0.00%\nstep 1551: train loss 1.8358, val loss 1.9604\niter 1551: loss 1.8856, time 4762.67ms, mfu 0.00%\nstep 1552: train loss 1.8359, val loss 1.9498\niter 1552: loss 1.7441, time 5369.00ms, mfu 0.00%\nstep 1553: train loss 1.8443, val loss 1.9591\niter 1553: loss 1.8998, time 4728.92ms, mfu 0.00%\nstep 1554: train loss 1.8431, val loss 1.9501\niter 1554: loss 1.8577, time 4889.23ms, mfu 0.00%\nstep 1555: train loss 1.8387, val loss 1.9505\niter 1555: loss 1.8368, time 4590.66ms, mfu 0.00%\nstep 1556: train loss 1.8375, val loss 1.9569\niter 1556: loss 1.8806, time 4594.91ms, mfu 0.00%\nstep 1557: train loss 1.8326, val loss 1.9550\niter 1557: loss 1.9063, time 4489.00ms, mfu 0.00%\nstep 1558: train loss 1.8434, val loss 1.9568\niter 1558: loss 1.9069, time 4504.98ms, mfu 0.00%\nstep 1559: train loss 1.8326, val loss 1.9581\niter 1559: loss 1.7650, time 5125.05ms, mfu 0.00%\nstep 1560: train loss 1.8267, val loss 1.9530\niter 1560: loss 1.9466, time 4507.56ms, mfu 0.00%\nstep 1561: train loss 1.8340, val loss 1.9512\niter 1561: loss 1.7976, time 4489.51ms, mfu 0.00%\nstep 1562: train loss 1.8454, val loss 1.9537\niter 1562: loss 1.8935, time 4461.37ms, mfu 0.00%\nstep 1563: train loss 1.8305, val loss 1.9595\niter 1563: loss 1.9700, time 4433.80ms, mfu 0.00%\nstep 1564: train loss 1.8323, val loss 1.9437\niter 1564: loss 1.7481, time 4546.61ms, mfu 0.00%\nstep 1565: train loss 1.8412, val loss 1.9477\niter 1565: loss 1.8982, time 4490.19ms, mfu 0.00%\nstep 1566: train loss 1.8399, val loss 1.9440\niter 1566: loss 1.8325, time 5251.62ms, mfu 0.00%\nstep 1567: train loss 1.8389, val loss 1.9451\niter 1567: loss 1.8400, time 4620.58ms, mfu 0.00%\nstep 1568: train loss 1.8356, val loss 1.9465\niter 1568: loss 1.8632, time 4497.41ms, mfu 0.00%\nstep 1569: train loss 1.8379, val loss 1.9439\niter 1569: loss 1.8058, time 4592.75ms, mfu 0.00%\nstep 1570: train loss 1.8389, val loss 1.9472\niter 1570: loss 1.7534, time 4458.91ms, mfu 0.00%\nstep 1571: train loss 1.8400, val loss 1.9437\niter 1571: loss 1.7786, time 4529.88ms, mfu 0.00%\nstep 1572: train loss 1.8347, val loss 1.9507\niter 1572: loss 1.8223, time 4491.84ms, mfu 0.00%\nstep 1573: train loss 1.8409, val loss 1.9426\niter 1573: loss 1.7721, time 5223.13ms, mfu 0.00%\nstep 1574: train loss 1.8386, val loss 1.9391\nsaving checkpoint to out-shakespeare-char\niter 1574: loss 1.8764, time 4578.46ms, mfu 0.00%\nstep 1575: train loss 1.8394, val loss 1.9442\niter 1575: loss 1.8098, time 4540.43ms, mfu 0.00%\nstep 1576: train loss 1.8397, val loss 1.9499\niter 1576: loss 1.8122, time 4401.87ms, mfu 0.00%\nstep 1577: train loss 1.8387, val loss 1.9440\niter 1577: loss 1.7751, time 4395.35ms, mfu 0.00%\nstep 1578: train loss 1.8349, val loss 1.9550\niter 1578: loss 1.8105, time 4474.03ms, mfu 0.00%\nstep 1579: train loss 1.8374, val loss 1.9607\niter 1579: loss 1.7965, time 4479.12ms, mfu 0.00%\nstep 1580: train loss 1.8356, val loss 1.9549\niter 1580: loss 1.8383, time 5252.32ms, mfu 0.00%\nstep 1581: train loss 1.8336, val loss 1.9533\niter 1581: loss 2.0217, time 4453.69ms, mfu 0.00%\nstep 1582: train loss 1.8341, val loss 1.9462\niter 1582: loss 1.8006, time 4591.40ms, mfu 0.00%\nstep 1583: train loss 1.8415, val loss 1.9385\nsaving checkpoint to out-shakespeare-char\niter 1583: loss 1.8777, time 4505.89ms, mfu 0.00%\nstep 1584: train loss 1.8254, val loss 1.9516\niter 1584: loss 1.8582, time 4506.35ms, mfu 0.00%\nstep 1585: train loss 1.8356, val loss 1.9456\niter 1585: loss 1.8700, time 4513.10ms, mfu 0.00%\nstep 1586: train loss 1.8364, val loss 1.9503\niter 1586: loss 1.9294, time 4814.31ms, mfu 0.00%\nstep 1587: train loss 1.8311, val loss 1.9435\niter 1587: loss 1.8635, time 5529.81ms, mfu 0.00%\nstep 1588: train loss 1.8334, val loss 1.9438\niter 1588: loss 1.7805, time 4815.64ms, mfu 0.00%\nstep 1589: train loss 1.8346, val loss 1.9458\niter 1589: loss 1.8595, time 4671.83ms, mfu 0.00%\nstep 1590: train loss 1.8291, val loss 1.9399\niter 1590: loss 1.8069, time 4577.01ms, mfu 0.00%\nstep 1591: train loss 1.8304, val loss 1.9379\nsaving checkpoint to out-shakespeare-char\niter 1591: loss 1.8207, time 4570.62ms, mfu 0.00%\nstep 1592: train loss 1.8252, val loss 1.9409\niter 1592: loss 1.8027, time 4564.74ms, mfu 0.00%\nstep 1593: train loss 1.8267, val loss 1.9295\nsaving checkpoint to out-shakespeare-char\niter 1593: loss 1.8343, time 4748.80ms, mfu 0.00%\nstep 1594: train loss 1.8306, val loss 1.9428\niter 1594: loss 1.8016, time 5157.56ms, mfu 0.00%\nstep 1595: train loss 1.8306, val loss 1.9516\niter 1595: loss 1.7657, time 4703.20ms, mfu 0.00%\nstep 1596: train loss 1.8379, val loss 1.9442\niter 1596: loss 1.8420, time 4539.71ms, mfu 0.00%\nstep 1597: train loss 1.8360, val loss 1.9467\niter 1597: loss 1.8396, time 4659.62ms, mfu 0.00%\nstep 1598: train loss 1.8291, val loss 1.9533\niter 1598: loss 1.8585, time 4551.61ms, mfu 0.00%\nstep 1599: train loss 1.8369, val loss 1.9499\niter 1599: loss 1.8752, time 4607.49ms, mfu 0.00%\nstep 1600: train loss 1.8357, val loss 1.9529\niter 1600: loss 1.7869, time 4682.96ms, mfu 0.00%\nstep 1601: train loss 1.8327, val loss 1.9517\niter 1601: loss 1.7817, time 5325.09ms, mfu 0.00%\nstep 1602: train loss 1.8275, val loss 1.9556\niter 1602: loss 1.7937, time 4682.38ms, mfu 0.00%\nstep 1603: train loss 1.8375, val loss 1.9525\niter 1603: loss 1.7843, time 4607.60ms, mfu 0.00%\nstep 1604: train loss 1.8389, val loss 1.9582\niter 1604: loss 1.8119, time 4606.32ms, mfu 0.00%\nstep 1605: train loss 1.8340, val loss 1.9555\niter 1605: loss 1.8014, time 4685.85ms, mfu 0.00%\nstep 1606: train loss 1.8320, val loss 1.9506\niter 1606: loss 1.8564, time 4627.23ms, mfu 0.00%\nstep 1607: train loss 1.8383, val loss 1.9477\niter 1607: loss 1.9138, time 5269.49ms, mfu 0.00%\nstep 1608: train loss 1.8316, val loss 1.9467\niter 1608: loss 1.9369, time 4953.57ms, mfu 0.00%\nstep 1609: train loss 1.8310, val loss 1.9555\niter 1609: loss 1.9282, time 4789.62ms, mfu 0.00%\nstep 1610: train loss 1.8403, val loss 1.9461\niter 1610: loss 1.7995, time 4666.63ms, mfu 0.00%\nstep 1611: train loss 1.8327, val loss 1.9487\niter 1611: loss 1.8684, time 4643.05ms, mfu 0.00%\nstep 1612: train loss 1.8302, val loss 1.9475\niter 1612: loss 1.8892, time 4686.29ms, mfu 0.00%\nstep 1613: train loss 1.8393, val loss 1.9537\niter 1613: loss 1.8723, time 4666.50ms, mfu 0.00%\nstep 1614: train loss 1.8352, val loss 1.9550\niter 1614: loss 1.7806, time 5108.46ms, mfu 0.00%\nstep 1615: train loss 1.8420, val loss 1.9459\niter 1615: loss 1.8766, time 4553.54ms, mfu 0.00%\nstep 1616: train loss 1.8319, val loss 1.9475\niter 1616: loss 1.8132, time 4594.61ms, mfu 0.00%\nstep 1617: train loss 1.8288, val loss 1.9480\niter 1617: loss 1.7720, time 4557.74ms, mfu 0.00%\nstep 1618: train loss 1.8293, val loss 1.9510\niter 1618: loss 1.8088, time 4672.49ms, mfu 0.00%\nstep 1619: train loss 1.8334, val loss 1.9513\niter 1619: loss 1.8545, time 4564.67ms, mfu 0.00%\nstep 1620: train loss 1.8210, val loss 1.9364\niter 1620: loss 1.9384, time 4689.38ms, mfu 0.00%\nstep 1621: train loss 1.8278, val loss 1.9396\niter 1621: loss 1.8518, time 5267.18ms, mfu 0.00%\nstep 1622: train loss 1.8260, val loss 1.9454\niter 1622: loss 1.8566, time 4656.12ms, mfu 0.00%\nstep 1623: train loss 1.8256, val loss 1.9457\niter 1623: loss 1.9116, time 4539.62ms, mfu 0.00%\nstep 1624: train loss 1.8212, val loss 1.9472\niter 1624: loss 1.8727, time 4834.74ms, mfu 0.00%\nstep 1625: train loss 1.8250, val loss 1.9485\niter 1625: loss 1.8928, time 4813.62ms, mfu 0.00%\nstep 1626: train loss 1.8244, val loss 1.9424\niter 1626: loss 1.8705, time 4934.89ms, mfu 0.00%\nstep 1627: train loss 1.8362, val loss 1.9508\niter 1627: loss 1.8438, time 4807.54ms, mfu 0.00%\nstep 1628: train loss 1.8297, val loss 1.9420\niter 1628: loss 1.8468, time 5306.67ms, mfu 0.00%\nstep 1629: train loss 1.8195, val loss 1.9430\niter 1629: loss 1.7477, time 4603.47ms, mfu 0.00%\nstep 1630: train loss 1.8244, val loss 1.9476\niter 1630: loss 1.8399, time 4646.13ms, mfu 0.00%\nstep 1631: train loss 1.8213, val loss 1.9448\niter 1631: loss 1.8154, time 4624.39ms, mfu 0.00%\nstep 1632: train loss 1.8186, val loss 1.9506\niter 1632: loss 1.7063, time 4638.80ms, mfu 0.00%\nstep 1633: train loss 1.8179, val loss 1.9342\niter 1633: loss 1.8609, time 4618.20ms, mfu 0.00%\nstep 1634: train loss 1.8243, val loss 1.9413\niter 1634: loss 1.8209, time 4779.63ms, mfu 0.00%\nstep 1635: train loss 1.8204, val loss 1.9270\nsaving checkpoint to out-shakespeare-char\niter 1635: loss 1.6879, time 5332.64ms, mfu 0.00%\nstep 1636: train loss 1.8282, val loss 1.9401\niter 1636: loss 1.8312, time 4752.62ms, mfu 0.00%\nstep 1637: train loss 1.8247, val loss 1.9382\niter 1637: loss 1.8319, time 4747.97ms, mfu 0.00%\nstep 1638: train loss 1.8155, val loss 1.9276\niter 1638: loss 1.8462, time 4605.59ms, mfu 0.00%\nstep 1639: train loss 1.8243, val loss 1.9369\niter 1639: loss 1.7853, time 4758.02ms, mfu 0.00%\nstep 1640: train loss 1.8285, val loss 1.9393\niter 1640: loss 1.8085, time 4657.99ms, mfu 0.00%\nstep 1641: train loss 1.8266, val loss 1.9234\nsaving checkpoint to out-shakespeare-char\niter 1641: loss 1.7009, time 5329.96ms, mfu 0.00%\nstep 1642: train loss 1.8228, val loss 1.9380\niter 1642: loss 1.7151, time 4726.95ms, mfu 0.00%\nstep 1643: train loss 1.8247, val loss 1.9338\niter 1643: loss 1.8197, time 4782.74ms, mfu 0.00%\nstep 1644: train loss 1.8300, val loss 1.9253\niter 1644: loss 1.9383, time 4649.46ms, mfu 0.00%\nstep 1645: train loss 1.8206, val loss 1.9321\niter 1645: loss 1.8356, time 4645.72ms, mfu 0.00%\nstep 1646: train loss 1.8262, val loss 1.9423\niter 1646: loss 1.6533, time 4644.95ms, mfu 0.00%\nstep 1647: train loss 1.8280, val loss 1.9308\niter 1647: loss 1.8084, time 4676.26ms, mfu 0.00%\nstep 1648: train loss 1.8176, val loss 1.9319\niter 1648: loss 1.8494, time 5068.32ms, mfu 0.00%\nstep 1649: train loss 1.8162, val loss 1.9310\niter 1649: loss 1.7978, time 4467.80ms, mfu 0.00%\nstep 1650: train loss 1.8163, val loss 1.9285\niter 1650: loss 1.7568, time 4492.95ms, mfu 0.00%\nstep 1651: train loss 1.8164, val loss 1.9298\niter 1651: loss 1.7903, time 4450.29ms, mfu 0.00%\nstep 1652: train loss 1.8217, val loss 1.9387\niter 1652: loss 1.9186, time 4477.99ms, mfu 0.00%\nstep 1653: train loss 1.8232, val loss 1.9297\niter 1653: loss 1.8333, time 4484.83ms, mfu 0.00%\nstep 1654: train loss 1.8235, val loss 1.9280\niter 1654: loss 1.7717, time 4355.25ms, mfu 0.00%\nstep 1655: train loss 1.8212, val loss 1.9411\niter 1655: loss 1.8197, time 5095.42ms, mfu 0.00%\nstep 1656: train loss 1.8125, val loss 1.9447\niter 1656: loss 1.7931, time 4510.12ms, mfu 0.00%\nstep 1657: train loss 1.8292, val loss 1.9399\niter 1657: loss 1.8747, time 4333.74ms, mfu 0.00%\nstep 1658: train loss 1.8160, val loss 1.9317\niter 1658: loss 1.8448, time 4679.85ms, mfu 0.00%\nstep 1659: train loss 1.8160, val loss 1.9344\niter 1659: loss 1.7688, time 4586.44ms, mfu 0.00%\nstep 1660: train loss 1.8224, val loss 1.9464\niter 1660: loss 1.8009, time 4727.99ms, mfu 0.00%\nstep 1661: train loss 1.8200, val loss 1.9367\niter 1661: loss 1.8187, time 4591.18ms, mfu 0.00%\nstep 1662: train loss 1.8234, val loss 1.9342\niter 1662: loss 1.8387, time 5214.67ms, mfu 0.00%\nstep 1663: train loss 1.8241, val loss 1.9422\niter 1663: loss 1.9943, time 4619.60ms, mfu 0.00%\nstep 1664: train loss 1.8214, val loss 1.9397\niter 1664: loss 1.8081, time 4383.36ms, mfu 0.00%\nstep 1665: train loss 1.8134, val loss 1.9316\niter 1665: loss 1.8028, time 4496.27ms, mfu 0.00%\nstep 1666: train loss 1.8157, val loss 1.9411\niter 1666: loss 1.8343, time 4338.96ms, mfu 0.00%\nstep 1667: train loss 1.8127, val loss 1.9356\niter 1667: loss 1.9602, time 4328.03ms, mfu 0.00%\nstep 1668: train loss 1.8171, val loss 1.9352\niter 1668: loss 1.7633, time 4409.74ms, mfu 0.00%\nstep 1669: train loss 1.8155, val loss 1.9365\niter 1669: loss 1.8044, time 4971.82ms, mfu 0.00%\nstep 1670: train loss 1.8158, val loss 1.9332\niter 1670: loss 1.8733, time 4484.81ms, mfu 0.00%\nstep 1671: train loss 1.8185, val loss 1.9333\niter 1671: loss 1.7676, time 4481.20ms, mfu 0.00%\nstep 1672: train loss 1.8158, val loss 1.9362\niter 1672: loss 1.8679, time 4372.70ms, mfu 0.00%\nstep 1673: train loss 1.8173, val loss 1.9290\niter 1673: loss 1.8433, time 4618.80ms, mfu 0.00%\nstep 1674: train loss 1.8084, val loss 1.9253\niter 1674: loss 1.8780, time 4533.59ms, mfu 0.00%\nstep 1675: train loss 1.8207, val loss 1.9295\niter 1675: loss 1.8108, time 4650.11ms, mfu 0.00%\nstep 1676: train loss 1.8087, val loss 1.9324\niter 1676: loss 1.7372, time 5257.01ms, mfu 0.00%\nstep 1677: train loss 1.8101, val loss 1.9313\niter 1677: loss 1.9348, time 4795.53ms, mfu 0.00%\nstep 1678: train loss 1.8093, val loss 1.9264\niter 1678: loss 1.9088, time 4631.58ms, mfu 0.00%\nstep 1679: train loss 1.8146, val loss 1.9263\niter 1679: loss 1.8977, time 4614.50ms, mfu 0.00%\nstep 1680: train loss 1.8154, val loss 1.9320\niter 1680: loss 1.7966, time 4630.98ms, mfu 0.00%\nstep 1681: train loss 1.8122, val loss 1.9198\nsaving checkpoint to out-shakespeare-char\niter 1681: loss 1.8163, time 4521.72ms, mfu 0.00%\nstep 1682: train loss 1.8112, val loss 1.9317\niter 1682: loss 1.9463, time 4585.74ms, mfu 0.00%\nstep 1683: train loss 1.8013, val loss 1.9274\niter 1683: loss 1.8769, time 5118.52ms, mfu 0.00%\nstep 1684: train loss 1.8125, val loss 1.9234\niter 1684: loss 1.9043, time 4528.57ms, mfu 0.00%\nstep 1685: train loss 1.8144, val loss 1.9311\niter 1685: loss 1.7021, time 4516.97ms, mfu 0.00%\nstep 1686: train loss 1.8118, val loss 1.9357\niter 1686: loss 1.8270, time 4584.59ms, mfu 0.00%\nstep 1687: train loss 1.8131, val loss 1.9282\niter 1687: loss 1.7587, time 4379.33ms, mfu 0.00%\nstep 1688: train loss 1.8171, val loss 1.9296\niter 1688: loss 1.9220, time 4672.00ms, mfu 0.00%\nstep 1689: train loss 1.8183, val loss 1.9274\niter 1689: loss 1.8856, time 4535.24ms, mfu 0.00%\nstep 1690: train loss 1.8185, val loss 1.9243\niter 1690: loss 1.8448, time 5199.37ms, mfu 0.00%\nstep 1691: train loss 1.8092, val loss 1.9277\niter 1691: loss 1.7444, time 5516.79ms, mfu 0.00%\nstep 1692: train loss 1.8197, val loss 1.9345\niter 1692: loss 1.8775, time 4789.50ms, mfu 0.00%\nstep 1693: train loss 1.8096, val loss 1.9281\niter 1693: loss 1.9010, time 4869.23ms, mfu 0.00%\nstep 1694: train loss 1.8187, val loss 1.9263\niter 1694: loss 1.7866, time 4836.04ms, mfu 0.00%\nstep 1695: train loss 1.8027, val loss 1.9277\niter 1695: loss 1.8525, time 4703.79ms, mfu 0.00%\nstep 1696: train loss 1.8155, val loss 1.9282\niter 1696: loss 1.8459, time 5007.08ms, mfu 0.00%\nstep 1697: train loss 1.8143, val loss 1.9334\niter 1697: loss 1.8432, time 5615.59ms, mfu 0.00%\nstep 1698: train loss 1.8200, val loss 1.9287\niter 1698: loss 1.8473, time 4774.96ms, mfu 0.00%\nstep 1699: train loss 1.8138, val loss 1.9238\niter 1699: loss 1.7101, time 4808.58ms, mfu 0.00%\nstep 1700: train loss 1.8090, val loss 1.9334\niter 1700: loss 1.8649, time 4958.52ms, mfu 0.00%\nstep 1701: train loss 1.8099, val loss 1.9306\niter 1701: loss 1.8329, time 4943.62ms, mfu 0.00%\nstep 1702: train loss 1.8136, val loss 1.9228\niter 1702: loss 1.6989, time 4896.79ms, mfu 0.00%\nstep 1703: train loss 1.8127, val loss 1.9348\niter 1703: loss 1.8513, time 5599.09ms, mfu 0.00%\nstep 1704: train loss 1.8049, val loss 1.9356\niter 1704: loss 1.8298, time 4968.26ms, mfu 0.00%\nstep 1705: train loss 1.8046, val loss 1.9255\niter 1705: loss 1.7835, time 4911.02ms, mfu 0.00%\nstep 1706: train loss 1.8101, val loss 1.9251\niter 1706: loss 1.7643, time 4879.65ms, mfu 0.00%\nstep 1707: train loss 1.8096, val loss 1.9297\niter 1707: loss 1.8640, time 4804.36ms, mfu 0.00%\nstep 1708: train loss 1.8059, val loss 1.9178\nsaving checkpoint to out-shakespeare-char\niter 1708: loss 1.7673, time 4742.28ms, mfu 0.00%\nstep 1709: train loss 1.8124, val loss 1.9244\niter 1709: loss 1.8523, time 4773.70ms, mfu 0.00%\nstep 1710: train loss 1.8071, val loss 1.9280\niter 1710: loss 1.6891, time 5316.59ms, mfu 0.00%\nstep 1711: train loss 1.8112, val loss 1.9178\nsaving checkpoint to out-shakespeare-char\niter 1711: loss 1.8152, time 4762.02ms, mfu 0.00%\nstep 1712: train loss 1.8086, val loss 1.9159\nsaving checkpoint to out-shakespeare-char\niter 1712: loss 1.6768, time 4811.62ms, mfu 0.00%\nstep 1713: train loss 1.8079, val loss 1.9258\niter 1713: loss 1.8860, time 4818.63ms, mfu 0.00%\nstep 1714: train loss 1.8061, val loss 1.9281\niter 1714: loss 1.8280, time 4731.46ms, mfu 0.00%\nstep 1715: train loss 1.8114, val loss 1.9280\niter 1715: loss 1.9072, time 4718.21ms, mfu 0.00%\nstep 1716: train loss 1.8005, val loss 1.9373\niter 1716: loss 1.9110, time 4772.90ms, mfu 0.00%\nstep 1717: train loss 1.8027, val loss 1.9328\niter 1717: loss 1.8432, time 5445.02ms, mfu 0.00%\nstep 1718: train loss 1.8083, val loss 1.9253\niter 1718: loss 1.8555, time 4475.36ms, mfu 0.00%\nstep 1719: train loss 1.8033, val loss 1.9321\niter 1719: loss 1.7963, time 4715.20ms, mfu 0.00%\nstep 1720: train loss 1.8069, val loss 1.9322\niter 1720: loss 1.7717, time 4704.47ms, mfu 0.00%\nstep 1721: train loss 1.8100, val loss 1.9204\niter 1721: loss 1.8223, time 4416.74ms, mfu 0.00%\nstep 1722: train loss 1.8117, val loss 1.9424\niter 1722: loss 1.7606, time 4428.04ms, mfu 0.00%\nstep 1723: train loss 1.8083, val loss 1.9351\niter 1723: loss 1.7299, time 4551.03ms, mfu 0.00%\nstep 1724: train loss 1.8108, val loss 1.9278\niter 1724: loss 1.7128, time 4826.19ms, mfu 0.00%\nstep 1725: train loss 1.8183, val loss 1.9272\niter 1725: loss 1.8153, time 4407.11ms, mfu 0.00%\nstep 1726: train loss 1.8169, val loss 1.9250\niter 1726: loss 1.8393, time 4356.98ms, mfu 0.00%\nstep 1727: train loss 1.8138, val loss 1.9276\niter 1727: loss 1.8578, time 4579.04ms, mfu 0.00%\nstep 1728: train loss 1.8081, val loss 1.9316\niter 1728: loss 1.8933, time 4665.45ms, mfu 0.00%\nstep 1729: train loss 1.8003, val loss 1.9363\niter 1729: loss 1.8446, time 4676.54ms, mfu 0.00%\nstep 1730: train loss 1.8155, val loss 1.9288\niter 1730: loss 1.7991, time 4489.05ms, mfu 0.00%\nstep 1731: train loss 1.8109, val loss 1.9273\niter 1731: loss 1.7266, time 5025.74ms, mfu 0.00%\nstep 1732: train loss 1.8057, val loss 1.9266\niter 1732: loss 1.7724, time 4495.40ms, mfu 0.00%\nstep 1733: train loss 1.8061, val loss 1.9236\niter 1733: loss 1.8139, time 4584.75ms, mfu 0.00%\nstep 1734: train loss 1.8144, val loss 1.9277\niter 1734: loss 1.8608, time 4560.02ms, mfu 0.00%\nstep 1735: train loss 1.8049, val loss 1.9221\niter 1735: loss 1.8330, time 4474.85ms, mfu 0.00%\nstep 1736: train loss 1.8139, val loss 1.9210\niter 1736: loss 1.7541, time 4526.01ms, mfu 0.00%\nstep 1737: train loss 1.8038, val loss 1.9123\nsaving checkpoint to out-shakespeare-char\niter 1737: loss 1.7918, time 4373.59ms, mfu 0.00%\nstep 1738: train loss 1.8080, val loss 1.9207\niter 1738: loss 1.8241, time 5159.60ms, mfu 0.00%\nstep 1739: train loss 1.8054, val loss 1.9223\niter 1739: loss 1.8753, time 4579.68ms, mfu 0.00%\nstep 1740: train loss 1.8079, val loss 1.9180\niter 1740: loss 1.7532, time 4638.87ms, mfu 0.00%\nstep 1741: train loss 1.8097, val loss 1.9219\niter 1741: loss 1.7079, time 4734.98ms, mfu 0.00%\nstep 1742: train loss 1.7985, val loss 1.9252\niter 1742: loss 1.8372, time 4502.88ms, mfu 0.00%\nstep 1743: train loss 1.8002, val loss 1.9206\niter 1743: loss 1.7341, time 4494.86ms, mfu 0.00%\nstep 1744: train loss 1.8119, val loss 1.9160\niter 1744: loss 1.7735, time 4519.75ms, mfu 0.00%\nstep 1745: train loss 1.8067, val loss 1.9212\niter 1745: loss 1.7888, time 5149.01ms, mfu 0.00%\nstep 1746: train loss 1.8128, val loss 1.9288\niter 1746: loss 1.8587, time 4438.00ms, mfu 0.00%\nstep 1747: train loss 1.8122, val loss 1.9186\niter 1747: loss 1.7678, time 4471.35ms, mfu 0.00%\nstep 1748: train loss 1.8135, val loss 1.9210\niter 1748: loss 1.8173, time 4478.99ms, mfu 0.00%\nstep 1749: train loss 1.8069, val loss 1.9223\niter 1749: loss 1.8148, time 4440.59ms, mfu 0.00%\nstep 1750: train loss 1.8015, val loss 1.9237\niter 1750: loss 1.7158, time 4754.19ms, mfu 0.00%\nstep 1751: train loss 1.8107, val loss 1.9258\niter 1751: loss 1.8221, time 4739.50ms, mfu 0.00%\nstep 1752: train loss 1.8079, val loss 1.9313\niter 1752: loss 1.8844, time 5046.85ms, mfu 0.00%\nstep 1753: train loss 1.8064, val loss 1.9273\niter 1753: loss 1.7897, time 4506.65ms, mfu 0.00%\nstep 1754: train loss 1.8063, val loss 1.9322\niter 1754: loss 1.8384, time 4481.07ms, mfu 0.00%\nstep 1755: train loss 1.8044, val loss 1.9262\niter 1755: loss 1.8573, time 4607.20ms, mfu 0.00%\nstep 1756: train loss 1.8030, val loss 1.9239\niter 1756: loss 1.7963, time 4489.29ms, mfu 0.00%\nstep 1757: train loss 1.8021, val loss 1.9220\niter 1757: loss 1.7569, time 4477.92ms, mfu 0.00%\nstep 1758: train loss 1.8098, val loss 1.9132\niter 1758: loss 1.8376, time 4918.28ms, mfu 0.00%\nstep 1759: train loss 1.8038, val loss 1.9238\niter 1759: loss 1.7443, time 4710.32ms, mfu 0.00%\nstep 1760: train loss 1.8031, val loss 1.9219\niter 1760: loss 1.7927, time 4512.36ms, mfu 0.00%\nstep 1761: train loss 1.8030, val loss 1.9230\niter 1761: loss 1.8323, time 4562.60ms, mfu 0.00%\nstep 1762: train loss 1.8037, val loss 1.9140\niter 1762: loss 1.7413, time 4608.94ms, mfu 0.00%\nstep 1763: train loss 1.8084, val loss 1.9088\nsaving checkpoint to out-shakespeare-char\niter 1763: loss 1.8529, time 4545.27ms, mfu 0.00%\nstep 1764: train loss 1.7953, val loss 1.9202\niter 1764: loss 1.7594, time 4480.49ms, mfu 0.00%\nstep 1765: train loss 1.8092, val loss 1.9168\niter 1765: loss 1.8641, time 4564.44ms, mfu 0.00%\nstep 1766: train loss 1.7971, val loss 1.9222\niter 1766: loss 1.8382, time 4710.09ms, mfu 0.00%\nstep 1767: train loss 1.8074, val loss 1.9144\niter 1767: loss 1.8340, time 4342.19ms, mfu 0.00%\nstep 1768: train loss 1.8041, val loss 1.9162\niter 1768: loss 1.8912, time 4333.78ms, mfu 0.00%\nstep 1769: train loss 1.7987, val loss 1.9119\niter 1769: loss 1.7323, time 4538.72ms, mfu 0.00%\nstep 1770: train loss 1.7999, val loss 1.9182\niter 1770: loss 1.8224, time 4421.41ms, mfu 0.00%\nstep 1771: train loss 1.7961, val loss 1.9212\niter 1771: loss 1.7460, time 4826.84ms, mfu 0.00%\nstep 1772: train loss 1.8059, val loss 1.9166\niter 1772: loss 1.8927, time 4916.83ms, mfu 0.00%\nstep 1773: train loss 1.8024, val loss 1.9199\niter 1773: loss 1.6879, time 4966.60ms, mfu 0.00%\nstep 1774: train loss 1.8091, val loss 1.9170\niter 1774: loss 1.7314, time 4475.68ms, mfu 0.00%\nstep 1775: train loss 1.8006, val loss 1.9232\niter 1775: loss 1.7895, time 4546.52ms, mfu 0.00%\nstep 1776: train loss 1.8028, val loss 1.9228\niter 1776: loss 1.7784, time 4463.90ms, mfu 0.00%\nstep 1777: train loss 1.8006, val loss 1.9259\niter 1777: loss 1.8075, time 4527.12ms, mfu 0.00%\nstep 1778: train loss 1.8004, val loss 1.9236\niter 1778: loss 1.7972, time 4384.77ms, mfu 0.00%\nstep 1779: train loss 1.8028, val loss 1.9260\niter 1779: loss 1.8342, time 4447.10ms, mfu 0.00%\nstep 1780: train loss 1.8099, val loss 1.9205\niter 1780: loss 1.7508, time 4987.11ms, mfu 0.00%\nstep 1781: train loss 1.7968, val loss 1.9196\niter 1781: loss 1.7934, time 4370.71ms, mfu 0.00%\nstep 1782: train loss 1.7992, val loss 1.9292\niter 1782: loss 1.7289, time 4526.94ms, mfu 0.00%\nstep 1783: train loss 1.7985, val loss 1.9268\niter 1783: loss 1.8168, time 4475.62ms, mfu 0.00%\nstep 1784: train loss 1.7933, val loss 1.9178\niter 1784: loss 1.8069, time 4497.44ms, mfu 0.00%\nstep 1785: train loss 1.8064, val loss 1.9274\niter 1785: loss 1.8314, time 4557.28ms, mfu 0.00%\nstep 1786: train loss 1.8037, val loss 1.9268\niter 1786: loss 1.8313, time 4469.57ms, mfu 0.00%\nstep 1787: train loss 1.8002, val loss 1.9268\niter 1787: loss 1.7437, time 4948.39ms, mfu 0.00%\nstep 1788: train loss 1.8055, val loss 1.9241\niter 1788: loss 1.7836, time 4395.47ms, mfu 0.00%\nstep 1789: train loss 1.8010, val loss 1.9189\niter 1789: loss 1.8519, time 4425.92ms, mfu 0.00%\nstep 1790: train loss 1.7991, val loss 1.9132\niter 1790: loss 1.6461, time 4385.10ms, mfu 0.00%\nstep 1791: train loss 1.7995, val loss 1.9161\niter 1791: loss 1.7880, time 4483.50ms, mfu 0.00%\nstep 1792: train loss 1.7935, val loss 1.9042\nsaving checkpoint to out-shakespeare-char\niter 1792: loss 1.8426, time 4370.31ms, mfu 0.00%\nstep 1793: train loss 1.7946, val loss 1.9142\niter 1793: loss 1.8400, time 4336.23ms, mfu 0.00%\nstep 1794: train loss 1.7993, val loss 1.9161\niter 1794: loss 1.8435, time 5056.05ms, mfu 0.00%\nstep 1795: train loss 1.7985, val loss 1.9170\niter 1795: loss 1.7604, time 4613.13ms, mfu 0.00%\nstep 1796: train loss 1.8050, val loss 1.9234\niter 1796: loss 1.7629, time 4402.76ms, mfu 0.00%\nstep 1797: train loss 1.8023, val loss 1.9224\niter 1797: loss 1.8575, time 4488.40ms, mfu 0.00%\nstep 1798: train loss 1.7977, val loss 1.9222\niter 1798: loss 1.8373, time 4507.70ms, mfu 0.00%\nstep 1799: train loss 1.7930, val loss 1.9173\niter 1799: loss 1.8347, time 4518.50ms, mfu 0.00%\nstep 1800: train loss 1.8033, val loss 1.9171\niter 1800: loss 1.7459, time 4483.54ms, mfu 0.00%\nstep 1801: train loss 1.7953, val loss 1.9169\niter 1801: loss 1.7945, time 4999.71ms, mfu 0.00%\nstep 1802: train loss 1.8002, val loss 1.9214\niter 1802: loss 1.7433, time 4355.64ms, mfu 0.00%\nstep 1803: train loss 1.7953, val loss 1.9214\niter 1803: loss 1.9123, time 4447.03ms, mfu 0.00%\nstep 1804: train loss 1.7904, val loss 1.9196\niter 1804: loss 1.6891, time 4452.14ms, mfu 0.00%\nstep 1805: train loss 1.7894, val loss 1.9192\niter 1805: loss 1.8725, time 4430.85ms, mfu 0.00%\nstep 1806: train loss 1.7963, val loss 1.9222\niter 1806: loss 1.7877, time 4536.81ms, mfu 0.00%\nstep 1807: train loss 1.8006, val loss 1.9165\niter 1807: loss 1.7998, time 4775.37ms, mfu 0.00%\nstep 1808: train loss 1.7867, val loss 1.9202\niter 1808: loss 1.8362, time 5627.78ms, mfu 0.00%\nstep 1809: train loss 1.7978, val loss 1.9240\niter 1809: loss 1.7980, time 4465.84ms, mfu 0.00%\nstep 1810: train loss 1.7933, val loss 1.9151\niter 1810: loss 1.8425, time 4566.65ms, mfu 0.00%\nstep 1811: train loss 1.7917, val loss 1.9173\niter 1811: loss 1.7611, time 4502.83ms, mfu 0.00%\nstep 1812: train loss 1.7825, val loss 1.9158\niter 1812: loss 1.7884, time 4631.56ms, mfu 0.00%\nstep 1813: train loss 1.7968, val loss 1.9151\niter 1813: loss 1.8392, time 4667.38ms, mfu 0.00%\nstep 1814: train loss 1.7917, val loss 1.9250\niter 1814: loss 1.8806, time 4561.66ms, mfu 0.00%\nstep 1815: train loss 1.7940, val loss 1.9193\niter 1815: loss 1.8393, time 5309.74ms, mfu 0.00%\nstep 1816: train loss 1.7953, val loss 1.9200\niter 1816: loss 1.7572, time 4599.06ms, mfu 0.00%\nstep 1817: train loss 1.7913, val loss 1.9129\niter 1817: loss 1.7780, time 4519.63ms, mfu 0.00%\nstep 1818: train loss 1.7915, val loss 1.9183\niter 1818: loss 1.7586, time 4569.93ms, mfu 0.00%\nstep 1819: train loss 1.8017, val loss 1.9176\niter 1819: loss 1.7238, time 4504.67ms, mfu 0.00%\nstep 1820: train loss 1.7967, val loss 1.9221\niter 1820: loss 1.8203, time 4410.43ms, mfu 0.00%\nstep 1821: train loss 1.7994, val loss 1.9142\niter 1821: loss 1.8089, time 4458.99ms, mfu 0.00%\nstep 1822: train loss 1.7955, val loss 1.9138\niter 1822: loss 1.7872, time 5005.78ms, mfu 0.00%\nstep 1823: train loss 1.8026, val loss 1.9208\niter 1823: loss 1.7301, time 4445.47ms, mfu 0.00%\nstep 1824: train loss 1.7994, val loss 1.9170\niter 1824: loss 1.7667, time 4490.97ms, mfu 0.00%\nstep 1825: train loss 1.7979, val loss 1.9163\niter 1825: loss 1.7916, time 4305.71ms, mfu 0.00%\nstep 1826: train loss 1.7953, val loss 1.9203\niter 1826: loss 1.8548, time 4538.90ms, mfu 0.00%\nstep 1827: train loss 1.7940, val loss 1.9076\niter 1827: loss 1.7817, time 4525.37ms, mfu 0.00%\nstep 1828: train loss 1.7984, val loss 1.9192\niter 1828: loss 1.7134, time 4551.14ms, mfu 0.00%\nstep 1829: train loss 1.7983, val loss 1.9159\niter 1829: loss 1.7792, time 5137.95ms, mfu 0.00%\nstep 1830: train loss 1.7966, val loss 1.9091\niter 1830: loss 1.9131, time 4965.92ms, mfu 0.00%\nstep 1831: train loss 1.7981, val loss 1.9127\niter 1831: loss 1.8063, time 4809.98ms, mfu 0.00%\nstep 1832: train loss 1.7921, val loss 1.9101\niter 1832: loss 1.6924, time 4741.61ms, mfu 0.00%\nstep 1833: train loss 1.7986, val loss 1.9179\niter 1833: loss 1.7641, time 4618.34ms, mfu 0.00%\nstep 1834: train loss 1.7972, val loss 1.9163\niter 1834: loss 1.7520, time 4629.89ms, mfu 0.00%\nstep 1835: train loss 1.7923, val loss 1.9139\niter 1835: loss 1.8012, time 4634.44ms, mfu 0.00%\nstep 1836: train loss 1.8076, val loss 1.9123\niter 1836: loss 1.7979, time 5383.30ms, mfu 0.00%\nstep 1837: train loss 1.7960, val loss 1.9256\niter 1837: loss 1.7714, time 4695.92ms, mfu 0.00%\nstep 1838: train loss 1.7995, val loss 1.9164\niter 1838: loss 1.8206, time 4697.17ms, mfu 0.00%\nstep 1839: train loss 1.7969, val loss 1.9200\niter 1839: loss 1.8657, time 4603.01ms, mfu 0.00%\nstep 1840: train loss 1.7921, val loss 1.9155\niter 1840: loss 1.7997, time 4709.59ms, mfu 0.00%\nstep 1841: train loss 1.7935, val loss 1.9110\niter 1841: loss 1.9179, time 4585.36ms, mfu 0.00%\nstep 1842: train loss 1.7951, val loss 1.9099\niter 1842: loss 1.7702, time 4587.11ms, mfu 0.00%\nstep 1843: train loss 1.7880, val loss 1.9162\niter 1843: loss 1.7335, time 5334.83ms, mfu 0.00%\nstep 1844: train loss 1.7925, val loss 1.9205\niter 1844: loss 1.6527, time 4355.44ms, mfu 0.00%\nstep 1845: train loss 1.8070, val loss 1.9145\niter 1845: loss 1.8745, time 4497.07ms, mfu 0.00%\nstep 1846: train loss 1.7905, val loss 1.9115\niter 1846: loss 1.7604, time 4376.33ms, mfu 0.00%\nstep 1847: train loss 1.7945, val loss 1.9200\niter 1847: loss 1.8247, time 4496.65ms, mfu 0.00%\nstep 1848: train loss 1.7972, val loss 1.9197\niter 1848: loss 1.8210, time 4550.92ms, mfu 0.00%\nstep 1849: train loss 1.7975, val loss 1.9123\niter 1849: loss 1.7836, time 4499.41ms, mfu 0.00%\nstep 1850: train loss 1.7895, val loss 1.9129\niter 1850: loss 1.8160, time 5188.29ms, mfu 0.00%\nstep 1851: train loss 1.7910, val loss 1.9175\niter 1851: loss 1.8273, time 4514.68ms, mfu 0.00%\nstep 1852: train loss 1.7922, val loss 1.9055\niter 1852: loss 1.8272, time 4502.71ms, mfu 0.00%\nstep 1853: train loss 1.7952, val loss 1.9141\niter 1853: loss 1.9362, time 4537.99ms, mfu 0.00%\nstep 1854: train loss 1.7832, val loss 1.9142\niter 1854: loss 1.9047, time 4348.40ms, mfu 0.00%\nstep 1855: train loss 1.7835, val loss 1.9137\niter 1855: loss 1.7684, time 4690.74ms, mfu 0.00%\nstep 1856: train loss 1.7941, val loss 1.9128\niter 1856: loss 1.8077, time 4932.27ms, mfu 0.00%\nstep 1857: train loss 1.7850, val loss 1.9039\nsaving checkpoint to out-shakespeare-char\niter 1857: loss 1.9313, time 5599.02ms, mfu 0.00%\nstep 1858: train loss 1.7918, val loss 1.9136\niter 1858: loss 1.7396, time 4737.21ms, mfu 0.00%\nstep 1859: train loss 1.7902, val loss 1.9134\niter 1859: loss 1.8348, time 4594.01ms, mfu 0.00%\nstep 1860: train loss 1.7883, val loss 1.9037\nsaving checkpoint to out-shakespeare-char\niter 1860: loss 1.7550, time 4619.41ms, mfu 0.00%\nstep 1861: train loss 1.7925, val loss 1.9096\niter 1861: loss 1.7228, time 4812.38ms, mfu 0.00%\nstep 1862: train loss 1.7854, val loss 1.9129\niter 1862: loss 1.6676, time 4918.81ms, mfu 0.00%\nstep 1863: train loss 1.7972, val loss 1.9151\niter 1863: loss 1.8230, time 5380.84ms, mfu 0.00%\nstep 1864: train loss 1.7865, val loss 1.9144\niter 1864: loss 1.8106, time 4755.82ms, mfu 0.00%\nstep 1865: train loss 1.7866, val loss 1.9054\niter 1865: loss 1.8082, time 4613.03ms, mfu 0.00%\nstep 1866: train loss 1.7904, val loss 1.9106\niter 1866: loss 1.7339, time 4694.75ms, mfu 0.00%\nstep 1867: train loss 1.7866, val loss 1.9063\niter 1867: loss 1.7581, time 4650.29ms, mfu 0.00%\nstep 1868: train loss 1.7877, val loss 1.9182\niter 1868: loss 1.8347, time 4724.56ms, mfu 0.00%\nstep 1869: train loss 1.7992, val loss 1.9097\niter 1869: loss 1.8132, time 4691.29ms, mfu 0.00%\nstep 1870: train loss 1.7883, val loss 1.9112\niter 1870: loss 1.7878, time 5384.81ms, mfu 0.00%\nstep 1871: train loss 1.7946, val loss 1.9128\niter 1871: loss 1.6889, time 4696.23ms, mfu 0.00%\nstep 1872: train loss 1.7916, val loss 1.9144\niter 1872: loss 1.6565, time 4765.28ms, mfu 0.00%\nstep 1873: train loss 1.7917, val loss 1.9095\niter 1873: loss 1.7922, time 4712.93ms, mfu 0.00%\nstep 1874: train loss 1.7921, val loss 1.9125\niter 1874: loss 1.6912, time 4860.50ms, mfu 0.00%\nstep 1875: train loss 1.7869, val loss 1.9119\niter 1875: loss 1.7545, time 4752.51ms, mfu 0.00%\nstep 1876: train loss 1.7886, val loss 1.9077\niter 1876: loss 1.8827, time 4777.94ms, mfu 0.00%\nstep 1877: train loss 1.7850, val loss 1.9127\niter 1877: loss 1.7760, time 5252.99ms, mfu 0.00%\nstep 1878: train loss 1.7932, val loss 1.9182\niter 1878: loss 1.9184, time 4698.21ms, mfu 0.00%\nstep 1879: train loss 1.7926, val loss 1.9193\niter 1879: loss 1.9419, time 4685.80ms, mfu 0.00%\nstep 1880: train loss 1.7911, val loss 1.9185\niter 1880: loss 1.7859, time 4651.57ms, mfu 0.00%\nstep 1881: train loss 1.7827, val loss 1.9153\niter 1881: loss 1.8362, time 4598.98ms, mfu 0.00%\nstep 1882: train loss 1.7965, val loss 1.9072\niter 1882: loss 1.7354, time 5071.55ms, mfu 0.00%\nstep 1883: train loss 1.7982, val loss 1.9108\niter 1883: loss 1.8201, time 5585.48ms, mfu 0.00%\nstep 1884: train loss 1.7867, val loss 1.9257\niter 1884: loss 1.8319, time 4679.01ms, mfu 0.00%\nstep 1885: train loss 1.7887, val loss 1.9108\niter 1885: loss 1.8170, time 4735.75ms, mfu 0.00%\nstep 1886: train loss 1.7875, val loss 1.9099\niter 1886: loss 1.8414, time 4776.23ms, mfu 0.00%\nstep 1887: train loss 1.7852, val loss 1.9139\niter 1887: loss 1.7650, time 4771.39ms, mfu 0.00%\nstep 1888: train loss 1.7877, val loss 1.9098\niter 1888: loss 1.7639, time 4698.75ms, mfu 0.00%\nstep 1889: train loss 1.7956, val loss 1.9061\niter 1889: loss 1.8001, time 4722.56ms, mfu 0.00%\nstep 1890: train loss 1.7980, val loss 1.9228\niter 1890: loss 1.7487, time 5371.70ms, mfu 0.00%\nstep 1891: train loss 1.7914, val loss 1.9135\niter 1891: loss 1.7394, time 4602.98ms, mfu 0.00%\nstep 1892: train loss 1.7940, val loss 1.9150\niter 1892: loss 1.6462, time 4574.91ms, mfu 0.00%\nstep 1893: train loss 1.7873, val loss 1.9201\niter 1893: loss 1.6895, time 4544.77ms, mfu 0.00%\nstep 1894: train loss 1.7923, val loss 1.9167\niter 1894: loss 1.7987, time 4502.99ms, mfu 0.00%\nstep 1895: train loss 1.7909, val loss 1.9134\niter 1895: loss 1.8003, time 4540.91ms, mfu 0.00%\nstep 1896: train loss 1.7948, val loss 1.9101\niter 1896: loss 1.6916, time 4508.48ms, mfu 0.00%\nstep 1897: train loss 1.7877, val loss 1.9128\niter 1897: loss 1.7562, time 5077.51ms, mfu 0.00%\nstep 1898: train loss 1.7968, val loss 1.9212\niter 1898: loss 1.6997, time 4541.07ms, mfu 0.00%\nstep 1899: train loss 1.7918, val loss 1.9225\niter 1899: loss 1.7803, time 4577.85ms, mfu 0.00%\nstep 1900: train loss 1.7915, val loss 1.9229\niter 1900: loss 1.8432, time 4513.43ms, mfu 0.00%\nstep 1901: train loss 1.7787, val loss 1.9144\niter 1901: loss 1.7627, time 4534.54ms, mfu 0.00%\nstep 1902: train loss 1.7855, val loss 1.9064\niter 1902: loss 1.7923, time 4471.44ms, mfu 0.00%\nstep 1903: train loss 1.7943, val loss 1.9115\niter 1903: loss 1.7957, time 4576.15ms, mfu 0.00%\nstep 1904: train loss 1.7884, val loss 1.9148\niter 1904: loss 1.7782, time 5469.18ms, mfu 0.00%\nstep 1905: train loss 1.7806, val loss 1.9129\niter 1905: loss 1.8340, time 4719.78ms, mfu 0.00%\nstep 1906: train loss 1.7820, val loss 1.9046\niter 1906: loss 1.7941, time 4767.93ms, mfu 0.00%\nstep 1907: train loss 1.7777, val loss 1.9010\nsaving checkpoint to out-shakespeare-char\niter 1907: loss 1.8179, time 4539.32ms, mfu 0.00%\nstep 1908: train loss 1.7911, val loss 1.9033\niter 1908: loss 1.8110, time 4485.19ms, mfu 0.00%\nstep 1909: train loss 1.7810, val loss 1.9108\niter 1909: loss 1.7928, time 4500.59ms, mfu 0.00%\nstep 1910: train loss 1.7854, val loss 1.8975\nsaving checkpoint to out-shakespeare-char\niter 1910: loss 1.7635, time 4563.84ms, mfu 0.00%\nstep 1911: train loss 1.7891, val loss 1.9130\niter 1911: loss 1.7579, time 5138.13ms, mfu 0.00%\nstep 1912: train loss 1.7827, val loss 1.9102\niter 1912: loss 1.7843, time 4644.95ms, mfu 0.00%\nstep 1913: train loss 1.7925, val loss 1.9090\niter 1913: loss 1.8254, time 4503.12ms, mfu 0.00%\nstep 1914: train loss 1.7855, val loss 1.9044\niter 1914: loss 1.7188, time 4565.73ms, mfu 0.00%\nstep 1915: train loss 1.7938, val loss 1.9079\niter 1915: loss 1.6972, time 4446.62ms, mfu 0.00%\nstep 1916: train loss 1.7862, val loss 1.9098\niter 1916: loss 1.8112, time 4467.61ms, mfu 0.00%\nstep 1917: train loss 1.7727, val loss 1.9020\niter 1917: loss 1.7135, time 4653.17ms, mfu 0.00%\nstep 1918: train loss 1.7864, val loss 1.9011\niter 1918: loss 1.7315, time 5250.22ms, mfu 0.00%\nstep 1919: train loss 1.7790, val loss 1.9084\niter 1919: loss 1.7147, time 4542.99ms, mfu 0.00%\nstep 1920: train loss 1.7832, val loss 1.9110\niter 1920: loss 1.7523, time 4583.73ms, mfu 0.00%\nstep 1921: train loss 1.7853, val loss 1.9056\niter 1921: loss 1.7555, time 4565.70ms, mfu 0.00%\nstep 1922: train loss 1.7914, val loss 1.9147\niter 1922: loss 1.6119, time 4552.85ms, mfu 0.00%\nstep 1923: train loss 1.7889, val loss 1.8994\niter 1923: loss 1.8623, time 4700.15ms, mfu 0.00%\nstep 1924: train loss 1.7834, val loss 1.9093\niter 1924: loss 1.7233, time 4622.01ms, mfu 0.00%\nstep 1925: train loss 1.7860, val loss 1.9104\niter 1925: loss 1.7549, time 5311.37ms, mfu 0.00%\nstep 1926: train loss 1.7838, val loss 1.9097\niter 1926: loss 1.7317, time 4830.81ms, mfu 0.00%\nstep 1927: train loss 1.7820, val loss 1.9115\niter 1927: loss 1.7619, time 5019.16ms, mfu 0.00%\nstep 1928: train loss 1.7910, val loss 1.8970\nsaving checkpoint to out-shakespeare-char\niter 1928: loss 1.7165, time 4580.54ms, mfu 0.00%\nstep 1929: train loss 1.7869, val loss 1.9170\niter 1929: loss 1.8729, time 4609.45ms, mfu 0.00%\nstep 1930: train loss 1.7875, val loss 1.9091\niter 1930: loss 1.7120, time 4428.87ms, mfu 0.00%\nstep 1931: train loss 1.7806, val loss 1.9067\niter 1931: loss 1.8063, time 4788.60ms, mfu 0.00%\nstep 1932: train loss 1.7866, val loss 1.9045\niter 1932: loss 1.7638, time 4775.74ms, mfu 0.00%\nstep 1933: train loss 1.7811, val loss 1.9057\niter 1933: loss 1.7095, time 4704.57ms, mfu 0.00%\nstep 1934: train loss 1.7821, val loss 1.9041\niter 1934: loss 1.7212, time 4307.59ms, mfu 0.00%\nstep 1935: train loss 1.7835, val loss 1.9050\niter 1935: loss 1.7552, time 4330.80ms, mfu 0.00%\nstep 1936: train loss 1.7901, val loss 1.9103\niter 1936: loss 1.9215, time 4473.60ms, mfu 0.00%\nstep 1937: train loss 1.7807, val loss 1.9122\niter 1937: loss 1.6971, time 4277.62ms, mfu 0.00%\nstep 1938: train loss 1.7741, val loss 1.9041\niter 1938: loss 1.7714, time 4525.79ms, mfu 0.00%\nstep 1939: train loss 1.7906, val loss 1.9085\niter 1939: loss 1.8687, time 5086.30ms, mfu 0.00%\nstep 1940: train loss 1.7856, val loss 1.9096\niter 1940: loss 1.8169, time 4930.35ms, mfu 0.00%\nstep 1941: train loss 1.7875, val loss 1.9115\niter 1941: loss 1.7131, time 4471.32ms, mfu 0.00%\nstep 1942: train loss 1.7832, val loss 1.9017\niter 1942: loss 1.7710, time 4642.82ms, mfu 0.00%\nstep 1943: train loss 1.7837, val loss 1.8997\niter 1943: loss 1.7462, time 4606.85ms, mfu 0.00%\nstep 1944: train loss 1.7867, val loss 1.9089\niter 1944: loss 1.6723, time 4850.12ms, mfu 0.00%\nstep 1945: train loss 1.7825, val loss 1.9001\niter 1945: loss 1.8526, time 5351.66ms, mfu 0.00%\nstep 1946: train loss 1.7794, val loss 1.9108\niter 1946: loss 1.8295, time 4581.98ms, mfu 0.00%\nstep 1947: train loss 1.7818, val loss 1.9052\niter 1947: loss 1.6755, time 4507.67ms, mfu 0.00%\nstep 1948: train loss 1.7799, val loss 1.8970\nsaving checkpoint to out-shakespeare-char\niter 1948: loss 1.7548, time 4501.07ms, mfu 0.00%\nstep 1949: train loss 1.7789, val loss 1.9062\niter 1949: loss 1.8270, time 4524.15ms, mfu 0.00%\nstep 1950: train loss 1.7803, val loss 1.9034\niter 1950: loss 1.8144, time 4576.26ms, mfu 0.00%\nstep 1951: train loss 1.7770, val loss 1.9016\niter 1951: loss 1.8272, time 4566.10ms, mfu 0.00%\nstep 1952: train loss 1.7791, val loss 1.9038\niter 1952: loss 1.7614, time 5210.12ms, mfu 0.00%\nstep 1953: train loss 1.7817, val loss 1.8987\niter 1953: loss 1.6741, time 4639.05ms, mfu 0.00%\nstep 1954: train loss 1.7875, val loss 1.9117\niter 1954: loss 1.6065, time 4504.26ms, mfu 0.00%\nstep 1955: train loss 1.7814, val loss 1.9079\niter 1955: loss 1.8177, time 4707.70ms, mfu 0.00%\nstep 1956: train loss 1.7766, val loss 1.9184\niter 1956: loss 1.7756, time 4490.48ms, mfu 0.00%\nstep 1957: train loss 1.7853, val loss 1.9136\niter 1957: loss 1.7855, time 4618.43ms, mfu 0.00%\nstep 1958: train loss 1.7859, val loss 1.9099\niter 1958: loss 1.7398, time 4696.53ms, mfu 0.00%\nstep 1959: train loss 1.7874, val loss 1.9111\niter 1959: loss 1.7911, time 5458.21ms, mfu 0.00%\nstep 1960: train loss 1.7729, val loss 1.9199\niter 1960: loss 1.7062, time 4572.33ms, mfu 0.00%\nstep 1961: train loss 1.7789, val loss 1.9096\niter 1961: loss 1.7851, time 4680.44ms, mfu 0.00%\nstep 1962: train loss 1.7854, val loss 1.9219\niter 1962: loss 1.7563, time 4849.01ms, mfu 0.00%\nstep 1963: train loss 1.7879, val loss 1.9059\niter 1963: loss 1.6755, time 4916.57ms, mfu 0.00%\nstep 1964: train loss 1.7839, val loss 1.9030\niter 1964: loss 1.7909, time 4657.46ms, mfu 0.00%\nstep 1965: train loss 1.7792, val loss 1.9134\niter 1965: loss 1.8912, time 4693.00ms, mfu 0.00%\nstep 1966: train loss 1.7832, val loss 1.9118\niter 1966: loss 1.7235, time 5339.28ms, mfu 0.00%\nstep 1967: train loss 1.7765, val loss 1.9028\niter 1967: loss 1.7835, time 4604.79ms, mfu 0.00%\nstep 1968: train loss 1.7714, val loss 1.9116\niter 1968: loss 1.8458, time 4519.32ms, mfu 0.00%\nstep 1969: train loss 1.7842, val loss 1.9070\niter 1969: loss 1.7543, time 4492.52ms, mfu 0.00%\nstep 1970: train loss 1.7799, val loss 1.9095\niter 1970: loss 1.7182, time 4515.48ms, mfu 0.00%\nstep 1971: train loss 1.7782, val loss 1.9115\niter 1971: loss 1.7313, time 4378.83ms, mfu 0.00%\nstep 1972: train loss 1.7864, val loss 1.9098\niter 1972: loss 1.7741, time 4327.81ms, mfu 0.00%\nstep 1973: train loss 1.7827, val loss 1.9027\niter 1973: loss 1.8090, time 4972.66ms, mfu 0.00%\nstep 1974: train loss 1.7790, val loss 1.9106\niter 1974: loss 1.7067, time 4430.11ms, mfu 0.00%\nstep 1975: train loss 1.7800, val loss 1.9083\niter 1975: loss 1.8772, time 4297.69ms, mfu 0.00%\nstep 1976: train loss 1.7720, val loss 1.9035\niter 1976: loss 1.7146, time 4315.14ms, mfu 0.00%\nstep 1977: train loss 1.7726, val loss 1.9086\niter 1977: loss 1.7656, time 4545.39ms, mfu 0.00%\nstep 1978: train loss 1.7757, val loss 1.9047\niter 1978: loss 1.7416, time 4384.90ms, mfu 0.00%\nstep 1979: train loss 1.7836, val loss 1.9072\niter 1979: loss 1.8011, time 4716.04ms, mfu 0.00%\nstep 1980: train loss 1.7711, val loss 1.9049\niter 1980: loss 1.7919, time 5330.80ms, mfu 0.00%\nstep 1981: train loss 1.7868, val loss 1.8953\nsaving checkpoint to out-shakespeare-char\niter 1981: loss 1.8516, time 4521.97ms, mfu 0.00%\nstep 1982: train loss 1.7731, val loss 1.9005\niter 1982: loss 1.7952, time 4421.12ms, mfu 0.00%\nstep 1983: train loss 1.7782, val loss 1.9062\niter 1983: loss 1.7364, time 4425.14ms, mfu 0.00%\nstep 1984: train loss 1.7758, val loss 1.9005\niter 1984: loss 1.8092, time 4423.59ms, mfu 0.00%\nstep 1985: train loss 1.7720, val loss 1.9067\niter 1985: loss 1.7449, time 4475.81ms, mfu 0.00%\nstep 1986: train loss 1.7818, val loss 1.9069\niter 1986: loss 1.7122, time 4423.63ms, mfu 0.00%\nstep 1987: train loss 1.7786, val loss 1.9037\niter 1987: loss 1.7748, time 5182.37ms, mfu 0.00%\nstep 1988: train loss 1.7773, val loss 1.8865\nsaving checkpoint to out-shakespeare-char\niter 1988: loss 1.7446, time 4448.43ms, mfu 0.00%\nstep 1989: train loss 1.7803, val loss 1.8974\niter 1989: loss 1.8863, time 4371.58ms, mfu 0.00%\nstep 1990: train loss 1.7780, val loss 1.9049\niter 1990: loss 1.7662, time 4387.82ms, mfu 0.00%\nstep 1991: train loss 1.7753, val loss 1.9083\niter 1991: loss 1.7430, time 4495.66ms, mfu 0.00%\nstep 1992: train loss 1.7762, val loss 1.9036\niter 1992: loss 1.8374, time 4445.62ms, mfu 0.00%\nstep 1993: train loss 1.7853, val loss 1.9030\niter 1993: loss 1.7092, time 4425.68ms, mfu 0.00%\nstep 1994: train loss 1.7776, val loss 1.9094\niter 1994: loss 1.7869, time 5031.35ms, mfu 0.00%\nstep 1995: train loss 1.7852, val loss 1.9035\niter 1995: loss 1.7709, time 4553.30ms, mfu 0.00%\nstep 1996: train loss 1.7796, val loss 1.9066\niter 1996: loss 1.7788, time 4491.24ms, mfu 0.00%\nstep 1997: train loss 1.7890, val loss 1.9004\niter 1997: loss 1.8201, time 4310.34ms, mfu 0.00%\nstep 1998: train loss 1.7813, val loss 1.9060\niter 1998: loss 1.8200, time 4538.02ms, mfu 0.00%\nstep 1999: train loss 1.7841, val loss 1.9083\niter 1999: loss 1.8063, time 4687.38ms, mfu 0.00%\nstep 2000: train loss 1.7796, val loss 1.9091\niter 2000: loss 1.6105, time 4584.88ms, mfu 0.00%\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"! pip install tiktoken","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-14T22:52:04.110243Z","iopub.execute_input":"2025-02-14T22:52:04.110647Z","iopub.status.idle":"2025-02-14T22:52:09.395512Z","shell.execute_reply.started":"2025-02-14T22:52:04.110612Z","shell.execute_reply":"2025-02-14T22:52:09.39405Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%cd /kaggle/working/nanoGPT","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-14T23:05:02.774858Z","iopub.execute_input":"2025-02-14T23:05:02.775327Z","iopub.status.idle":"2025-02-14T23:05:02.783832Z","shell.execute_reply.started":"2025-02-14T23:05:02.775288Z","shell.execute_reply":"2025-02-14T23:05:02.782319Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"! python sample.py --out_dir=out-shakespeare-char","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T01:53:14.866702Z","iopub.execute_input":"2025-02-15T01:53:14.867242Z","iopub.status.idle":"2025-02-15T01:53:34.911245Z","shell.execute_reply.started":"2025-02-15T01:53:14.867214Z","shell.execute_reply":"2025-02-15T01:53:34.910206Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"! python sample.py --out_dir=out-shakespeare-char --device=cpu","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T01:44:09.855904Z","iopub.execute_input":"2025-02-15T01:44:09.856227Z","iopub.status.idle":"2025-02-15T01:44:26.200599Z","shell.execute_reply.started":"2025-02-15T01:44:09.856203Z","shell.execute_reply":"2025-02-15T01:44:26.199501Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%cd /kaggle/input/spotify-million-song-dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T02:01:58.355339Z","iopub.execute_input":"2025-02-15T02:01:58.35566Z","iopub.status.idle":"2025-02-15T02:01:58.361926Z","shell.execute_reply.started":"2025-02-15T02:01:58.355636Z","shell.execute_reply":"2025-02-15T02:01:58.361089Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport tiktoken\nimport numpy as np\nimport pandas as pd\n\ndf = pd.read_csv('/kaggle/input/spotify-million-song-dataset/spotify_millsongdata.csv')\ndata = df['text'].str.cat(sep='\\n')\n\nn = len(data)\ntrain_data = data[:int(n*0.9)]\nval_data = data[int(n*0.9):]\n\n# encode with tiktoken gpt2 bpe\nenc = tiktoken.get_encoding(\"gpt2\")\ntrain_ids = enc.encode_ordinary(train_data)\nval_ids = enc.encode_ordinary(val_data)\nprint(f\"train has {len(train_ids):,} tokens\")\nprint(f\"val has {len(val_ids):,} tokens\")\n\n# export to bin files\ntrain_ids = np.array(train_ids, dtype=np.uint16)\nval_ids = np.array(val_ids, dtype=np.uint16)\ntrain_ids.tofile('/kaggle/working/nanoGPT/data/train.bin')\nval_ids.tofile('/kaggle/working/nanoGPT/data/val.bin')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T02:29:38.448637Z","iopub.execute_input":"2025-02-15T02:29:38.449045Z","iopub.status.idle":"2025-02-15T02:29:50.698938Z","shell.execute_reply.started":"2025-02-15T02:29:38.449018Z","shell.execute_reply":"2025-02-15T02:29:50.69822Z"}},"outputs":[{"name":"stdout","text":"train has 22,308,928 tokens\nval has 2,456,916 tokens\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"%cd /kaggle/working/nanoGPT\n%cd /kaggle/input/lyrics\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T03:52:39.965765Z","iopub.execute_input":"2025-02-15T03:52:39.966142Z","iopub.status.idle":"2025-02-15T03:52:39.972712Z","shell.execute_reply.started":"2025-02-15T03:52:39.966118Z","shell.execute_reply":"2025-02-15T03:52:39.971989Z"}},"outputs":[{"name":"stdout","text":"/kaggle/working/nanoGPT\n/kaggle/input/lyrics\n","output_type":"stream"}],"execution_count":55},{"cell_type":"code","source":"import shutil\nimport os\n\n# Define the source path and destination path\nsource_path = '/kaggle/input/lyrics/train_lyrics.py'\ndestination_path = '/kaggle/working/nanoGPT/config/train.py'\n\n# Copy the file from the source to the destination\nshutil.copy(source_path, destination_path)\nprint(f'File copied to {destination_path}')\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T03:52:44.292596Z","iopub.execute_input":"2025-02-15T03:52:44.292945Z","iopub.status.idle":"2025-02-15T03:52:44.300117Z","shell.execute_reply.started":"2025-02-15T03:52:44.292916Z","shell.execute_reply":"2025-02-15T03:52:44.299158Z"}},"outputs":[{"name":"stdout","text":"File copied to /kaggle/working/nanoGPT/config/train.py\n","output_type":"stream"}],"execution_count":56},{"cell_type":"code","source":"%cd /kaggle/working/nanoGPT","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T03:57:01.977808Z","iopub.execute_input":"2025-02-15T03:57:01.978124Z","iopub.status.idle":"2025-02-15T03:57:01.983932Z","shell.execute_reply.started":"2025-02-15T03:57:01.978098Z","shell.execute_reply":"2025-02-15T03:57:01.983246Z"}},"outputs":[{"name":"stdout","text":"[Errno 2] No such file or directory: '/kaggle/working/nanoGPT'\n/kaggle/working\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"! python train.py config/train_lyrics.py  device=cpu  compile=False  eval_iters=20  log_interval=1  block_size=64  batch_size=12  n_layer=4  n_head=4  n_embd=128  max_iters=2000  lr_decay_iters=2000  dropout=0.0","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T03:54:22.31935Z","iopub.execute_input":"2025-02-15T03:54:22.31973Z","iopub.status.idle":"2025-02-15T03:54:24.640091Z","shell.execute_reply.started":"2025-02-15T03:54:22.319664Z","shell.execute_reply":"2025-02-15T03:54:24.639214Z"}},"outputs":[{"name":"stdout","text":"Overriding config with config/train_lyrics.py:\n# train a miniature character-level shakespeare model\n# good for debugging and playing on macbooks and such\n\nout_dir = 'out-lyrics'\neval_interval = 250 # keep frequent because we'll overfit\neval_iters = 200\nlog_interval = 10 # don't print too too often\n\n# we expect to overfit on this small dataset, so only save when val improves\nalways_save_checkpoint = False\n\nwandb_log = False # override via command line if you like\nwandb_project = 'lyrics'\nwandb_run_name = 'mini-gpt'\n\ndataset = 'lyrics'\nbatch_size = 64\nblock_size = 256 # context of up to 256 previous characters\n\n# baby GPT model :)\nn_layer = 6\nn_head = 6\nn_embd = 384\ndropout = 0.2\n\nlearning_rate = 1e-3 # with baby networks can afford to go a bit higher\nmax_iters = 5000\nlr_decay_iters = 5000 # make equal to max_iters usually\nmin_lr = 1e-4 # learning_rate / 10 usually\nbeta2 = 0.99 # make a bit bigger because number of tokens per iter is small\n\nwarmup_iters = 100 # not super necessary potentially\n\n# on macbook also add\n# device = 'cpu'  # run on cpu only\n# compile = False # do not torch compile the model\nOverriding config with :\nTraceback (most recent call last):\n  File \"/kaggle/working/nanoGPT/train.py\", line 77, in <module>\n    exec(open('configurator.py').read()) # overrides from command line or config file\n  File \"<string>\", line 26, in <module>\nFileNotFoundError: [Errno 2] No such file or directory: ''\n","output_type":"stream"}],"execution_count":58}]}