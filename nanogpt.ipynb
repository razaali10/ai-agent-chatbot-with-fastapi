{"cells":[{"source":"<a href=\"https://www.kaggle.com/code/razaali10/nanogpt-fine-tune-shakespare?scriptVersionId=223482617\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","execution_count":1,"id":"7d0da892","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2025-02-20T03:46:16.466803Z","iopub.status.busy":"2025-02-20T03:46:16.466504Z","iopub.status.idle":"2025-02-20T03:46:17.210444Z","shell.execute_reply":"2025-02-20T03:46:17.209541Z"},"papermill":{"duration":0.74921,"end_time":"2025-02-20T03:46:17.211841","exception":false,"start_time":"2025-02-20T03:46:16.462631","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["/kaggle/input/lyrics/train_lyrics.py\n","/kaggle/input/spotify-million-song-dataset/spotify_millsongdata.csv\n"]}],"source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load\n","\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","# Input data files are available in the read-only \"../input/\" directory\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","import os\n","for dirname, _, filenames in os.walk('/kaggle/input'):\n","    for filename in filenames:\n","        print(os.path.join(dirname, filename))\n","\n","# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n","# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"]},{"cell_type":"markdown","id":"3aab80f3","metadata":{"papermill":{"duration":0.002193,"end_time":"2025-02-20T03:46:17.2168","exception":false,"start_time":"2025-02-20T03:46:17.214607","status":"completed"},"tags":[]},"source":["# NanoGPT codes written by Andrej Karpathy bsed on OpenAI GPT 2"]},{"cell_type":"markdown","id":"c13192c1","metadata":{"papermill":{"duration":0.002123,"end_time":"2025-02-20T03:46:17.221208","exception":false,"start_time":"2025-02-20T03:46:17.219085","status":"completed"},"tags":[]},"source":["# 1. Files imported from Github  "]},{"cell_type":"code","execution_count":2,"id":"a0d9f48d","metadata":{"execution":{"iopub.execute_input":"2025-02-20T03:46:17.226644Z","iopub.status.busy":"2025-02-20T03:46:17.226331Z","iopub.status.idle":"2025-02-20T03:46:18.104966Z","shell.execute_reply":"2025-02-20T03:46:18.104137Z"},"papermill":{"duration":0.882713,"end_time":"2025-02-20T03:46:18.106266","exception":false,"start_time":"2025-02-20T03:46:17.223553","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Cloning into 'nanoGPT'...\r\n","remote: Enumerating objects: 686, done.\u001b[K\r\n","remote: Total 686 (delta 0), reused 0 (delta 0), pack-reused 686 (from 1)\u001b[K\r\n","Receiving objects: 100% (686/686), 954.03 KiB | 22.19 MiB/s, done.\r\n","Resolving deltas: 100% (387/387), done.\r\n"]}],"source":["!git clone https://github.com/karpathy/nanoGPT.git"]},{"cell_type":"code","execution_count":3,"id":"f8c6c990","metadata":{"execution":{"iopub.execute_input":"2025-02-20T03:46:18.112789Z","iopub.status.busy":"2025-02-20T03:46:18.112558Z","iopub.status.idle":"2025-02-20T03:46:18.118486Z","shell.execute_reply":"2025-02-20T03:46:18.117761Z"},"papermill":{"duration":0.010534,"end_time":"2025-02-20T03:46:18.119705","exception":false,"start_time":"2025-02-20T03:46:18.109171","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["/kaggle/working/nanoGPT/data/shakespeare_char\n"]}],"source":["%cd /kaggle/working/nanoGPT/data/shakespeare_char"]},{"cell_type":"markdown","id":"a1455cd6","metadata":{"papermill":{"duration":0.002511,"end_time":"2025-02-20T03:46:18.124881","exception":false,"start_time":"2025-02-20T03:46:18.12237","status":"completed"},"tags":[]},"source":["# 2.Tokenizing training and validation Shakespeare writing"]},{"cell_type":"code","execution_count":4,"id":"d328a49e","metadata":{"execution":{"iopub.execute_input":"2025-02-20T03:46:18.130723Z","iopub.status.busy":"2025-02-20T03:46:18.130514Z","iopub.status.idle":"2025-02-20T03:46:18.990541Z","shell.execute_reply":"2025-02-20T03:46:18.989426Z"},"papermill":{"duration":0.864697,"end_time":"2025-02-20T03:46:18.992122","exception":false,"start_time":"2025-02-20T03:46:18.127425","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["length of dataset in characters: 1,115,394\r\n","all the unique characters: \r\n"," !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\r\n","vocab size: 65\r\n","train has 1,003,854 tokens\r\n","val has 111,540 tokens\r\n"]}],"source":["!python prepare.py"]},{"cell_type":"code","execution_count":5,"id":"3293cbe0","metadata":{"execution":{"iopub.execute_input":"2025-02-20T03:46:18.9989Z","iopub.status.busy":"2025-02-20T03:46:18.998655Z","iopub.status.idle":"2025-02-20T03:46:19.003901Z","shell.execute_reply":"2025-02-20T03:46:19.003183Z"},"papermill":{"duration":0.009963,"end_time":"2025-02-20T03:46:19.005106","exception":false,"start_time":"2025-02-20T03:46:18.995143","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["/kaggle/working/nanoGPT\n"]}],"source":["%cd /kaggle/working/nanoGPT"]},{"cell_type":"markdown","id":"71356b62","metadata":{"papermill":{"duration":0.002631,"end_time":"2025-02-20T03:46:19.010748","exception":false,"start_time":"2025-02-20T03:46:19.008117","status":"completed"},"tags":[]},"source":["# 3. Pretraining the Large Language Model (GPT 2) running for 2000 Epoch"]},{"cell_type":"code","execution_count":6,"id":"6e0446a3","metadata":{"execution":{"iopub.execute_input":"2025-02-20T03:46:19.017002Z","iopub.status.busy":"2025-02-20T03:46:19.016785Z","iopub.status.idle":"2025-02-20T06:19:47.502931Z","shell.execute_reply":"2025-02-20T06:19:47.501799Z"},"papermill":{"duration":9208.490992,"end_time":"2025-02-20T06:19:47.504553","exception":false,"start_time":"2025-02-20T03:46:19.013561","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Overriding config with config/train_shakespeare_char.py:\r\n","# train a miniature character-level shakespeare model\r\n","# good for debugging and playing on macbooks and such\r\n","\r\n","out_dir = 'out-shakespeare-char'\r\n","eval_interval = 250 # keep frequent because we'll overfit\r\n","eval_iters = 200\r\n","log_interval = 10 # don't print too too often\r\n","\r\n","# we expect to overfit on this small dataset, so only save when val improves\r\n","always_save_checkpoint = False\r\n","\r\n","wandb_log = False # override via command line if you like\r\n","wandb_project = 'shakespeare-char'\r\n","wandb_run_name = 'mini-gpt'\r\n","\r\n","dataset = 'shakespeare_char'\r\n","gradient_accumulation_steps = 1\r\n","batch_size = 64\r\n","block_size = 256 # context of up to 256 previous characters\r\n","\r\n","# baby GPT model :)\r\n","n_layer = 6\r\n","n_head = 6\r\n","n_embd = 384\r\n","dropout = 0.2\r\n","\r\n","learning_rate = 1e-3 # with baby networks can afford to go a bit higher\r\n","max_iters = 5000\r\n","lr_decay_iters = 5000 # make equal to max_iters usually\r\n","min_lr = 1e-4 # learning_rate / 10 usually\r\n","beta2 = 0.99 # make a bit bigger because number of tokens per iter is small\r\n","\r\n","warmup_iters = 100 # not super necessary potentially\r\n","\r\n","# on macbook also add\r\n","# device = 'cpu'  # run on cpu only\r\n","# compile = False # do not torch compile the model\r\n","\r\n","Overriding: device = cpu\r\n","Overriding: compile = False\r\n","Overriding: eval_interval = 1\r\n","Overriding: log_interval = 1\r\n","Overriding: block_size = 64\r\n","Overriding: batch_size = 12\r\n","Overriding: n_layer = 4\r\n","Overriding: n_head = 4\r\n","Overriding: n_embd = 128\r\n","Overriding: max_iters = 2000\r\n","Overriding: lr_decay_iters = 2000\r\n","Overriding: dropout = 0.0\r\n","tokens per iteration will be: 768\r\n","found vocab_size = 65 (inside data/shakespeare_char/meta.pkl)\r\n","Initializing a new model from scratch\r\n","number of parameters: 0.80M\r\n","/kaggle/working/nanoGPT/train.py:196: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\r\n","  scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\r\n","num decayed parameter tensors: 18, with 802,944 parameters\r\n","num non-decayed parameter tensors: 9, with 1,152 parameters\r\n","using fused AdamW: False\r\n","step 0: train loss 4.1697, val loss 4.1662\r\n","iter 0: loss 4.1828, time 6271.91ms, mfu -100.00%\r\n","step 1: train loss 4.1558, val loss 4.1510\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 1: loss 4.1455, time 4669.54ms, mfu -100.00%\r\n","step 2: train loss 4.1257, val loss 4.1237\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 2: loss 4.1335, time 4647.05ms, mfu -100.00%\r\n","step 3: train loss 4.0837, val loss 4.0799\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 3: loss 4.0706, time 4599.35ms, mfu -100.00%\r\n","step 4: train loss 4.0278, val loss 4.0261\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 4: loss 4.0370, time 4771.58ms, mfu -100.00%\r\n","step 5: train loss 3.9669, val loss 3.9661\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 5: loss 3.9445, time 5361.22ms, mfu 0.00%\r\n","step 6: train loss 3.9051, val loss 3.9104\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 6: loss 3.9046, time 4685.10ms, mfu 0.00%\r\n","step 7: train loss 3.8496, val loss 3.8556\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 7: loss 3.8214, time 4589.36ms, mfu 0.00%\r\n","step 8: train loss 3.7998, val loss 3.8098\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 8: loss 3.7935, time 4601.90ms, mfu 0.00%\r\n","step 9: train loss 3.7670, val loss 3.7746\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 9: loss 3.7732, time 4716.68ms, mfu 0.00%\r\n","step 10: train loss 3.7356, val loss 3.7453\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 10: loss 3.7551, time 4644.62ms, mfu 0.00%\r\n","step 11: train loss 3.7129, val loss 3.7232\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 11: loss 3.7169, time 4773.58ms, mfu 0.00%\r\n","step 12: train loss 3.6905, val loss 3.7006\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 12: loss 3.6445, time 5502.19ms, mfu 0.00%\r\n","step 13: train loss 3.6724, val loss 3.6862\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 13: loss 3.6745, time 4712.20ms, mfu 0.00%\r\n","step 14: train loss 3.6578, val loss 3.6692\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 14: loss 3.6128, time 4701.75ms, mfu 0.00%\r\n","step 15: train loss 3.6440, val loss 3.6538\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 15: loss 3.6557, time 4730.53ms, mfu 0.00%\r\n","step 16: train loss 3.6276, val loss 3.6402\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 16: loss 3.6441, time 4721.73ms, mfu 0.00%\r\n","step 17: train loss 3.6085, val loss 3.6233\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 17: loss 3.6133, time 4745.02ms, mfu 0.00%\r\n","step 18: train loss 3.5964, val loss 3.6061\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 18: loss 3.5905, time 4946.32ms, mfu 0.00%\r\n","step 19: train loss 3.5764, val loss 3.5916\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 19: loss 3.5901, time 5089.19ms, mfu 0.00%\r\n","step 20: train loss 3.5492, val loss 3.5671\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 20: loss 3.5567, time 4509.18ms, mfu 0.00%\r\n","step 21: train loss 3.5263, val loss 3.5405\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 21: loss 3.5237, time 4476.72ms, mfu 0.00%\r\n","step 22: train loss 3.5000, val loss 3.5106\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 22: loss 3.4619, time 4559.68ms, mfu 0.00%\r\n","step 23: train loss 3.4590, val loss 3.4681\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 23: loss 3.4039, time 4467.62ms, mfu 0.00%\r\n","step 24: train loss 3.4250, val loss 3.4430\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 24: loss 3.4260, time 4504.43ms, mfu 0.00%\r\n","step 25: train loss 3.4137, val loss 3.4198\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 25: loss 3.3636, time 5062.40ms, mfu 0.00%\r\n","step 26: train loss 3.3890, val loss 3.3955\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 26: loss 3.3847, time 4693.32ms, mfu 0.00%\r\n","step 27: train loss 3.3506, val loss 3.3646\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 27: loss 3.2691, time 4588.32ms, mfu 0.00%\r\n","step 28: train loss 3.3231, val loss 3.3363\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 28: loss 3.3378, time 4531.31ms, mfu 0.00%\r\n","step 29: train loss 3.2990, val loss 3.3175\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 29: loss 3.2871, time 4467.20ms, mfu 0.00%\r\n","step 30: train loss 3.2785, val loss 3.2916\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 30: loss 3.2869, time 4600.85ms, mfu 0.00%\r\n","step 31: train loss 3.2559, val loss 3.2646\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 31: loss 3.2759, time 4504.12ms, mfu 0.00%\r\n","step 32: train loss 3.2262, val loss 3.2382\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 32: loss 3.2085, time 5047.53ms, mfu 0.00%\r\n","step 33: train loss 3.2099, val loss 3.2218\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 33: loss 3.2599, time 4727.86ms, mfu 0.00%\r\n","step 34: train loss 3.1854, val loss 3.1967\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 34: loss 3.2117, time 4745.77ms, mfu 0.00%\r\n","step 35: train loss 3.1619, val loss 3.1751\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 35: loss 3.2230, time 4510.33ms, mfu 0.00%\r\n","step 36: train loss 3.1503, val loss 3.1638\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 36: loss 3.1499, time 4617.07ms, mfu 0.00%\r\n","step 37: train loss 3.1296, val loss 3.1370\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 37: loss 3.1359, time 4557.52ms, mfu 0.00%\r\n","step 38: train loss 3.1055, val loss 3.1127\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 38: loss 3.0841, time 4655.52ms, mfu 0.00%\r\n","step 39: train loss 3.0839, val loss 3.0882\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 39: loss 3.0125, time 5142.69ms, mfu 0.00%\r\n","step 40: train loss 3.0662, val loss 3.0693\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 40: loss 3.0222, time 4626.19ms, mfu 0.00%\r\n","step 41: train loss 3.0518, val loss 3.0530\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 41: loss 3.0251, time 4591.42ms, mfu 0.00%\r\n","step 42: train loss 3.0285, val loss 3.0328\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 42: loss 2.9626, time 4705.62ms, mfu 0.00%\r\n","step 43: train loss 3.0091, val loss 3.0202\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 43: loss 3.0333, time 4848.95ms, mfu 0.00%\r\n","step 44: train loss 2.9967, val loss 3.0030\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 44: loss 2.9897, time 4515.11ms, mfu 0.00%\r\n","step 45: train loss 2.9733, val loss 2.9895\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 45: loss 2.9791, time 4583.50ms, mfu 0.00%\r\n","step 46: train loss 2.9582, val loss 2.9664\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 46: loss 2.9959, time 5267.33ms, mfu 0.00%\r\n","step 47: train loss 2.9427, val loss 2.9527\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 47: loss 2.9067, time 4544.39ms, mfu 0.00%\r\n","step 48: train loss 2.9320, val loss 2.9405\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 48: loss 2.9619, time 4399.34ms, mfu 0.00%\r\n","step 49: train loss 2.9195, val loss 2.9286\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 49: loss 2.9168, time 4591.05ms, mfu 0.00%\r\n","step 50: train loss 2.9035, val loss 2.9198\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 50: loss 2.8462, time 4548.21ms, mfu 0.00%\r\n","step 51: train loss 2.8996, val loss 2.9149\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 51: loss 2.8592, time 4569.86ms, mfu 0.00%\r\n","step 52: train loss 2.8798, val loss 2.8995\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 52: loss 2.8908, time 4521.10ms, mfu 0.00%\r\n","step 53: train loss 2.8734, val loss 2.8928\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 53: loss 2.7950, time 5155.38ms, mfu 0.00%\r\n","step 54: train loss 2.8690, val loss 2.8888\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 54: loss 2.8302, time 4450.91ms, mfu 0.00%\r\n","step 55: train loss 2.8558, val loss 2.8725\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 55: loss 2.8843, time 4435.28ms, mfu 0.00%\r\n","step 56: train loss 2.8384, val loss 2.8563\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 56: loss 2.7331, time 4377.33ms, mfu 0.00%\r\n","step 57: train loss 2.8249, val loss 2.8387\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 57: loss 2.8523, time 4784.72ms, mfu 0.00%\r\n","step 58: train loss 2.8108, val loss 2.8149\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 58: loss 2.8405, time 4686.42ms, mfu 0.00%\r\n","step 59: train loss 2.8136, val loss 2.8201\r\n","iter 59: loss 2.8322, time 4793.75ms, mfu 0.00%\r\n","step 60: train loss 2.8089, val loss 2.8146\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 60: loss 2.7747, time 5603.13ms, mfu 0.00%\r\n","step 61: train loss 2.7867, val loss 2.7957\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 61: loss 2.7930, time 4547.42ms, mfu 0.00%\r\n","step 62: train loss 2.7874, val loss 2.7942\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 62: loss 2.8293, time 4772.94ms, mfu 0.00%\r\n","step 63: train loss 2.7852, val loss 2.7903\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 63: loss 2.7791, time 4655.77ms, mfu 0.00%\r\n","step 64: train loss 2.7653, val loss 2.7833\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 64: loss 2.7191, time 4679.97ms, mfu 0.00%\r\n","step 65: train loss 2.7596, val loss 2.7795\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 65: loss 2.7813, time 4685.12ms, mfu 0.00%\r\n","step 66: train loss 2.7652, val loss 2.7791\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 66: loss 2.7450, time 4657.49ms, mfu 0.00%\r\n","step 67: train loss 2.7540, val loss 2.7700\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 67: loss 2.6875, time 5134.28ms, mfu 0.00%\r\n","step 68: train loss 2.7335, val loss 2.7492\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 68: loss 2.8111, time 4733.78ms, mfu 0.00%\r\n","step 69: train loss 2.7232, val loss 2.7235\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 69: loss 2.7291, time 4489.83ms, mfu 0.00%\r\n","step 70: train loss 2.7277, val loss 2.7264\r\n","iter 70: loss 2.7071, time 4667.33ms, mfu 0.00%\r\n","step 71: train loss 2.7167, val loss 2.7227\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 71: loss 2.7682, time 4619.79ms, mfu 0.00%\r\n","step 72: train loss 2.7082, val loss 2.7036\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 72: loss 2.6151, time 4650.54ms, mfu 0.00%\r\n","step 73: train loss 2.7046, val loss 2.7046\r\n","iter 73: loss 2.7146, time 4733.88ms, mfu 0.00%\r\n","step 74: train loss 2.7130, val loss 2.7043\r\n","iter 74: loss 2.6650, time 5281.45ms, mfu 0.00%\r\n","step 75: train loss 2.7004, val loss 2.7081\r\n","iter 75: loss 2.6811, time 4623.05ms, mfu 0.00%\r\n","step 76: train loss 2.6881, val loss 2.7007\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 76: loss 2.7535, time 4692.68ms, mfu 0.00%\r\n","step 77: train loss 2.6809, val loss 2.6951\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 77: loss 2.6363, time 4601.64ms, mfu 0.00%\r\n","step 78: train loss 2.6918, val loss 2.6895\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 78: loss 2.7383, time 4693.48ms, mfu 0.00%\r\n","step 79: train loss 2.6905, val loss 2.6908\r\n","iter 79: loss 2.6738, time 4736.98ms, mfu 0.00%\r\n","step 80: train loss 2.6679, val loss 2.6870\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 80: loss 2.7252, time 4653.90ms, mfu 0.00%\r\n","step 81: train loss 2.6701, val loss 2.6788\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 81: loss 2.6339, time 5703.57ms, mfu 0.00%\r\n","step 82: train loss 2.6622, val loss 2.6708\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 82: loss 2.6721, time 4786.98ms, mfu 0.00%\r\n","step 83: train loss 2.6701, val loss 2.6648\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 83: loss 2.5895, time 4828.03ms, mfu 0.00%\r\n","step 84: train loss 2.6589, val loss 2.6596\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 84: loss 2.6917, time 4851.76ms, mfu 0.00%\r\n","step 85: train loss 2.6599, val loss 2.6525\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 85: loss 2.6682, time 4918.15ms, mfu 0.00%\r\n","step 86: train loss 2.6574, val loss 2.6517\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 86: loss 2.6821, time 4856.68ms, mfu 0.00%\r\n","step 87: train loss 2.6588, val loss 2.6488\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 87: loss 2.5995, time 5255.91ms, mfu 0.00%\r\n","step 88: train loss 2.6442, val loss 2.6495\r\n","iter 88: loss 2.6408, time 4611.28ms, mfu 0.00%\r\n","step 89: train loss 2.6408, val loss 2.6434\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 89: loss 2.6595, time 4708.12ms, mfu 0.00%\r\n","step 90: train loss 2.6408, val loss 2.6441\r\n","iter 90: loss 2.5472, time 4652.95ms, mfu 0.00%\r\n","step 91: train loss 2.6461, val loss 2.6596\r\n","iter 91: loss 2.6703, time 4610.75ms, mfu 0.00%\r\n","step 92: train loss 2.6484, val loss 2.6574\r\n","iter 92: loss 2.6478, time 4461.39ms, mfu 0.00%\r\n","step 93: train loss 2.6430, val loss 2.6522\r\n","iter 93: loss 2.6163, time 4609.23ms, mfu 0.00%\r\n","step 94: train loss 2.6398, val loss 2.6407\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 94: loss 2.6509, time 5409.31ms, mfu 0.00%\r\n","step 95: train loss 2.6301, val loss 2.6321\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 95: loss 2.6384, time 4711.56ms, mfu 0.00%\r\n","step 96: train loss 2.6339, val loss 2.6379\r\n","iter 96: loss 2.6293, time 4560.51ms, mfu 0.00%\r\n","step 97: train loss 2.6225, val loss 2.6345\r\n","iter 97: loss 2.6523, time 4880.87ms, mfu 0.00%\r\n","step 98: train loss 2.6285, val loss 2.6443\r\n","iter 98: loss 2.6382, time 4656.77ms, mfu 0.00%\r\n","step 99: train loss 2.6393, val loss 2.6430\r\n","iter 99: loss 2.6464, time 4711.67ms, mfu 0.00%\r\n","step 100: train loss 2.6157, val loss 2.6237\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 100: loss 2.6548, time 4755.52ms, mfu 0.00%\r\n","step 101: train loss 2.6127, val loss 2.5990\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 101: loss 2.5952, time 5511.53ms, mfu 0.00%\r\n","step 102: train loss 2.6193, val loss 2.6049\r\n","iter 102: loss 2.6545, time 4825.48ms, mfu 0.00%\r\n","step 103: train loss 2.6093, val loss 2.6142\r\n","iter 103: loss 2.5176, time 4698.71ms, mfu 0.00%\r\n","step 104: train loss 2.6233, val loss 2.6309\r\n","iter 104: loss 2.5712, time 4609.83ms, mfu 0.00%\r\n","step 105: train loss 2.6178, val loss 2.6227\r\n","iter 105: loss 2.5512, time 4649.65ms, mfu 0.00%\r\n","step 106: train loss 2.6083, val loss 2.6179\r\n","iter 106: loss 2.5272, time 4782.92ms, mfu 0.00%\r\n","step 107: train loss 2.6234, val loss 2.6369\r\n","iter 107: loss 2.5388, time 4666.49ms, mfu 0.00%\r\n","step 108: train loss 2.6121, val loss 2.6240\r\n","iter 108: loss 2.5781, time 5414.12ms, mfu 0.00%\r\n","step 109: train loss 2.6018, val loss 2.6089\r\n","iter 109: loss 2.6221, time 4674.37ms, mfu 0.00%\r\n","step 110: train loss 2.6125, val loss 2.6057\r\n","iter 110: loss 2.6760, time 4773.29ms, mfu 0.00%\r\n","step 111: train loss 2.5977, val loss 2.5963\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 111: loss 2.5206, time 4643.62ms, mfu 0.00%\r\n","step 112: train loss 2.5924, val loss 2.5938\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 112: loss 2.5894, time 4705.74ms, mfu 0.00%\r\n","step 113: train loss 2.5890, val loss 2.5964\r\n","iter 113: loss 2.6199, time 4591.90ms, mfu 0.00%\r\n","step 114: train loss 2.5864, val loss 2.5806\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 114: loss 2.5846, time 4790.94ms, mfu 0.00%\r\n","step 115: train loss 2.5807, val loss 2.5683\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 115: loss 2.5307, time 5260.91ms, mfu 0.00%\r\n","step 116: train loss 2.5786, val loss 2.5631\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 116: loss 2.4961, time 4747.14ms, mfu 0.00%\r\n","step 117: train loss 2.5895, val loss 2.5896\r\n","iter 117: loss 2.5758, time 4580.87ms, mfu 0.00%\r\n","step 118: train loss 2.5791, val loss 2.5874\r\n","iter 118: loss 2.6668, time 4650.06ms, mfu 0.00%\r\n","step 119: train loss 2.5768, val loss 2.5832\r\n","iter 119: loss 2.5889, time 4757.49ms, mfu 0.00%\r\n","step 120: train loss 2.5658, val loss 2.5704\r\n","iter 120: loss 2.5177, time 4557.14ms, mfu 0.00%\r\n","step 121: train loss 2.5692, val loss 2.5694\r\n","iter 121: loss 2.6103, time 5178.54ms, mfu 0.00%\r\n","step 122: train loss 2.5695, val loss 2.5744\r\n","iter 122: loss 2.5472, time 4878.04ms, mfu 0.00%\r\n","step 123: train loss 2.5651, val loss 2.5713\r\n","iter 123: loss 2.5182, time 4748.96ms, mfu 0.00%\r\n","step 124: train loss 2.5613, val loss 2.5710\r\n","iter 124: loss 2.5380, time 4544.09ms, mfu 0.00%\r\n","step 125: train loss 2.5664, val loss 2.5790\r\n","iter 125: loss 2.5222, time 4587.35ms, mfu 0.00%\r\n","step 126: train loss 2.5636, val loss 2.5823\r\n","iter 126: loss 2.5255, time 4677.38ms, mfu 0.00%\r\n","step 127: train loss 2.5656, val loss 2.5897\r\n","iter 127: loss 2.5850, time 4628.82ms, mfu 0.00%\r\n","step 128: train loss 2.5715, val loss 2.5942\r\n","iter 128: loss 2.6535, time 5361.66ms, mfu 0.00%\r\n","step 129: train loss 2.5699, val loss 2.5903\r\n","iter 129: loss 2.5311, time 4692.31ms, mfu 0.00%\r\n","step 130: train loss 2.5512, val loss 2.5712\r\n","iter 130: loss 2.6370, time 4683.02ms, mfu 0.00%\r\n","step 131: train loss 2.5575, val loss 2.5726\r\n","iter 131: loss 2.4962, time 4794.74ms, mfu 0.00%\r\n","step 132: train loss 2.5581, val loss 2.5670\r\n","iter 132: loss 2.6181, time 4613.45ms, mfu 0.00%\r\n","step 133: train loss 2.5383, val loss 2.5496\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 133: loss 2.5395, time 4621.10ms, mfu 0.00%\r\n","step 134: train loss 2.5465, val loss 2.5539\r\n","iter 134: loss 2.6027, time 4553.99ms, mfu 0.00%\r\n","step 135: train loss 2.5468, val loss 2.5517\r\n","iter 135: loss 2.5163, time 5210.83ms, mfu 0.00%\r\n","step 136: train loss 2.5447, val loss 2.5501\r\n","iter 136: loss 2.5583, time 4565.88ms, mfu 0.00%\r\n","step 137: train loss 2.5468, val loss 2.5577\r\n","iter 137: loss 2.4955, time 4550.20ms, mfu 0.00%\r\n","step 138: train loss 2.5445, val loss 2.5501\r\n","iter 138: loss 2.4911, time 4434.77ms, mfu 0.00%\r\n","step 139: train loss 2.5339, val loss 2.5463\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 139: loss 2.6229, time 4679.54ms, mfu 0.00%\r\n","step 140: train loss 2.5452, val loss 2.5424\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 140: loss 2.4750, time 4545.20ms, mfu 0.00%\r\n","step 141: train loss 2.5479, val loss 2.5435\r\n","iter 141: loss 2.5597, time 4562.73ms, mfu 0.00%\r\n","step 142: train loss 2.5496, val loss 2.5429\r\n","iter 142: loss 2.5229, time 5286.62ms, mfu 0.00%\r\n","step 143: train loss 2.5472, val loss 2.5588\r\n","iter 143: loss 2.5717, time 4531.02ms, mfu 0.00%\r\n","step 144: train loss 2.5407, val loss 2.5525\r\n","iter 144: loss 2.5614, time 4668.78ms, mfu 0.00%\r\n","step 145: train loss 2.5376, val loss 2.5579\r\n","iter 145: loss 2.5714, time 4580.95ms, mfu 0.00%\r\n","step 146: train loss 2.5393, val loss 2.5384\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 146: loss 2.5257, time 4627.51ms, mfu 0.00%\r\n","step 147: train loss 2.5298, val loss 2.5306\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 147: loss 2.5661, time 4492.16ms, mfu 0.00%\r\n","step 148: train loss 2.5297, val loss 2.5275\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 148: loss 2.5270, time 4560.65ms, mfu 0.00%\r\n","step 149: train loss 2.5265, val loss 2.5301\r\n","iter 149: loss 2.4821, time 4880.10ms, mfu 0.00%\r\n","step 150: train loss 2.5287, val loss 2.5289\r\n","iter 150: loss 2.4727, time 4543.68ms, mfu 0.00%\r\n","step 151: train loss 2.5164, val loss 2.5292\r\n","iter 151: loss 2.4763, time 4430.24ms, mfu 0.00%\r\n","step 152: train loss 2.5165, val loss 2.5248\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 152: loss 2.4250, time 4635.85ms, mfu 0.00%\r\n","step 153: train loss 2.5238, val loss 2.5340\r\n","iter 153: loss 2.5771, time 4742.51ms, mfu 0.00%\r\n","step 154: train loss 2.5302, val loss 2.5290\r\n","iter 154: loss 2.4380, time 4615.63ms, mfu 0.00%\r\n","step 155: train loss 2.5461, val loss 2.5423\r\n","iter 155: loss 2.6410, time 4596.08ms, mfu 0.00%\r\n","step 156: train loss 2.5477, val loss 2.5452\r\n","iter 156: loss 2.5445, time 5395.68ms, mfu 0.00%\r\n","step 157: train loss 2.5183, val loss 2.5293\r\n","iter 157: loss 2.5405, time 4541.52ms, mfu 0.00%\r\n","step 158: train loss 2.5189, val loss 2.5287\r\n","iter 158: loss 2.4906, time 4756.40ms, mfu 0.00%\r\n","step 159: train loss 2.5291, val loss 2.5350\r\n","iter 159: loss 2.4315, time 4670.39ms, mfu 0.00%\r\n","step 160: train loss 2.5347, val loss 2.5489\r\n","iter 160: loss 2.4587, time 4644.89ms, mfu 0.00%\r\n","step 161: train loss 2.5481, val loss 2.5596\r\n","iter 161: loss 2.5619, time 4784.64ms, mfu 0.00%\r\n","step 162: train loss 2.5271, val loss 2.5375\r\n","iter 162: loss 2.4469, time 4679.15ms, mfu 0.00%\r\n","step 163: train loss 2.5240, val loss 2.5237\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 163: loss 2.4961, time 5492.31ms, mfu 0.00%\r\n","step 164: train loss 2.5180, val loss 2.5242\r\n","iter 164: loss 2.5512, time 4854.67ms, mfu 0.00%\r\n","step 165: train loss 2.5116, val loss 2.5204\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 165: loss 2.4679, time 4999.04ms, mfu 0.00%\r\n","step 166: train loss 2.5086, val loss 2.5146\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 166: loss 2.4689, time 5042.42ms, mfu 0.00%\r\n","step 167: train loss 2.5043, val loss 2.5071\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 167: loss 2.6496, time 4835.92ms, mfu 0.00%\r\n","step 168: train loss 2.5065, val loss 2.5167\r\n","iter 168: loss 2.4696, time 4803.10ms, mfu 0.00%\r\n","step 169: train loss 2.5021, val loss 2.5252\r\n","iter 169: loss 2.4383, time 5413.67ms, mfu 0.00%\r\n","step 170: train loss 2.5095, val loss 2.5210\r\n","iter 170: loss 2.5046, time 5147.66ms, mfu 0.00%\r\n","step 171: train loss 2.5007, val loss 2.5137\r\n","iter 171: loss 2.5698, time 4940.98ms, mfu 0.00%\r\n","step 172: train loss 2.4957, val loss 2.5134\r\n","iter 172: loss 2.3987, time 4816.30ms, mfu 0.00%\r\n","step 173: train loss 2.4933, val loss 2.5060\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 173: loss 2.4362, time 4846.70ms, mfu 0.00%\r\n","step 174: train loss 2.5027, val loss 2.5148\r\n","iter 174: loss 2.6065, time 4760.69ms, mfu 0.00%\r\n","step 175: train loss 2.5033, val loss 2.5186\r\n","iter 175: loss 2.4948, time 4755.80ms, mfu 0.00%\r\n","step 176: train loss 2.4933, val loss 2.5090\r\n","iter 176: loss 2.4755, time 5606.50ms, mfu 0.00%\r\n","step 177: train loss 2.4896, val loss 2.5073\r\n","iter 177: loss 2.4476, time 4823.57ms, mfu 0.00%\r\n","step 178: train loss 2.4952, val loss 2.5080\r\n","iter 178: loss 2.5624, time 4749.57ms, mfu 0.00%\r\n","step 179: train loss 2.5019, val loss 2.5225\r\n","iter 179: loss 2.4543, time 4837.25ms, mfu 0.00%\r\n","step 180: train loss 2.4908, val loss 2.5073\r\n","iter 180: loss 2.4800, time 4652.18ms, mfu 0.00%\r\n","step 181: train loss 2.4958, val loss 2.4979\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 181: loss 2.4691, time 4868.07ms, mfu 0.00%\r\n","step 182: train loss 2.4945, val loss 2.4934\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 182: loss 2.4920, time 4736.84ms, mfu 0.00%\r\n","step 183: train loss 2.4991, val loss 2.4965\r\n","iter 183: loss 2.4536, time 5368.36ms, mfu 0.00%\r\n","step 184: train loss 2.4838, val loss 2.4934\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 184: loss 2.4828, time 4958.04ms, mfu 0.00%\r\n","step 185: train loss 2.4872, val loss 2.4939\r\n","iter 185: loss 2.5211, time 5003.59ms, mfu 0.00%\r\n","step 186: train loss 2.5027, val loss 2.5118\r\n","iter 186: loss 2.5300, time 4850.34ms, mfu 0.00%\r\n","step 187: train loss 2.4969, val loss 2.5097\r\n","iter 187: loss 2.5102, time 4992.46ms, mfu 0.00%\r\n","step 188: train loss 2.4939, val loss 2.5009\r\n","iter 188: loss 2.4956, time 4936.06ms, mfu 0.00%\r\n","step 189: train loss 2.4888, val loss 2.4947\r\n","iter 189: loss 2.4740, time 5729.88ms, mfu 0.00%\r\n","step 190: train loss 2.4912, val loss 2.5034\r\n","iter 190: loss 2.5033, time 4887.85ms, mfu 0.00%\r\n","step 191: train loss 2.4916, val loss 2.4962\r\n","iter 191: loss 2.5445, time 4706.57ms, mfu 0.00%\r\n","step 192: train loss 2.4802, val loss 2.4932\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 192: loss 2.4458, time 4606.93ms, mfu 0.00%\r\n","step 193: train loss 2.4869, val loss 2.4970\r\n","iter 193: loss 2.4497, time 4729.72ms, mfu 0.00%\r\n","step 194: train loss 2.4832, val loss 2.4980\r\n","iter 194: loss 2.4476, time 4703.65ms, mfu 0.00%\r\n","step 195: train loss 2.4843, val loss 2.4971\r\n","iter 195: loss 2.5027, time 4639.27ms, mfu 0.00%\r\n","step 196: train loss 2.4899, val loss 2.5127\r\n","iter 196: loss 2.5206, time 5200.11ms, mfu 0.00%\r\n","step 197: train loss 2.4918, val loss 2.4999\r\n","iter 197: loss 2.4552, time 4827.23ms, mfu 0.00%\r\n","step 198: train loss 2.4849, val loss 2.5014\r\n","iter 198: loss 2.5340, time 4627.88ms, mfu 0.00%\r\n","step 199: train loss 2.4870, val loss 2.4953\r\n","iter 199: loss 2.4706, time 4711.31ms, mfu 0.00%\r\n","step 200: train loss 2.4786, val loss 2.4884\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 200: loss 2.4900, time 4679.28ms, mfu 0.00%\r\n","step 201: train loss 2.4772, val loss 2.4891\r\n","iter 201: loss 2.5390, time 4744.31ms, mfu 0.00%\r\n","step 202: train loss 2.4873, val loss 2.4827\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 202: loss 2.4196, time 4717.74ms, mfu 0.00%\r\n","step 203: train loss 2.4792, val loss 2.4905\r\n","iter 203: loss 2.5687, time 5533.34ms, mfu 0.00%\r\n","step 204: train loss 2.4676, val loss 2.4896\r\n","iter 204: loss 2.5397, time 4613.51ms, mfu 0.00%\r\n","step 205: train loss 2.4584, val loss 2.4784\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 205: loss 2.4480, time 4761.26ms, mfu 0.00%\r\n","step 206: train loss 2.4628, val loss 2.4782\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 206: loss 2.4758, time 4749.36ms, mfu 0.00%\r\n","step 207: train loss 2.4628, val loss 2.4810\r\n","iter 207: loss 2.5181, time 4815.99ms, mfu 0.00%\r\n","step 208: train loss 2.4684, val loss 2.4738\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 208: loss 2.5006, time 4653.50ms, mfu 0.00%\r\n","step 209: train loss 2.4716, val loss 2.4838\r\n","iter 209: loss 2.5342, time 4993.96ms, mfu 0.00%\r\n","step 210: train loss 2.4766, val loss 2.4925\r\n","iter 210: loss 2.5144, time 5047.67ms, mfu 0.00%\r\n","step 211: train loss 2.4812, val loss 2.5009\r\n","iter 211: loss 2.4057, time 4555.05ms, mfu 0.00%\r\n","step 212: train loss 2.4889, val loss 2.5094\r\n","iter 212: loss 2.4287, time 4651.59ms, mfu 0.00%\r\n","step 213: train loss 2.4791, val loss 2.5078\r\n","iter 213: loss 2.4299, time 4579.80ms, mfu 0.00%\r\n","step 214: train loss 2.4859, val loss 2.5043\r\n","iter 214: loss 2.4171, time 4626.66ms, mfu 0.00%\r\n","step 215: train loss 2.4820, val loss 2.5004\r\n","iter 215: loss 2.5838, time 4648.20ms, mfu 0.00%\r\n","step 216: train loss 2.4675, val loss 2.4935\r\n","iter 216: loss 2.4553, time 5481.73ms, mfu 0.00%\r\n","step 217: train loss 2.4640, val loss 2.4799\r\n","iter 217: loss 2.5024, time 4755.32ms, mfu 0.00%\r\n","step 218: train loss 2.4616, val loss 2.4759\r\n","iter 218: loss 2.4798, time 4832.03ms, mfu 0.00%\r\n","step 219: train loss 2.4701, val loss 2.4837\r\n","iter 219: loss 2.5094, time 4771.46ms, mfu 0.00%\r\n","step 220: train loss 2.4691, val loss 2.4774\r\n","iter 220: loss 2.4338, time 4975.82ms, mfu 0.00%\r\n","step 221: train loss 2.4554, val loss 2.4672\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 221: loss 2.3961, time 4814.79ms, mfu 0.00%\r\n","step 222: train loss 2.4455, val loss 2.4585\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 222: loss 2.5535, time 4846.05ms, mfu 0.00%\r\n","step 223: train loss 2.4646, val loss 2.4720\r\n","iter 223: loss 2.4801, time 5424.33ms, mfu 0.00%\r\n","step 224: train loss 2.4706, val loss 2.4803\r\n","iter 224: loss 2.4777, time 4782.53ms, mfu 0.00%\r\n","step 225: train loss 2.4642, val loss 2.4819\r\n","iter 225: loss 2.3947, time 4803.49ms, mfu 0.00%\r\n","step 226: train loss 2.4598, val loss 2.4779\r\n","iter 226: loss 2.4102, time 4812.18ms, mfu 0.00%\r\n","step 227: train loss 2.4586, val loss 2.4710\r\n","iter 227: loss 2.4895, time 4667.24ms, mfu 0.00%\r\n","step 228: train loss 2.4507, val loss 2.4630\r\n","iter 228: loss 2.5052, time 4758.61ms, mfu 0.00%\r\n","step 229: train loss 2.4517, val loss 2.4645\r\n","iter 229: loss 2.4611, time 4910.07ms, mfu 0.00%\r\n","step 230: train loss 2.4599, val loss 2.4681\r\n","iter 230: loss 2.4940, time 5182.14ms, mfu 0.00%\r\n","step 231: train loss 2.4593, val loss 2.4678\r\n","iter 231: loss 2.5094, time 4813.78ms, mfu 0.00%\r\n","step 232: train loss 2.4493, val loss 2.4495\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 232: loss 2.4404, time 5320.11ms, mfu 0.00%\r\n","step 233: train loss 2.4459, val loss 2.4560\r\n","iter 233: loss 2.4863, time 4891.62ms, mfu 0.00%\r\n","step 234: train loss 2.4461, val loss 2.4523\r\n","iter 234: loss 2.4477, time 5031.57ms, mfu 0.00%\r\n","step 235: train loss 2.4448, val loss 2.4534\r\n","iter 235: loss 2.4267, time 4900.47ms, mfu 0.00%\r\n","step 236: train loss 2.4484, val loss 2.4554\r\n","iter 236: loss 2.3950, time 5649.27ms, mfu 0.00%\r\n","step 237: train loss 2.4491, val loss 2.4598\r\n","iter 237: loss 2.4743, time 5030.47ms, mfu 0.00%\r\n","step 238: train loss 2.4451, val loss 2.4639\r\n","iter 238: loss 2.3600, time 4931.84ms, mfu 0.00%\r\n","step 239: train loss 2.4438, val loss 2.4575\r\n","iter 239: loss 2.5339, time 4706.88ms, mfu 0.00%\r\n","step 240: train loss 2.4538, val loss 2.4599\r\n","iter 240: loss 2.4701, time 4748.37ms, mfu 0.00%\r\n","step 241: train loss 2.4395, val loss 2.4491\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 241: loss 2.4486, time 4841.93ms, mfu 0.00%\r\n","step 242: train loss 2.4475, val loss 2.4542\r\n","iter 242: loss 2.4640, time 5494.87ms, mfu 0.00%\r\n","step 243: train loss 2.4448, val loss 2.4602\r\n","iter 243: loss 2.5007, time 5020.68ms, mfu 0.00%\r\n","step 244: train loss 2.4437, val loss 2.4553\r\n","iter 244: loss 2.3644, time 4801.41ms, mfu 0.00%\r\n","step 245: train loss 2.4347, val loss 2.4516\r\n","iter 245: loss 2.4013, time 4694.99ms, mfu 0.00%\r\n","step 246: train loss 2.4350, val loss 2.4476\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 246: loss 2.4722, time 4851.39ms, mfu 0.00%\r\n","step 247: train loss 2.4318, val loss 2.4402\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 247: loss 2.4644, time 4705.69ms, mfu 0.00%\r\n","step 248: train loss 2.4238, val loss 2.4305\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 248: loss 2.4744, time 4877.80ms, mfu 0.00%\r\n","step 249: train loss 2.4300, val loss 2.4212\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 249: loss 2.4327, time 5405.08ms, mfu 0.00%\r\n","step 250: train loss 2.4238, val loss 2.4220\r\n","iter 250: loss 2.4592, time 4818.49ms, mfu 0.00%\r\n","step 251: train loss 2.4182, val loss 2.4186\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 251: loss 2.4552, time 4818.91ms, mfu 0.00%\r\n","step 252: train loss 2.4199, val loss 2.4193\r\n","iter 252: loss 2.3979, time 4782.04ms, mfu 0.00%\r\n","step 253: train loss 2.4163, val loss 2.4215\r\n","iter 253: loss 2.3794, time 4792.79ms, mfu 0.00%\r\n","step 254: train loss 2.4284, val loss 2.4256\r\n","iter 254: loss 2.4050, time 4881.98ms, mfu 0.00%\r\n","step 255: train loss 2.4199, val loss 2.4265\r\n","iter 255: loss 2.3337, time 4871.16ms, mfu 0.00%\r\n","step 256: train loss 2.4191, val loss 2.4155\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 256: loss 2.3786, time 5513.79ms, mfu 0.00%\r\n","step 257: train loss 2.4171, val loss 2.4142\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 257: loss 2.3712, time 4876.68ms, mfu 0.00%\r\n","step 258: train loss 2.4198, val loss 2.4347\r\n","iter 258: loss 2.4209, time 4890.65ms, mfu 0.00%\r\n","step 259: train loss 2.4252, val loss 2.4417\r\n","iter 259: loss 2.4366, time 4795.56ms, mfu 0.00%\r\n","step 260: train loss 2.4204, val loss 2.4361\r\n","iter 260: loss 2.3539, time 4685.46ms, mfu 0.00%\r\n","step 261: train loss 2.4235, val loss 2.4408\r\n","iter 261: loss 2.3664, time 4766.10ms, mfu 0.00%\r\n","step 262: train loss 2.4193, val loss 2.4320\r\n","iter 262: loss 2.4586, time 5343.98ms, mfu 0.00%\r\n","step 263: train loss 2.4142, val loss 2.4266\r\n","iter 263: loss 2.4883, time 4600.74ms, mfu 0.00%\r\n","step 264: train loss 2.4164, val loss 2.4227\r\n","iter 264: loss 2.3405, time 4652.71ms, mfu 0.00%\r\n","step 265: train loss 2.4090, val loss 2.4197\r\n","iter 265: loss 2.3747, time 4590.17ms, mfu 0.00%\r\n","step 266: train loss 2.4161, val loss 2.4273\r\n","iter 266: loss 2.4193, time 4592.20ms, mfu 0.00%\r\n","step 267: train loss 2.4161, val loss 2.4229\r\n","iter 267: loss 2.3142, time 4763.77ms, mfu 0.00%\r\n","step 268: train loss 2.4169, val loss 2.4182\r\n","iter 268: loss 2.4888, time 4579.36ms, mfu 0.00%\r\n","step 269: train loss 2.4152, val loss 2.4284\r\n","iter 269: loss 2.3606, time 5489.10ms, mfu 0.00%\r\n","step 270: train loss 2.4214, val loss 2.4233\r\n","iter 270: loss 2.3879, time 4643.67ms, mfu 0.00%\r\n","step 271: train loss 2.4214, val loss 2.4305\r\n","iter 271: loss 2.4044, time 4799.90ms, mfu 0.00%\r\n","step 272: train loss 2.4234, val loss 2.4374\r\n","iter 272: loss 2.4457, time 4676.96ms, mfu 0.00%\r\n","step 273: train loss 2.4161, val loss 2.4224\r\n","iter 273: loss 2.4947, time 4665.44ms, mfu 0.00%\r\n","step 274: train loss 2.4094, val loss 2.4018\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 274: loss 2.3691, time 4670.32ms, mfu 0.00%\r\n","step 275: train loss 2.4116, val loss 2.4107\r\n","iter 275: loss 2.3672, time 4827.61ms, mfu 0.00%\r\n","step 276: train loss 2.4190, val loss 2.4183\r\n","iter 276: loss 2.3744, time 5319.77ms, mfu 0.00%\r\n","step 277: train loss 2.4161, val loss 2.4207\r\n","iter 277: loss 2.4075, time 4771.25ms, mfu 0.00%\r\n","step 278: train loss 2.4031, val loss 2.4139\r\n","iter 278: loss 2.4930, time 4620.72ms, mfu 0.00%\r\n","step 279: train loss 2.4015, val loss 2.4022\r\n","iter 279: loss 2.4201, time 4722.06ms, mfu 0.00%\r\n","step 280: train loss 2.3983, val loss 2.4125\r\n","iter 280: loss 2.4168, time 4633.31ms, mfu 0.00%\r\n","step 281: train loss 2.4057, val loss 2.4247\r\n","iter 281: loss 2.4243, time 4607.40ms, mfu 0.00%\r\n","step 282: train loss 2.4106, val loss 2.4212\r\n","iter 282: loss 2.3919, time 4608.01ms, mfu 0.00%\r\n","step 283: train loss 2.4049, val loss 2.4363\r\n","iter 283: loss 2.3871, time 5301.35ms, mfu 0.00%\r\n","step 284: train loss 2.4084, val loss 2.4351\r\n","iter 284: loss 2.4440, time 4402.43ms, mfu 0.00%\r\n","step 285: train loss 2.4083, val loss 2.4251\r\n","iter 285: loss 2.4060, time 4531.80ms, mfu 0.00%\r\n","step 286: train loss 2.4106, val loss 2.4194\r\n","iter 286: loss 2.4375, time 4612.71ms, mfu 0.00%\r\n","step 287: train loss 2.4090, val loss 2.4252\r\n","iter 287: loss 2.3662, time 4517.58ms, mfu 0.00%\r\n","step 288: train loss 2.4282, val loss 2.4329\r\n","iter 288: loss 2.4190, time 4523.03ms, mfu 0.00%\r\n","step 289: train loss 2.4311, val loss 2.4250\r\n","iter 289: loss 2.4165, time 4492.42ms, mfu 0.00%\r\n","step 290: train loss 2.4211, val loss 2.4115\r\n","iter 290: loss 2.4412, time 5021.89ms, mfu 0.00%\r\n","step 291: train loss 2.4210, val loss 2.4268\r\n","iter 291: loss 2.3396, time 4561.46ms, mfu 0.00%\r\n","step 292: train loss 2.4239, val loss 2.4273\r\n","iter 292: loss 2.4380, time 4567.61ms, mfu 0.00%\r\n","step 293: train loss 2.4081, val loss 2.4125\r\n","iter 293: loss 2.3953, time 4576.74ms, mfu 0.00%\r\n","step 294: train loss 2.4105, val loss 2.4217\r\n","iter 294: loss 2.4270, time 4379.53ms, mfu 0.00%\r\n","step 295: train loss 2.4050, val loss 2.4148\r\n","iter 295: loss 2.4387, time 4352.24ms, mfu 0.00%\r\n","step 296: train loss 2.3898, val loss 2.4067\r\n","iter 296: loss 2.3575, time 4540.39ms, mfu 0.00%\r\n","step 297: train loss 2.3905, val loss 2.4107\r\n","iter 297: loss 2.3905, time 5063.55ms, mfu 0.00%\r\n","step 298: train loss 2.3971, val loss 2.4141\r\n","iter 298: loss 2.4570, time 4579.69ms, mfu 0.00%\r\n","step 299: train loss 2.3972, val loss 2.4262\r\n","iter 299: loss 2.4136, time 4704.49ms, mfu 0.00%\r\n","step 300: train loss 2.4053, val loss 2.4316\r\n","iter 300: loss 2.4085, time 4450.77ms, mfu 0.00%\r\n","step 301: train loss 2.3957, val loss 2.4218\r\n","iter 301: loss 2.3920, time 4458.06ms, mfu 0.00%\r\n","step 302: train loss 2.3976, val loss 2.4146\r\n","iter 302: loss 2.3960, time 4637.38ms, mfu 0.00%\r\n","step 303: train loss 2.4044, val loss 2.4129\r\n","iter 303: loss 2.4451, time 4500.19ms, mfu 0.00%\r\n","step 304: train loss 2.4034, val loss 2.4201\r\n","iter 304: loss 2.4165, time 5250.76ms, mfu 0.00%\r\n","step 305: train loss 2.4011, val loss 2.4180\r\n","iter 305: loss 2.4079, time 4431.59ms, mfu 0.00%\r\n","step 306: train loss 2.3948, val loss 2.4215\r\n","iter 306: loss 2.3678, time 4321.29ms, mfu 0.00%\r\n","step 307: train loss 2.3984, val loss 2.4204\r\n","iter 307: loss 2.3884, time 4415.06ms, mfu 0.00%\r\n","step 308: train loss 2.3936, val loss 2.4033\r\n","iter 308: loss 2.5262, time 4382.88ms, mfu 0.00%\r\n","step 309: train loss 2.3948, val loss 2.4207\r\n","iter 309: loss 2.4120, time 4602.19ms, mfu 0.00%\r\n","step 310: train loss 2.3910, val loss 2.4162\r\n","iter 310: loss 2.3322, time 4631.16ms, mfu 0.00%\r\n","step 311: train loss 2.3836, val loss 2.3913\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 311: loss 2.3501, time 5068.76ms, mfu 0.00%\r\n","step 312: train loss 2.3936, val loss 2.4038\r\n","iter 312: loss 2.4110, time 4496.37ms, mfu 0.00%\r\n","step 313: train loss 2.4038, val loss 2.4204\r\n","iter 313: loss 2.3310, time 4526.69ms, mfu 0.00%\r\n","step 314: train loss 2.3946, val loss 2.4004\r\n","iter 314: loss 2.4146, time 4443.08ms, mfu 0.00%\r\n","step 315: train loss 2.3931, val loss 2.3995\r\n","iter 315: loss 2.3604, time 4365.31ms, mfu 0.00%\r\n","step 316: train loss 2.4016, val loss 2.4039\r\n","iter 316: loss 2.4164, time 4658.76ms, mfu 0.00%\r\n","step 317: train loss 2.3889, val loss 2.4027\r\n","iter 317: loss 2.4273, time 4493.31ms, mfu 0.00%\r\n","step 318: train loss 2.3837, val loss 2.3950\r\n","iter 318: loss 2.3610, time 5041.45ms, mfu 0.00%\r\n","step 319: train loss 2.3794, val loss 2.3984\r\n","iter 319: loss 2.3740, time 4552.85ms, mfu 0.00%\r\n","step 320: train loss 2.3824, val loss 2.3964\r\n","iter 320: loss 2.3339, time 4383.26ms, mfu 0.00%\r\n","step 321: train loss 2.3822, val loss 2.3903\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 321: loss 2.3462, time 4642.76ms, mfu 0.00%\r\n","step 322: train loss 2.3921, val loss 2.4048\r\n","iter 322: loss 2.3544, time 4510.42ms, mfu 0.00%\r\n","step 323: train loss 2.3862, val loss 2.3988\r\n","iter 323: loss 2.3733, time 4567.85ms, mfu 0.00%\r\n","step 324: train loss 2.3811, val loss 2.4001\r\n","iter 324: loss 2.3556, time 4466.93ms, mfu 0.00%\r\n","step 325: train loss 2.3849, val loss 2.4065\r\n","iter 325: loss 2.4627, time 5125.48ms, mfu 0.00%\r\n","step 326: train loss 2.3855, val loss 2.3963\r\n","iter 326: loss 2.3877, time 4458.83ms, mfu 0.00%\r\n","step 327: train loss 2.3808, val loss 2.4011\r\n","iter 327: loss 2.3051, time 4520.20ms, mfu 0.00%\r\n","step 328: train loss 2.3780, val loss 2.3935\r\n","iter 328: loss 2.3247, time 4408.67ms, mfu 0.00%\r\n","step 329: train loss 2.3842, val loss 2.3959\r\n","iter 329: loss 2.3571, time 4560.20ms, mfu 0.00%\r\n","step 330: train loss 2.3889, val loss 2.3957\r\n","iter 330: loss 2.4064, time 4393.77ms, mfu 0.00%\r\n","step 331: train loss 2.3835, val loss 2.3887\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 331: loss 2.3303, time 4575.81ms, mfu 0.00%\r\n","step 332: train loss 2.3748, val loss 2.3915\r\n","iter 332: loss 2.3835, time 4922.92ms, mfu 0.00%\r\n","step 333: train loss 2.3803, val loss 2.3993\r\n","iter 333: loss 2.3712, time 4507.07ms, mfu 0.00%\r\n","step 334: train loss 2.3715, val loss 2.4006\r\n","iter 334: loss 2.3798, time 4506.92ms, mfu 0.00%\r\n","step 335: train loss 2.3773, val loss 2.3986\r\n","iter 335: loss 2.3452, time 4557.52ms, mfu 0.00%\r\n","step 336: train loss 2.3691, val loss 2.3835\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 336: loss 2.2677, time 4656.00ms, mfu 0.00%\r\n","step 337: train loss 2.3650, val loss 2.3797\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 337: loss 2.2991, time 4669.15ms, mfu 0.00%\r\n","step 338: train loss 2.3664, val loss 2.3865\r\n","iter 338: loss 2.3222, time 4642.51ms, mfu 0.00%\r\n","step 339: train loss 2.3707, val loss 2.3808\r\n","iter 339: loss 2.4078, time 5048.10ms, mfu 0.00%\r\n","step 340: train loss 2.3722, val loss 2.3874\r\n","iter 340: loss 2.4330, time 4853.84ms, mfu 0.00%\r\n","step 341: train loss 2.3719, val loss 2.3880\r\n","iter 341: loss 2.3181, time 4673.04ms, mfu 0.00%\r\n","step 342: train loss 2.3672, val loss 2.3915\r\n","iter 342: loss 2.3765, time 4809.18ms, mfu 0.00%\r\n","step 343: train loss 2.3693, val loss 2.3883\r\n","iter 343: loss 2.3421, time 4584.69ms, mfu 0.00%\r\n","step 344: train loss 2.3714, val loss 2.3891\r\n","iter 344: loss 2.3678, time 4643.66ms, mfu 0.00%\r\n","step 345: train loss 2.3641, val loss 2.3865\r\n","iter 345: loss 2.3057, time 4683.80ms, mfu 0.00%\r\n","step 346: train loss 2.3602, val loss 2.3800\r\n","iter 346: loss 2.3671, time 5189.74ms, mfu 0.00%\r\n","step 347: train loss 2.3687, val loss 2.3794\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 347: loss 2.4302, time 4687.79ms, mfu 0.00%\r\n","step 348: train loss 2.3719, val loss 2.3743\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 348: loss 2.4831, time 4560.66ms, mfu 0.00%\r\n","step 349: train loss 2.3650, val loss 2.3771\r\n","iter 349: loss 2.3431, time 4458.54ms, mfu 0.00%\r\n","step 350: train loss 2.3699, val loss 2.3918\r\n","iter 350: loss 2.4055, time 4425.51ms, mfu 0.00%\r\n","step 351: train loss 2.3839, val loss 2.3838\r\n","iter 351: loss 2.3579, time 4394.63ms, mfu 0.00%\r\n","step 352: train loss 2.3808, val loss 2.3813\r\n","iter 352: loss 2.4460, time 4427.32ms, mfu 0.00%\r\n","step 353: train loss 2.3706, val loss 2.3728\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 353: loss 2.4366, time 5104.94ms, mfu 0.00%\r\n","step 354: train loss 2.3715, val loss 2.3775\r\n","iter 354: loss 2.4199, time 4704.31ms, mfu 0.00%\r\n","step 355: train loss 2.3752, val loss 2.3788\r\n","iter 355: loss 2.3969, time 4628.51ms, mfu 0.00%\r\n","step 356: train loss 2.3672, val loss 2.3925\r\n","iter 356: loss 2.4193, time 4485.68ms, mfu 0.00%\r\n","step 357: train loss 2.3760, val loss 2.3861\r\n","iter 357: loss 2.3955, time 4701.45ms, mfu 0.00%\r\n","step 358: train loss 2.3629, val loss 2.3843\r\n","iter 358: loss 2.3711, time 4355.70ms, mfu 0.00%\r\n","step 359: train loss 2.3631, val loss 2.3849\r\n","iter 359: loss 2.3313, time 4478.08ms, mfu 0.00%\r\n","step 360: train loss 2.3667, val loss 2.3871\r\n","iter 360: loss 2.3553, time 4938.41ms, mfu 0.00%\r\n","step 361: train loss 2.3630, val loss 2.3859\r\n","iter 361: loss 2.3894, time 4607.34ms, mfu 0.00%\r\n","step 362: train loss 2.3726, val loss 2.3833\r\n","iter 362: loss 2.3814, time 4563.58ms, mfu 0.00%\r\n","step 363: train loss 2.3713, val loss 2.3851\r\n","iter 363: loss 2.3893, time 4660.45ms, mfu 0.00%\r\n","step 364: train loss 2.3705, val loss 2.3778\r\n","iter 364: loss 2.3391, time 4399.29ms, mfu 0.00%\r\n","step 365: train loss 2.3646, val loss 2.3718\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 365: loss 2.4306, time 4668.54ms, mfu 0.00%\r\n","step 366: train loss 2.3557, val loss 2.3685\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 366: loss 2.2958, time 4478.69ms, mfu 0.00%\r\n","step 367: train loss 2.3552, val loss 2.3590\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 367: loss 2.3457, time 4720.75ms, mfu 0.00%\r\n","step 368: train loss 2.3579, val loss 2.3694\r\n","iter 368: loss 2.3725, time 4732.35ms, mfu 0.00%\r\n","step 369: train loss 2.3600, val loss 2.3716\r\n","iter 369: loss 2.2695, time 4433.40ms, mfu 0.00%\r\n","step 370: train loss 2.3669, val loss 2.3770\r\n","iter 370: loss 2.3775, time 4486.14ms, mfu 0.00%\r\n","step 371: train loss 2.3595, val loss 2.3795\r\n","iter 371: loss 2.3623, time 4449.58ms, mfu 0.00%\r\n","step 372: train loss 2.3641, val loss 2.3765\r\n","iter 372: loss 2.3421, time 4468.94ms, mfu 0.00%\r\n","step 373: train loss 2.3574, val loss 2.3649\r\n","iter 373: loss 2.4190, time 4581.89ms, mfu 0.00%\r\n","step 374: train loss 2.3591, val loss 2.3716\r\n","iter 374: loss 2.4088, time 4864.97ms, mfu 0.00%\r\n","step 375: train loss 2.3757, val loss 2.3906\r\n","iter 375: loss 2.3990, time 5091.72ms, mfu 0.00%\r\n","step 376: train loss 2.3692, val loss 2.3816\r\n","iter 376: loss 2.3797, time 4610.44ms, mfu 0.00%\r\n","step 377: train loss 2.3546, val loss 2.3663\r\n","iter 377: loss 2.3053, time 4616.64ms, mfu 0.00%\r\n","step 378: train loss 2.3548, val loss 2.3766\r\n","iter 378: loss 2.3136, time 4481.73ms, mfu 0.00%\r\n","step 379: train loss 2.3710, val loss 2.3769\r\n","iter 379: loss 2.3493, time 4641.93ms, mfu 0.00%\r\n","step 380: train loss 2.3653, val loss 2.3885\r\n","iter 380: loss 2.3377, time 4469.72ms, mfu 0.00%\r\n","step 381: train loss 2.3605, val loss 2.3750\r\n","iter 381: loss 2.3967, time 4766.10ms, mfu 0.00%\r\n","step 382: train loss 2.3542, val loss 2.3759\r\n","iter 382: loss 2.2779, time 4855.08ms, mfu 0.00%\r\n","step 383: train loss 2.3452, val loss 2.3660\r\n","iter 383: loss 2.3268, time 4491.30ms, mfu 0.00%\r\n","step 384: train loss 2.3565, val loss 2.3680\r\n","iter 384: loss 2.3028, time 4316.17ms, mfu 0.00%\r\n","step 385: train loss 2.3528, val loss 2.3643\r\n","iter 385: loss 2.3480, time 4491.39ms, mfu 0.00%\r\n","step 386: train loss 2.3557, val loss 2.3671\r\n","iter 386: loss 2.3332, time 4540.55ms, mfu 0.00%\r\n","step 387: train loss 2.3527, val loss 2.3625\r\n","iter 387: loss 2.4561, time 4414.02ms, mfu 0.00%\r\n","step 388: train loss 2.3414, val loss 2.3588\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 388: loss 2.3449, time 4454.15ms, mfu 0.00%\r\n","step 389: train loss 2.3442, val loss 2.3599\r\n","iter 389: loss 2.4244, time 4990.64ms, mfu 0.00%\r\n","step 390: train loss 2.3512, val loss 2.3589\r\n","iter 390: loss 2.3186, time 4410.29ms, mfu 0.00%\r\n","step 391: train loss 2.3440, val loss 2.3525\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 391: loss 2.3364, time 4497.86ms, mfu 0.00%\r\n","step 392: train loss 2.3318, val loss 2.3373\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 392: loss 2.3858, time 4572.38ms, mfu 0.00%\r\n","step 393: train loss 2.3310, val loss 2.3439\r\n","iter 393: loss 2.3420, time 4501.11ms, mfu 0.00%\r\n","step 394: train loss 2.3446, val loss 2.3514\r\n","iter 394: loss 2.3883, time 4495.64ms, mfu 0.00%\r\n","step 395: train loss 2.3460, val loss 2.3528\r\n","iter 395: loss 2.3438, time 4430.00ms, mfu 0.00%\r\n","step 396: train loss 2.3447, val loss 2.3554\r\n","iter 396: loss 2.3936, time 5221.62ms, mfu 0.00%\r\n","step 397: train loss 2.3415, val loss 2.3452\r\n","iter 397: loss 2.3282, time 4433.81ms, mfu 0.00%\r\n","step 398: train loss 2.3340, val loss 2.3378\r\n","iter 398: loss 2.3457, time 4369.48ms, mfu 0.00%\r\n","step 399: train loss 2.3414, val loss 2.3432\r\n","iter 399: loss 2.3193, time 4581.84ms, mfu 0.00%\r\n","step 400: train loss 2.3360, val loss 2.3487\r\n","iter 400: loss 2.3843, time 4432.75ms, mfu 0.00%\r\n","step 401: train loss 2.3328, val loss 2.3426\r\n","iter 401: loss 2.3124, time 4654.10ms, mfu 0.00%\r\n","step 402: train loss 2.3326, val loss 2.3462\r\n","iter 402: loss 2.3303, time 4369.76ms, mfu 0.00%\r\n","step 403: train loss 2.3421, val loss 2.3526\r\n","iter 403: loss 2.3276, time 4896.77ms, mfu 0.00%\r\n","step 404: train loss 2.3391, val loss 2.3555\r\n","iter 404: loss 2.4670, time 4404.50ms, mfu 0.00%\r\n","step 405: train loss 2.3310, val loss 2.3476\r\n","iter 405: loss 2.3596, time 4295.48ms, mfu 0.00%\r\n","step 406: train loss 2.3347, val loss 2.3401\r\n","iter 406: loss 2.2640, time 4374.10ms, mfu 0.00%\r\n","step 407: train loss 2.3329, val loss 2.3382\r\n","iter 407: loss 2.3391, time 4612.72ms, mfu 0.00%\r\n","step 408: train loss 2.3352, val loss 2.3367\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 408: loss 2.3015, time 4372.16ms, mfu 0.00%\r\n","step 409: train loss 2.3334, val loss 2.3354\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 409: loss 2.3599, time 4449.89ms, mfu 0.00%\r\n","step 410: train loss 2.3394, val loss 2.3457\r\n","iter 410: loss 2.3970, time 5045.92ms, mfu 0.00%\r\n","step 411: train loss 2.3413, val loss 2.3352\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 411: loss 2.3973, time 4299.57ms, mfu 0.00%\r\n","step 412: train loss 2.3325, val loss 2.3378\r\n","iter 412: loss 2.3372, time 4455.60ms, mfu 0.00%\r\n","step 413: train loss 2.3409, val loss 2.3389\r\n","iter 413: loss 2.3252, time 4342.81ms, mfu 0.00%\r\n","step 414: train loss 2.3412, val loss 2.3516\r\n","iter 414: loss 2.2974, time 4501.99ms, mfu 0.00%\r\n","step 415: train loss 2.3406, val loss 2.3450\r\n","iter 415: loss 2.3699, time 4352.31ms, mfu 0.00%\r\n","step 416: train loss 2.3364, val loss 2.3423\r\n","iter 416: loss 2.2391, time 4373.42ms, mfu 0.00%\r\n","step 417: train loss 2.3281, val loss 2.3362\r\n","iter 417: loss 2.2284, time 5014.34ms, mfu 0.00%\r\n","step 418: train loss 2.3203, val loss 2.3366\r\n","iter 418: loss 2.3081, time 4301.51ms, mfu 0.00%\r\n","step 419: train loss 2.3284, val loss 2.3385\r\n","iter 419: loss 2.3580, time 4387.19ms, mfu 0.00%\r\n","step 420: train loss 2.3242, val loss 2.3376\r\n","iter 420: loss 2.3424, time 4439.08ms, mfu 0.00%\r\n","step 421: train loss 2.3255, val loss 2.3374\r\n","iter 421: loss 2.2665, time 4600.43ms, mfu 0.00%\r\n","step 422: train loss 2.3205, val loss 2.3285\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 422: loss 2.2720, time 4651.53ms, mfu 0.00%\r\n","step 423: train loss 2.3219, val loss 2.3308\r\n","iter 423: loss 2.3334, time 4503.66ms, mfu 0.00%\r\n","step 424: train loss 2.3271, val loss 2.3312\r\n","iter 424: loss 2.2861, time 4777.12ms, mfu 0.00%\r\n","step 425: train loss 2.3322, val loss 2.3283\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 425: loss 2.2409, time 4578.99ms, mfu 0.00%\r\n","step 426: train loss 2.3299, val loss 2.3383\r\n","iter 426: loss 2.3128, time 4276.48ms, mfu 0.00%\r\n","step 427: train loss 2.3302, val loss 2.3359\r\n","iter 427: loss 2.3226, time 4340.22ms, mfu 0.00%\r\n","step 428: train loss 2.3345, val loss 2.3442\r\n","iter 428: loss 2.3438, time 4443.11ms, mfu 0.00%\r\n","step 429: train loss 2.3439, val loss 2.3418\r\n","iter 429: loss 2.3818, time 4370.00ms, mfu 0.00%\r\n","step 430: train loss 2.3250, val loss 2.3347\r\n","iter 430: loss 2.3717, time 4399.75ms, mfu 0.00%\r\n","step 431: train loss 2.3082, val loss 2.3217\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 431: loss 2.3349, time 4192.46ms, mfu 0.00%\r\n","step 432: train loss 2.3231, val loss 2.3313\r\n","iter 432: loss 2.4055, time 4968.05ms, mfu 0.00%\r\n","step 433: train loss 2.3413, val loss 2.3532\r\n","iter 433: loss 2.4038, time 4419.98ms, mfu 0.00%\r\n","step 434: train loss 2.3541, val loss 2.3794\r\n","iter 434: loss 2.2718, time 4359.91ms, mfu 0.00%\r\n","step 435: train loss 2.3411, val loss 2.3726\r\n","iter 435: loss 2.4319, time 4270.11ms, mfu 0.00%\r\n","step 436: train loss 2.3194, val loss 2.3496\r\n","iter 436: loss 2.3355, time 4398.35ms, mfu 0.00%\r\n","step 437: train loss 2.3220, val loss 2.3392\r\n","iter 437: loss 2.2982, time 4326.18ms, mfu 0.00%\r\n","step 438: train loss 2.3231, val loss 2.3358\r\n","iter 438: loss 2.3664, time 4456.44ms, mfu 0.00%\r\n","step 439: train loss 2.3173, val loss 2.3290\r\n","iter 439: loss 2.2864, time 4987.26ms, mfu 0.00%\r\n","step 440: train loss 2.3127, val loss 2.3171\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 440: loss 2.3927, time 4312.90ms, mfu 0.00%\r\n","step 441: train loss 2.3110, val loss 2.3190\r\n","iter 441: loss 2.2831, time 4360.22ms, mfu 0.00%\r\n","step 442: train loss 2.3082, val loss 2.3182\r\n","iter 442: loss 2.3212, time 4274.01ms, mfu 0.00%\r\n","step 443: train loss 2.3043, val loss 2.3292\r\n","iter 443: loss 2.3724, time 4456.29ms, mfu 0.00%\r\n","step 444: train loss 2.3160, val loss 2.3318\r\n","iter 444: loss 2.2674, time 4240.44ms, mfu 0.00%\r\n","step 445: train loss 2.3163, val loss 2.3326\r\n","iter 445: loss 2.2897, time 4403.42ms, mfu 0.00%\r\n","step 446: train loss 2.3161, val loss 2.3312\r\n","iter 446: loss 2.2731, time 4975.36ms, mfu 0.00%\r\n","step 447: train loss 2.3017, val loss 2.3251\r\n","iter 447: loss 2.2993, time 4491.95ms, mfu 0.00%\r\n","step 448: train loss 2.3087, val loss 2.3193\r\n","iter 448: loss 2.2874, time 4402.49ms, mfu 0.00%\r\n","step 449: train loss 2.3005, val loss 2.3144\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 449: loss 2.3529, time 4505.92ms, mfu 0.00%\r\n","step 450: train loss 2.3035, val loss 2.3283\r\n","iter 450: loss 2.3520, time 4924.04ms, mfu 0.00%\r\n","step 451: train loss 2.3071, val loss 2.3338\r\n","iter 451: loss 2.2864, time 4597.67ms, mfu 0.00%\r\n","step 452: train loss 2.3083, val loss 2.3276\r\n","iter 452: loss 2.2195, time 4723.83ms, mfu 0.00%\r\n","step 453: train loss 2.3082, val loss 2.3235\r\n","iter 453: loss 2.2556, time 5971.76ms, mfu 0.00%\r\n","step 454: train loss 2.3067, val loss 2.3251\r\n","iter 454: loss 2.3479, time 4782.59ms, mfu 0.00%\r\n","step 455: train loss 2.3039, val loss 2.3296\r\n","iter 455: loss 2.3269, time 4699.06ms, mfu 0.00%\r\n","step 456: train loss 2.3026, val loss 2.3341\r\n","iter 456: loss 2.3055, time 4701.43ms, mfu 0.00%\r\n","step 457: train loss 2.3069, val loss 2.3276\r\n","iter 457: loss 2.4399, time 4729.33ms, mfu 0.00%\r\n","step 458: train loss 2.3058, val loss 2.3287\r\n","iter 458: loss 2.3289, time 5008.95ms, mfu 0.00%\r\n","step 459: train loss 2.2931, val loss 2.3183\r\n","iter 459: loss 2.2937, time 4610.20ms, mfu 0.00%\r\n","step 460: train loss 2.2988, val loss 2.3206\r\n","iter 460: loss 2.2329, time 5076.75ms, mfu 0.00%\r\n","step 461: train loss 2.3169, val loss 2.3291\r\n","iter 461: loss 2.3118, time 4448.52ms, mfu 0.00%\r\n","step 462: train loss 2.3130, val loss 2.3257\r\n","iter 462: loss 2.2998, time 4720.03ms, mfu 0.00%\r\n","step 463: train loss 2.3012, val loss 2.3198\r\n","iter 463: loss 2.2926, time 4860.65ms, mfu 0.00%\r\n","step 464: train loss 2.3033, val loss 2.3207\r\n","iter 464: loss 2.3400, time 4770.13ms, mfu 0.00%\r\n","step 465: train loss 2.3055, val loss 2.3245\r\n","iter 465: loss 2.2117, time 4750.70ms, mfu 0.00%\r\n","step 466: train loss 2.3009, val loss 2.3199\r\n","iter 466: loss 2.2456, time 4811.49ms, mfu 0.00%\r\n","step 467: train loss 2.2967, val loss 2.3150\r\n","iter 467: loss 2.2435, time 5246.88ms, mfu 0.00%\r\n","step 468: train loss 2.2948, val loss 2.3170\r\n","iter 468: loss 2.3428, time 4767.26ms, mfu 0.00%\r\n","step 469: train loss 2.2911, val loss 2.3279\r\n","iter 469: loss 2.2633, time 4660.34ms, mfu 0.00%\r\n","step 470: train loss 2.2938, val loss 2.3155\r\n","iter 470: loss 2.3522, time 4787.58ms, mfu 0.00%\r\n","step 471: train loss 2.2902, val loss 2.3172\r\n","iter 471: loss 2.2197, time 4832.91ms, mfu 0.00%\r\n","step 472: train loss 2.2986, val loss 2.3307\r\n","iter 472: loss 2.2660, time 4712.19ms, mfu 0.00%\r\n","step 473: train loss 2.2955, val loss 2.3261\r\n","iter 473: loss 2.2552, time 4859.05ms, mfu 0.00%\r\n","step 474: train loss 2.3028, val loss 2.3238\r\n","iter 474: loss 2.2760, time 5156.05ms, mfu 0.00%\r\n","step 475: train loss 2.2926, val loss 2.3140\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 475: loss 2.3275, time 4964.35ms, mfu 0.00%\r\n","step 476: train loss 2.2908, val loss 2.3143\r\n","iter 476: loss 2.3525, time 4838.89ms, mfu 0.00%\r\n","step 477: train loss 2.2990, val loss 2.3126\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 477: loss 2.2786, time 4779.02ms, mfu 0.00%\r\n","step 478: train loss 2.2874, val loss 2.3154\r\n","iter 478: loss 2.3715, time 4813.85ms, mfu 0.00%\r\n","step 479: train loss 2.2894, val loss 2.3094\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 479: loss 2.2500, time 4701.49ms, mfu 0.00%\r\n","step 480: train loss 2.2922, val loss 2.3162\r\n","iter 480: loss 2.2656, time 5509.01ms, mfu 0.00%\r\n","step 481: train loss 2.3002, val loss 2.3261\r\n","iter 481: loss 2.2699, time 4577.65ms, mfu 0.00%\r\n","step 482: train loss 2.2926, val loss 2.3223\r\n","iter 482: loss 2.3339, time 4507.65ms, mfu 0.00%\r\n","step 483: train loss 2.2814, val loss 2.3118\r\n","iter 483: loss 2.3312, time 4527.35ms, mfu 0.00%\r\n","step 484: train loss 2.2890, val loss 2.3076\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 484: loss 2.2162, time 4408.76ms, mfu 0.00%\r\n","step 485: train loss 2.2885, val loss 2.3133\r\n","iter 485: loss 2.2838, time 4473.26ms, mfu 0.00%\r\n","step 486: train loss 2.2962, val loss 2.3194\r\n","iter 486: loss 2.3021, time 4447.25ms, mfu 0.00%\r\n","step 487: train loss 2.2925, val loss 2.3243\r\n","iter 487: loss 2.3696, time 4905.43ms, mfu 0.00%\r\n","step 488: train loss 2.2904, val loss 2.3129\r\n","iter 488: loss 2.2911, time 4465.57ms, mfu 0.00%\r\n","step 489: train loss 2.2861, val loss 2.3141\r\n","iter 489: loss 2.2432, time 4483.60ms, mfu 0.00%\r\n","step 490: train loss 2.2854, val loss 2.3107\r\n","iter 490: loss 2.3973, time 4476.79ms, mfu 0.00%\r\n","step 491: train loss 2.2916, val loss 2.3092\r\n","iter 491: loss 2.3054, time 4528.79ms, mfu 0.00%\r\n","step 492: train loss 2.2849, val loss 2.3161\r\n","iter 492: loss 2.3602, time 4529.24ms, mfu 0.00%\r\n","step 493: train loss 2.2860, val loss 2.3144\r\n","iter 493: loss 2.2466, time 4451.95ms, mfu 0.00%\r\n","step 494: train loss 2.2830, val loss 2.3073\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 494: loss 2.2947, time 5082.91ms, mfu 0.00%\r\n","step 495: train loss 2.2834, val loss 2.3087\r\n","iter 495: loss 2.2906, time 4546.20ms, mfu 0.00%\r\n","step 496: train loss 2.2980, val loss 2.3188\r\n","iter 496: loss 2.3272, time 4562.85ms, mfu 0.00%\r\n","step 497: train loss 2.2922, val loss 2.3237\r\n","iter 497: loss 2.2997, time 4619.62ms, mfu 0.00%\r\n","step 498: train loss 2.2814, val loss 2.3210\r\n","iter 498: loss 2.3668, time 4595.60ms, mfu 0.00%\r\n","step 499: train loss 2.2744, val loss 2.3020\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 499: loss 2.3322, time 4451.87ms, mfu 0.00%\r\n","step 500: train loss 2.2753, val loss 2.3049\r\n","iter 500: loss 2.2721, time 4502.38ms, mfu 0.00%\r\n","step 501: train loss 2.2716, val loss 2.3110\r\n","iter 501: loss 2.2011, time 5053.66ms, mfu 0.00%\r\n","step 502: train loss 2.2745, val loss 2.2976\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 502: loss 2.3015, time 4717.84ms, mfu 0.00%\r\n","step 503: train loss 2.2751, val loss 2.3044\r\n","iter 503: loss 2.3432, time 4472.07ms, mfu 0.00%\r\n","step 504: train loss 2.2834, val loss 2.2960\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 504: loss 2.2586, time 4560.74ms, mfu 0.00%\r\n","step 505: train loss 2.2765, val loss 2.2964\r\n","iter 505: loss 2.2354, time 4427.24ms, mfu 0.00%\r\n","step 506: train loss 2.2785, val loss 2.2962\r\n","iter 506: loss 2.3356, time 4542.73ms, mfu 0.00%\r\n","step 507: train loss 2.2751, val loss 2.3031\r\n","iter 507: loss 2.2679, time 4428.43ms, mfu 0.00%\r\n","step 508: train loss 2.2651, val loss 2.2933\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 508: loss 2.3091, time 4533.07ms, mfu 0.00%\r\n","step 509: train loss 2.2639, val loss 2.2930\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 509: loss 2.2133, time 5034.61ms, mfu 0.00%\r\n","step 510: train loss 2.2684, val loss 2.2975\r\n","iter 510: loss 2.3239, time 4428.85ms, mfu 0.00%\r\n","step 511: train loss 2.2720, val loss 2.3068\r\n","iter 511: loss 2.2309, time 4469.92ms, mfu 0.00%\r\n","step 512: train loss 2.2774, val loss 2.2973\r\n","iter 512: loss 2.2458, time 4399.60ms, mfu 0.00%\r\n","step 513: train loss 2.2756, val loss 2.2993\r\n","iter 513: loss 2.2370, time 4415.00ms, mfu 0.00%\r\n","step 514: train loss 2.2686, val loss 2.2893\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 514: loss 2.2492, time 4486.46ms, mfu 0.00%\r\n","step 515: train loss 2.2656, val loss 2.2887\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 515: loss 2.2224, time 4493.75ms, mfu 0.00%\r\n","step 516: train loss 2.2787, val loss 2.2969\r\n","iter 516: loss 2.2815, time 5098.73ms, mfu 0.00%\r\n","step 517: train loss 2.2880, val loss 2.3105\r\n","iter 517: loss 2.3535, time 4485.61ms, mfu 0.00%\r\n","step 518: train loss 2.2894, val loss 2.3138\r\n","iter 518: loss 2.2978, time 4431.54ms, mfu 0.00%\r\n","step 519: train loss 2.2736, val loss 2.2938\r\n","iter 519: loss 2.2730, time 4421.36ms, mfu 0.00%\r\n","step 520: train loss 2.2731, val loss 2.2856\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 520: loss 2.2654, time 4536.12ms, mfu 0.00%\r\n","step 521: train loss 2.2763, val loss 2.2902\r\n","iter 521: loss 2.3270, time 4420.17ms, mfu 0.00%\r\n","step 522: train loss 2.2695, val loss 2.2843\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 522: loss 2.2762, time 4492.06ms, mfu 0.00%\r\n","step 523: train loss 2.2662, val loss 2.2789\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 523: loss 2.2885, time 5055.75ms, mfu 0.00%\r\n","step 524: train loss 2.2627, val loss 2.2771\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 524: loss 2.2952, time 4619.42ms, mfu 0.00%\r\n","step 525: train loss 2.2567, val loss 2.2692\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 525: loss 2.2119, time 4462.37ms, mfu 0.00%\r\n","step 526: train loss 2.2547, val loss 2.2708\r\n","iter 526: loss 2.3033, time 4385.52ms, mfu 0.00%\r\n","step 527: train loss 2.2611, val loss 2.2713\r\n","iter 527: loss 2.1757, time 4628.48ms, mfu 0.00%\r\n","step 528: train loss 2.2555, val loss 2.2782\r\n","iter 528: loss 2.2575, time 4410.31ms, mfu 0.00%\r\n","step 529: train loss 2.2513, val loss 2.2692\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 529: loss 2.2195, time 4559.75ms, mfu 0.00%\r\n","step 530: train loss 2.2502, val loss 2.2722\r\n","iter 530: loss 2.2495, time 5119.15ms, mfu 0.00%\r\n","step 531: train loss 2.2531, val loss 2.2726\r\n","iter 531: loss 2.2799, time 4517.55ms, mfu 0.00%\r\n","step 532: train loss 2.2451, val loss 2.2739\r\n","iter 532: loss 2.1897, time 4604.75ms, mfu 0.00%\r\n","step 533: train loss 2.2479, val loss 2.2702\r\n","iter 533: loss 2.2419, time 4675.30ms, mfu 0.00%\r\n","step 534: train loss 2.2467, val loss 2.2640\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 534: loss 2.2761, time 4463.33ms, mfu 0.00%\r\n","step 535: train loss 2.2505, val loss 2.2736\r\n","iter 535: loss 2.2912, time 4517.00ms, mfu 0.00%\r\n","step 536: train loss 2.2470, val loss 2.2729\r\n","iter 536: loss 2.1995, time 4373.86ms, mfu 0.00%\r\n","step 537: train loss 2.2510, val loss 2.2623\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 537: loss 2.3032, time 5153.37ms, mfu 0.00%\r\n","step 538: train loss 2.2550, val loss 2.2556\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 538: loss 2.2457, time 4424.56ms, mfu 0.00%\r\n","step 539: train loss 2.2503, val loss 2.2648\r\n","iter 539: loss 2.2396, time 4385.32ms, mfu 0.00%\r\n","step 540: train loss 2.2573, val loss 2.2637\r\n","iter 540: loss 2.2108, time 4530.16ms, mfu 0.00%\r\n","step 541: train loss 2.2517, val loss 2.2610\r\n","iter 541: loss 2.2424, time 4453.32ms, mfu 0.00%\r\n","step 542: train loss 2.2513, val loss 2.2550\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 542: loss 2.2975, time 4490.30ms, mfu 0.00%\r\n","step 543: train loss 2.2484, val loss 2.2499\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 543: loss 2.2563, time 4547.61ms, mfu 0.00%\r\n","step 544: train loss 2.2419, val loss 2.2619\r\n","iter 544: loss 2.2058, time 4984.24ms, mfu 0.00%\r\n","step 545: train loss 2.2469, val loss 2.2639\r\n","iter 545: loss 2.2511, time 4483.24ms, mfu 0.00%\r\n","step 546: train loss 2.2403, val loss 2.2662\r\n","iter 546: loss 2.1871, time 4518.72ms, mfu 0.00%\r\n","step 547: train loss 2.2570, val loss 2.2633\r\n","iter 547: loss 2.1812, time 4454.15ms, mfu 0.00%\r\n","step 548: train loss 2.2523, val loss 2.2636\r\n","iter 548: loss 2.1077, time 4508.63ms, mfu 0.00%\r\n","step 549: train loss 2.2503, val loss 2.2702\r\n","iter 549: loss 2.2594, time 4398.18ms, mfu 0.00%\r\n","step 550: train loss 2.2518, val loss 2.2783\r\n","iter 550: loss 2.3102, time 4566.08ms, mfu 0.00%\r\n","step 551: train loss 2.2562, val loss 2.2953\r\n","iter 551: loss 2.3231, time 5151.90ms, mfu 0.00%\r\n","step 552: train loss 2.2618, val loss 2.2978\r\n","iter 552: loss 2.2514, time 4554.25ms, mfu 0.00%\r\n","step 553: train loss 2.2576, val loss 2.2910\r\n","iter 553: loss 2.2808, time 4650.22ms, mfu 0.00%\r\n","step 554: train loss 2.2516, val loss 2.2851\r\n","iter 554: loss 2.2214, time 4462.25ms, mfu 0.00%\r\n","step 555: train loss 2.2550, val loss 2.2875\r\n","iter 555: loss 2.2530, time 4545.62ms, mfu 0.00%\r\n","step 556: train loss 2.2529, val loss 2.2664\r\n","iter 556: loss 2.3496, time 4448.60ms, mfu 0.00%\r\n","step 557: train loss 2.2487, val loss 2.2578\r\n","iter 557: loss 2.2805, time 4480.20ms, mfu 0.00%\r\n","step 558: train loss 2.2417, val loss 2.2618\r\n","iter 558: loss 2.2810, time 4882.23ms, mfu 0.00%\r\n","step 559: train loss 2.2361, val loss 2.2545\r\n","iter 559: loss 2.2344, time 4863.98ms, mfu 0.00%\r\n","step 560: train loss 2.2344, val loss 2.2498\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 560: loss 2.2989, time 4729.93ms, mfu 0.00%\r\n","step 561: train loss 2.2361, val loss 2.2534\r\n","iter 561: loss 2.2337, time 4631.42ms, mfu 0.00%\r\n","step 562: train loss 2.2350, val loss 2.2522\r\n","iter 562: loss 2.2540, time 4460.33ms, mfu 0.00%\r\n","step 563: train loss 2.2370, val loss 2.2590\r\n","iter 563: loss 2.1140, time 4498.62ms, mfu 0.00%\r\n","step 564: train loss 2.2339, val loss 2.2649\r\n","iter 564: loss 2.2444, time 4554.36ms, mfu 0.00%\r\n","step 565: train loss 2.2343, val loss 2.2640\r\n","iter 565: loss 2.1965, time 5033.85ms, mfu 0.00%\r\n","step 566: train loss 2.2407, val loss 2.2611\r\n","iter 566: loss 2.2696, time 4643.51ms, mfu 0.00%\r\n","step 567: train loss 2.2373, val loss 2.2670\r\n","iter 567: loss 2.3078, time 4446.25ms, mfu 0.00%\r\n","step 568: train loss 2.2255, val loss 2.2582\r\n","iter 568: loss 2.2142, time 4590.21ms, mfu 0.00%\r\n","step 569: train loss 2.2345, val loss 2.2641\r\n","iter 569: loss 2.2095, time 4429.62ms, mfu 0.00%\r\n","step 570: train loss 2.2374, val loss 2.2648\r\n","iter 570: loss 2.2153, time 4412.68ms, mfu 0.00%\r\n","step 571: train loss 2.2311, val loss 2.2599\r\n","iter 571: loss 2.2023, time 4440.22ms, mfu 0.00%\r\n","step 572: train loss 2.2311, val loss 2.2559\r\n","iter 572: loss 2.2349, time 4526.02ms, mfu 0.00%\r\n","step 573: train loss 2.2350, val loss 2.2554\r\n","iter 573: loss 2.2536, time 5009.49ms, mfu 0.00%\r\n","step 574: train loss 2.2387, val loss 2.2615\r\n","iter 574: loss 2.2742, time 4465.80ms, mfu 0.00%\r\n","step 575: train loss 2.2376, val loss 2.2572\r\n","iter 575: loss 2.2913, time 4491.13ms, mfu 0.00%\r\n","step 576: train loss 2.2299, val loss 2.2492\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 576: loss 2.2952, time 4458.40ms, mfu 0.00%\r\n","step 577: train loss 2.2332, val loss 2.2472\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 577: loss 2.2852, time 4347.51ms, mfu 0.00%\r\n","step 578: train loss 2.2278, val loss 2.2509\r\n","iter 578: loss 2.1917, time 4374.43ms, mfu 0.00%\r\n","step 579: train loss 2.2343, val loss 2.2508\r\n","iter 579: loss 2.1681, time 4534.60ms, mfu 0.00%\r\n","step 580: train loss 2.2338, val loss 2.2509\r\n","iter 580: loss 2.1715, time 4999.70ms, mfu 0.00%\r\n","step 581: train loss 2.2350, val loss 2.2470\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 581: loss 2.1638, time 4541.30ms, mfu 0.00%\r\n","step 582: train loss 2.2304, val loss 2.2404\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 582: loss 2.3142, time 4456.15ms, mfu 0.00%\r\n","step 583: train loss 2.2281, val loss 2.2489\r\n","iter 583: loss 2.2318, time 4517.71ms, mfu 0.00%\r\n","step 584: train loss 2.2281, val loss 2.2614\r\n","iter 584: loss 2.3034, time 4385.43ms, mfu 0.00%\r\n","step 585: train loss 2.2302, val loss 2.2471\r\n","iter 585: loss 2.1659, time 4416.66ms, mfu 0.00%\r\n","step 586: train loss 2.2284, val loss 2.2489\r\n","iter 586: loss 2.1789, time 4628.81ms, mfu 0.00%\r\n","step 587: train loss 2.2188, val loss 2.2476\r\n","iter 587: loss 2.2138, time 4989.15ms, mfu 0.00%\r\n","step 588: train loss 2.2180, val loss 2.2440\r\n","iter 588: loss 2.2246, time 4485.16ms, mfu 0.00%\r\n","step 589: train loss 2.2129, val loss 2.2429\r\n","iter 589: loss 2.2775, time 4488.24ms, mfu 0.00%\r\n","step 590: train loss 2.2038, val loss 2.2410\r\n","iter 590: loss 2.2150, time 4435.86ms, mfu 0.00%\r\n","step 591: train loss 2.2032, val loss 2.2435\r\n","iter 591: loss 2.2495, time 4405.96ms, mfu 0.00%\r\n","step 592: train loss 2.2110, val loss 2.2461\r\n","iter 592: loss 2.1888, time 4410.72ms, mfu 0.00%\r\n","step 593: train loss 2.2124, val loss 2.2419\r\n","iter 593: loss 2.2048, time 4376.09ms, mfu 0.00%\r\n","step 594: train loss 2.2103, val loss 2.2392\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 594: loss 2.1958, time 5144.84ms, mfu 0.00%\r\n","step 595: train loss 2.2068, val loss 2.2335\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 595: loss 2.1250, time 4457.87ms, mfu 0.00%\r\n","step 596: train loss 2.2113, val loss 2.2326\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 596: loss 2.1938, time 4451.61ms, mfu 0.00%\r\n","step 597: train loss 2.2161, val loss 2.2523\r\n","iter 597: loss 2.2203, time 4489.24ms, mfu 0.00%\r\n","step 598: train loss 2.2343, val loss 2.2578\r\n","iter 598: loss 2.2476, time 4396.43ms, mfu 0.00%\r\n","step 599: train loss 2.2274, val loss 2.2531\r\n","iter 599: loss 2.2056, time 4451.44ms, mfu 0.00%\r\n","step 600: train loss 2.2213, val loss 2.2508\r\n","iter 600: loss 2.1770, time 4377.77ms, mfu 0.00%\r\n","step 601: train loss 2.2126, val loss 2.2458\r\n","iter 601: loss 2.2842, time 4929.15ms, mfu 0.00%\r\n","step 602: train loss 2.2063, val loss 2.2326\r\n","iter 602: loss 2.2435, time 4556.71ms, mfu 0.00%\r\n","step 603: train loss 2.2108, val loss 2.2291\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 603: loss 2.2184, time 4563.77ms, mfu 0.00%\r\n","step 604: train loss 2.2126, val loss 2.2302\r\n","iter 604: loss 2.2370, time 4591.00ms, mfu 0.00%\r\n","step 605: train loss 2.2222, val loss 2.2391\r\n","iter 605: loss 2.2211, time 4447.61ms, mfu 0.00%\r\n","step 606: train loss 2.2230, val loss 2.2349\r\n","iter 606: loss 2.2350, time 4474.94ms, mfu 0.00%\r\n","step 607: train loss 2.2091, val loss 2.2353\r\n","iter 607: loss 2.3092, time 4495.18ms, mfu 0.00%\r\n","step 608: train loss 2.2107, val loss 2.2344\r\n","iter 608: loss 2.2736, time 4794.68ms, mfu 0.00%\r\n","step 609: train loss 2.2083, val loss 2.2257\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 609: loss 2.1953, time 4789.33ms, mfu 0.00%\r\n","step 610: train loss 2.2012, val loss 2.2236\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 610: loss 2.0856, time 4595.90ms, mfu 0.00%\r\n","step 611: train loss 2.1994, val loss 2.2302\r\n","iter 611: loss 2.3039, time 4454.83ms, mfu 0.00%\r\n","step 612: train loss 2.2076, val loss 2.2251\r\n","iter 612: loss 2.1544, time 4564.70ms, mfu 0.00%\r\n","step 613: train loss 2.1970, val loss 2.2265\r\n","iter 613: loss 2.2011, time 4388.94ms, mfu 0.00%\r\n","step 614: train loss 2.1899, val loss 2.2218\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 614: loss 2.1545, time 4543.92ms, mfu 0.00%\r\n","step 615: train loss 2.1975, val loss 2.2181\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 615: loss 2.1129, time 4772.76ms, mfu 0.00%\r\n","step 616: train loss 2.1962, val loss 2.2229\r\n","iter 616: loss 2.1599, time 4836.76ms, mfu 0.00%\r\n","step 617: train loss 2.2000, val loss 2.2203\r\n","iter 617: loss 2.2010, time 4443.52ms, mfu 0.00%\r\n","step 618: train loss 2.2091, val loss 2.2407\r\n","iter 618: loss 2.2072, time 4411.52ms, mfu 0.00%\r\n","step 619: train loss 2.2072, val loss 2.2317\r\n","iter 619: loss 2.2426, time 4341.68ms, mfu 0.00%\r\n","step 620: train loss 2.2118, val loss 2.2347\r\n","iter 620: loss 2.2218, time 4387.54ms, mfu 0.00%\r\n","step 621: train loss 2.2063, val loss 2.2302\r\n","iter 621: loss 2.1646, time 4482.63ms, mfu 0.00%\r\n","step 622: train loss 2.2042, val loss 2.2323\r\n","iter 622: loss 2.1916, time 4655.04ms, mfu 0.00%\r\n","step 623: train loss 2.2092, val loss 2.2376\r\n","iter 623: loss 2.1892, time 5303.12ms, mfu 0.00%\r\n","step 624: train loss 2.2092, val loss 2.2357\r\n","iter 624: loss 2.2283, time 4465.34ms, mfu 0.00%\r\n","step 625: train loss 2.2061, val loss 2.2349\r\n","iter 625: loss 2.1947, time 4582.53ms, mfu 0.00%\r\n","step 626: train loss 2.2053, val loss 2.2298\r\n","iter 626: loss 2.1745, time 4483.28ms, mfu 0.00%\r\n","step 627: train loss 2.2022, val loss 2.2253\r\n","iter 627: loss 2.1805, time 4362.10ms, mfu 0.00%\r\n","step 628: train loss 2.1948, val loss 2.2196\r\n","iter 628: loss 2.2311, time 4458.44ms, mfu 0.00%\r\n","step 629: train loss 2.1940, val loss 2.2075\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 629: loss 2.1220, time 4433.15ms, mfu 0.00%\r\n","step 630: train loss 2.1884, val loss 2.2148\r\n","iter 630: loss 2.1638, time 5009.18ms, mfu 0.00%\r\n","step 631: train loss 2.1997, val loss 2.2315\r\n","iter 631: loss 2.3158, time 4545.34ms, mfu 0.00%\r\n","step 632: train loss 2.2010, val loss 2.2229\r\n","iter 632: loss 2.3169, time 4742.13ms, mfu 0.00%\r\n","step 633: train loss 2.2062, val loss 2.2329\r\n","iter 633: loss 2.1821, time 4449.03ms, mfu 0.00%\r\n","step 634: train loss 2.1964, val loss 2.2308\r\n","iter 634: loss 2.1438, time 4359.87ms, mfu 0.00%\r\n","step 635: train loss 2.1882, val loss 2.2193\r\n","iter 635: loss 2.1887, time 4492.89ms, mfu 0.00%\r\n","step 636: train loss 2.2008, val loss 2.2238\r\n","iter 636: loss 2.1599, time 4456.09ms, mfu 0.00%\r\n","step 637: train loss 2.1869, val loss 2.2226\r\n","iter 637: loss 2.1680, time 4921.06ms, mfu 0.00%\r\n","step 638: train loss 2.1882, val loss 2.2098\r\n","iter 638: loss 2.1407, time 4472.35ms, mfu 0.00%\r\n","step 639: train loss 2.1936, val loss 2.2155\r\n","iter 639: loss 2.2015, time 4431.93ms, mfu 0.00%\r\n","step 640: train loss 2.1930, val loss 2.2209\r\n","iter 640: loss 2.2646, time 4447.97ms, mfu 0.00%\r\n","step 641: train loss 2.1925, val loss 2.2197\r\n","iter 641: loss 2.2230, time 4393.76ms, mfu 0.00%\r\n","step 642: train loss 2.1892, val loss 2.2147\r\n","iter 642: loss 2.0892, time 4437.28ms, mfu 0.00%\r\n","step 643: train loss 2.1905, val loss 2.2208\r\n","iter 643: loss 2.2020, time 4507.54ms, mfu 0.00%\r\n","step 644: train loss 2.2006, val loss 2.2238\r\n","iter 644: loss 2.1508, time 5265.91ms, mfu 0.00%\r\n","step 645: train loss 2.1898, val loss 2.2227\r\n","iter 645: loss 2.2073, time 4475.19ms, mfu 0.00%\r\n","step 646: train loss 2.1910, val loss 2.2295\r\n","iter 646: loss 2.1478, time 4488.13ms, mfu 0.00%\r\n","step 647: train loss 2.1950, val loss 2.2235\r\n","iter 647: loss 2.2142, time 4495.84ms, mfu 0.00%\r\n","step 648: train loss 2.1840, val loss 2.2254\r\n","iter 648: loss 2.2417, time 4491.95ms, mfu 0.00%\r\n","step 649: train loss 2.1912, val loss 2.2210\r\n","iter 649: loss 2.1753, time 4495.51ms, mfu 0.00%\r\n","step 650: train loss 2.1769, val loss 2.2151\r\n","iter 650: loss 2.1638, time 4404.90ms, mfu 0.00%\r\n","step 651: train loss 2.1899, val loss 2.2096\r\n","iter 651: loss 2.0979, time 5284.99ms, mfu 0.00%\r\n","step 652: train loss 2.1837, val loss 2.2223\r\n","iter 652: loss 2.1483, time 4389.29ms, mfu 0.00%\r\n","step 653: train loss 2.1840, val loss 2.2067\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 653: loss 2.2112, time 4595.28ms, mfu 0.00%\r\n","step 654: train loss 2.1838, val loss 2.2230\r\n","iter 654: loss 2.2123, time 4393.90ms, mfu 0.00%\r\n","step 655: train loss 2.1939, val loss 2.2260\r\n","iter 655: loss 2.1328, time 4466.13ms, mfu 0.00%\r\n","step 656: train loss 2.1874, val loss 2.2152\r\n","iter 656: loss 2.2429, time 4605.65ms, mfu 0.00%\r\n","step 657: train loss 2.1875, val loss 2.2199\r\n","iter 657: loss 2.1696, time 4529.25ms, mfu 0.00%\r\n","step 658: train loss 2.1870, val loss 2.2123\r\n","iter 658: loss 2.0956, time 5329.47ms, mfu 0.00%\r\n","step 659: train loss 2.1854, val loss 2.2142\r\n","iter 659: loss 2.1928, time 4391.90ms, mfu 0.00%\r\n","step 660: train loss 2.1819, val loss 2.2092\r\n","iter 660: loss 2.2012, time 4483.82ms, mfu 0.00%\r\n","step 661: train loss 2.1843, val loss 2.2046\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 661: loss 2.2108, time 4458.89ms, mfu 0.00%\r\n","step 662: train loss 2.1743, val loss 2.2016\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 662: loss 2.1284, time 4516.55ms, mfu 0.00%\r\n","step 663: train loss 2.1803, val loss 2.2124\r\n","iter 663: loss 2.2608, time 4330.86ms, mfu 0.00%\r\n","step 664: train loss 2.1862, val loss 2.2211\r\n","iter 664: loss 2.1086, time 4481.67ms, mfu 0.00%\r\n","step 665: train loss 2.1911, val loss 2.2202\r\n","iter 665: loss 2.1790, time 5047.54ms, mfu 0.00%\r\n","step 666: train loss 2.1858, val loss 2.2178\r\n","iter 666: loss 2.1794, time 4393.04ms, mfu 0.00%\r\n","step 667: train loss 2.1853, val loss 2.2130\r\n","iter 667: loss 2.1607, time 4443.60ms, mfu 0.00%\r\n","step 668: train loss 2.1775, val loss 2.2080\r\n","iter 668: loss 2.1338, time 4443.91ms, mfu 0.00%\r\n","step 669: train loss 2.1769, val loss 2.2096\r\n","iter 669: loss 2.1753, time 4492.06ms, mfu 0.00%\r\n","step 670: train loss 2.1752, val loss 2.2103\r\n","iter 670: loss 2.2320, time 4367.29ms, mfu 0.00%\r\n","step 671: train loss 2.1880, val loss 2.2160\r\n","iter 671: loss 2.1406, time 4405.24ms, mfu 0.00%\r\n","step 672: train loss 2.1874, val loss 2.2183\r\n","iter 672: loss 2.2091, time 4656.10ms, mfu 0.00%\r\n","step 673: train loss 2.1780, val loss 2.2033\r\n","iter 673: loss 2.1056, time 4836.61ms, mfu 0.00%\r\n","step 674: train loss 2.1732, val loss 2.1996\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 674: loss 2.0813, time 4461.31ms, mfu 0.00%\r\n","step 675: train loss 2.1868, val loss 2.2141\r\n","iter 675: loss 2.1569, time 4558.36ms, mfu 0.00%\r\n","step 676: train loss 2.1869, val loss 2.2136\r\n","iter 676: loss 2.1970, time 4397.96ms, mfu 0.00%\r\n","step 677: train loss 2.1754, val loss 2.2008\r\n","iter 677: loss 2.2146, time 4452.77ms, mfu 0.00%\r\n","step 678: train loss 2.1602, val loss 2.2004\r\n","iter 678: loss 2.1791, time 4418.35ms, mfu 0.00%\r\n","step 679: train loss 2.1623, val loss 2.1928\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 679: loss 2.1656, time 4601.87ms, mfu 0.00%\r\n","step 680: train loss 2.1621, val loss 2.1973\r\n","iter 680: loss 2.1875, time 5091.13ms, mfu 0.00%\r\n","step 681: train loss 2.1525, val loss 2.1869\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 681: loss 2.0889, time 4399.67ms, mfu 0.00%\r\n","step 682: train loss 2.1546, val loss 2.1871\r\n","iter 682: loss 2.2371, time 4406.46ms, mfu 0.00%\r\n","step 683: train loss 2.1605, val loss 2.1876\r\n","iter 683: loss 2.2027, time 4323.40ms, mfu 0.00%\r\n","step 684: train loss 2.1652, val loss 2.1871\r\n","iter 684: loss 2.1713, time 4368.12ms, mfu 0.00%\r\n","step 685: train loss 2.1547, val loss 2.1867\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 685: loss 2.1622, time 4484.35ms, mfu 0.00%\r\n","step 686: train loss 2.1589, val loss 2.1773\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 686: loss 2.2267, time 4408.93ms, mfu 0.00%\r\n","step 687: train loss 2.1542, val loss 2.1824\r\n","iter 687: loss 2.1380, time 5059.93ms, mfu 0.00%\r\n","step 688: train loss 2.1500, val loss 2.1850\r\n","iter 688: loss 2.1716, time 4386.88ms, mfu 0.00%\r\n","step 689: train loss 2.1605, val loss 2.1874\r\n","iter 689: loss 2.1498, time 4401.14ms, mfu 0.00%\r\n","step 690: train loss 2.1489, val loss 2.1851\r\n","iter 690: loss 2.1876, time 4487.22ms, mfu 0.00%\r\n","step 691: train loss 2.1497, val loss 2.1805\r\n","iter 691: loss 2.1804, time 4392.68ms, mfu 0.00%\r\n","step 692: train loss 2.1552, val loss 2.1770\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 692: loss 2.1397, time 4439.65ms, mfu 0.00%\r\n","step 693: train loss 2.1497, val loss 2.1866\r\n","iter 693: loss 2.1285, time 4628.25ms, mfu 0.00%\r\n","step 694: train loss 2.1483, val loss 2.1841\r\n","iter 694: loss 2.1439, time 5228.24ms, mfu 0.00%\r\n","step 695: train loss 2.1540, val loss 2.1760\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 695: loss 2.1979, time 4584.05ms, mfu 0.00%\r\n","step 696: train loss 2.1522, val loss 2.1891\r\n","iter 696: loss 2.1947, time 4362.71ms, mfu 0.00%\r\n","step 697: train loss 2.1494, val loss 2.1906\r\n","iter 697: loss 2.1597, time 4538.24ms, mfu 0.00%\r\n","step 698: train loss 2.1466, val loss 2.1829\r\n","iter 698: loss 2.1030, time 4404.22ms, mfu 0.00%\r\n","step 699: train loss 2.1484, val loss 2.1821\r\n","iter 699: loss 2.1053, time 4381.07ms, mfu 0.00%\r\n","step 700: train loss 2.1450, val loss 2.1866\r\n","iter 700: loss 2.1081, time 4524.54ms, mfu 0.00%\r\n","step 701: train loss 2.1480, val loss 2.1900\r\n","iter 701: loss 2.2523, time 5020.13ms, mfu 0.00%\r\n","step 702: train loss 2.1465, val loss 2.1923\r\n","iter 702: loss 2.1149, time 4524.17ms, mfu 0.00%\r\n","step 703: train loss 2.1516, val loss 2.1901\r\n","iter 703: loss 2.1716, time 4412.79ms, mfu 0.00%\r\n","step 704: train loss 2.1469, val loss 2.1781\r\n","iter 704: loss 2.1186, time 4417.30ms, mfu 0.00%\r\n","step 705: train loss 2.1469, val loss 2.1767\r\n","iter 705: loss 2.2034, time 4386.82ms, mfu 0.00%\r\n","step 706: train loss 2.1427, val loss 2.1823\r\n","iter 706: loss 2.2478, time 4390.78ms, mfu 0.00%\r\n","step 707: train loss 2.1494, val loss 2.1792\r\n","iter 707: loss 2.1523, time 4405.92ms, mfu 0.00%\r\n","step 708: train loss 2.1479, val loss 2.1872\r\n","iter 708: loss 2.1508, time 4937.45ms, mfu 0.00%\r\n","step 709: train loss 2.1503, val loss 2.1800\r\n","iter 709: loss 2.2091, time 4564.13ms, mfu 0.00%\r\n","step 710: train loss 2.1429, val loss 2.1703\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 710: loss 2.1013, time 4421.91ms, mfu 0.00%\r\n","step 711: train loss 2.1439, val loss 2.1788\r\n","iter 711: loss 2.1795, time 4704.52ms, mfu 0.00%\r\n","step 712: train loss 2.1420, val loss 2.1800\r\n","iter 712: loss 2.1378, time 4366.06ms, mfu 0.00%\r\n","step 713: train loss 2.1493, val loss 2.1801\r\n","iter 713: loss 2.2298, time 4543.03ms, mfu 0.00%\r\n","step 714: train loss 2.1549, val loss 2.1870\r\n","iter 714: loss 2.2007, time 4472.42ms, mfu 0.00%\r\n","step 715: train loss 2.1488, val loss 2.1847\r\n","iter 715: loss 2.1690, time 4737.11ms, mfu 0.00%\r\n","step 716: train loss 2.1505, val loss 2.1855\r\n","iter 716: loss 2.1363, time 4756.62ms, mfu 0.00%\r\n","step 717: train loss 2.1479, val loss 2.1867\r\n","iter 717: loss 2.1595, time 4387.73ms, mfu 0.00%\r\n","step 718: train loss 2.1467, val loss 2.1859\r\n","iter 718: loss 2.1038, time 4510.16ms, mfu 0.00%\r\n","step 719: train loss 2.1459, val loss 2.1800\r\n","iter 719: loss 2.2101, time 4450.76ms, mfu 0.00%\r\n","step 720: train loss 2.1455, val loss 2.1827\r\n","iter 720: loss 2.1863, time 4425.42ms, mfu 0.00%\r\n","step 721: train loss 2.1411, val loss 2.1871\r\n","iter 721: loss 2.1399, time 4539.36ms, mfu 0.00%\r\n","step 722: train loss 2.1397, val loss 2.1920\r\n","iter 722: loss 2.1954, time 4465.44ms, mfu 0.00%\r\n","step 723: train loss 2.1437, val loss 2.1841\r\n","iter 723: loss 2.0429, time 5263.62ms, mfu 0.00%\r\n","step 724: train loss 2.1366, val loss 2.1895\r\n","iter 724: loss 2.1546, time 4457.46ms, mfu 0.00%\r\n","step 725: train loss 2.1369, val loss 2.1834\r\n","iter 725: loss 2.1591, time 4401.59ms, mfu 0.00%\r\n","step 726: train loss 2.1331, val loss 2.1919\r\n","iter 726: loss 2.1597, time 4504.26ms, mfu 0.00%\r\n","step 727: train loss 2.1441, val loss 2.1929\r\n","iter 727: loss 2.1585, time 4356.87ms, mfu 0.00%\r\n","step 728: train loss 2.1458, val loss 2.2011\r\n","iter 728: loss 2.0433, time 4645.49ms, mfu 0.00%\r\n","step 729: train loss 2.1408, val loss 2.1946\r\n","iter 729: loss 2.1249, time 4447.45ms, mfu 0.00%\r\n","step 730: train loss 2.1352, val loss 2.1776\r\n","iter 730: loss 2.1219, time 5238.81ms, mfu 0.00%\r\n","step 731: train loss 2.1289, val loss 2.1774\r\n","iter 731: loss 2.1259, time 4490.23ms, mfu 0.00%\r\n","step 732: train loss 2.1396, val loss 2.1810\r\n","iter 732: loss 2.1358, time 4407.76ms, mfu 0.00%\r\n","step 733: train loss 2.1425, val loss 2.1884\r\n","iter 733: loss 2.0998, time 4454.85ms, mfu 0.00%\r\n","step 734: train loss 2.1455, val loss 2.1925\r\n","iter 734: loss 2.1009, time 4517.15ms, mfu 0.00%\r\n","step 735: train loss 2.1388, val loss 2.1882\r\n","iter 735: loss 2.1451, time 4594.84ms, mfu 0.00%\r\n","step 736: train loss 2.1276, val loss 2.1861\r\n","iter 736: loss 2.1530, time 4519.70ms, mfu 0.00%\r\n","step 737: train loss 2.1390, val loss 2.1814\r\n","iter 737: loss 2.1498, time 5248.40ms, mfu 0.00%\r\n","step 738: train loss 2.1393, val loss 2.1867\r\n","iter 738: loss 2.1847, time 4542.18ms, mfu 0.00%\r\n","step 739: train loss 2.1463, val loss 2.1824\r\n","iter 739: loss 2.1167, time 4546.76ms, mfu 0.00%\r\n","step 740: train loss 2.1393, val loss 2.1839\r\n","iter 740: loss 2.1798, time 4458.43ms, mfu 0.00%\r\n","step 741: train loss 2.1345, val loss 2.1791\r\n","iter 741: loss 2.1929, time 4494.68ms, mfu 0.00%\r\n","step 742: train loss 2.1367, val loss 2.1764\r\n","iter 742: loss 2.1251, time 4503.09ms, mfu 0.00%\r\n","step 743: train loss 2.1270, val loss 2.1693\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 743: loss 2.2128, time 4463.11ms, mfu 0.00%\r\n","step 744: train loss 2.1426, val loss 2.1691\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 744: loss 2.0833, time 5216.05ms, mfu 0.00%\r\n","step 745: train loss 2.1315, val loss 2.1680\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 745: loss 2.1664, time 4510.78ms, mfu 0.00%\r\n","step 746: train loss 2.1396, val loss 2.1677\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 746: loss 2.1099, time 4687.38ms, mfu 0.00%\r\n","step 747: train loss 2.1423, val loss 2.1681\r\n","iter 747: loss 2.1163, time 4665.11ms, mfu 0.00%\r\n","step 748: train loss 2.1347, val loss 2.1679\r\n","iter 748: loss 2.1149, time 4681.30ms, mfu 0.00%\r\n","step 749: train loss 2.1361, val loss 2.1675\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 749: loss 2.1313, time 4600.43ms, mfu 0.00%\r\n","step 750: train loss 2.1282, val loss 2.1651\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 750: loss 2.1991, time 4582.99ms, mfu 0.00%\r\n","step 751: train loss 2.1211, val loss 2.1594\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 751: loss 2.1508, time 5232.23ms, mfu 0.00%\r\n","step 752: train loss 2.1214, val loss 2.1501\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 752: loss 2.1293, time 4631.23ms, mfu 0.00%\r\n","step 753: train loss 2.1212, val loss 2.1622\r\n","iter 753: loss 2.1745, time 4406.31ms, mfu 0.00%\r\n","step 754: train loss 2.1280, val loss 2.1539\r\n","iter 754: loss 2.1321, time 4554.32ms, mfu 0.00%\r\n","step 755: train loss 2.1258, val loss 2.1565\r\n","iter 755: loss 2.1279, time 4446.00ms, mfu 0.00%\r\n","step 756: train loss 2.1275, val loss 2.1649\r\n","iter 756: loss 2.0958, time 4584.23ms, mfu 0.00%\r\n","step 757: train loss 2.1183, val loss 2.1595\r\n","iter 757: loss 2.0716, time 4528.47ms, mfu 0.00%\r\n","step 758: train loss 2.1136, val loss 2.1617\r\n","iter 758: loss 2.0949, time 5155.49ms, mfu 0.00%\r\n","step 759: train loss 2.1182, val loss 2.1499\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 759: loss 2.0990, time 4558.17ms, mfu 0.00%\r\n","step 760: train loss 2.1164, val loss 2.1565\r\n","iter 760: loss 2.1218, time 4528.54ms, mfu 0.00%\r\n","step 761: train loss 2.1121, val loss 2.1596\r\n","iter 761: loss 2.1388, time 4547.60ms, mfu 0.00%\r\n","step 762: train loss 2.1139, val loss 2.1537\r\n","iter 762: loss 2.1303, time 4521.39ms, mfu 0.00%\r\n","step 763: train loss 2.1127, val loss 2.1563\r\n","iter 763: loss 2.1257, time 4762.00ms, mfu 0.00%\r\n","step 764: train loss 2.1043, val loss 2.1500\r\n","iter 764: loss 2.1237, time 4591.89ms, mfu 0.00%\r\n","step 765: train loss 2.1029, val loss 2.1440\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 765: loss 2.0368, time 5353.39ms, mfu 0.00%\r\n","step 766: train loss 2.1007, val loss 2.1512\r\n","iter 766: loss 2.0644, time 4564.24ms, mfu 0.00%\r\n","step 767: train loss 2.1020, val loss 2.1496\r\n","iter 767: loss 2.0920, time 4585.26ms, mfu 0.00%\r\n","step 768: train loss 2.0974, val loss 2.1542\r\n","iter 768: loss 2.0428, time 4513.46ms, mfu 0.00%\r\n","step 769: train loss 2.1007, val loss 2.1509\r\n","iter 769: loss 2.0530, time 4668.79ms, mfu 0.00%\r\n","step 770: train loss 2.1029, val loss 2.1521\r\n","iter 770: loss 2.2021, time 4537.55ms, mfu 0.00%\r\n","step 771: train loss 2.0980, val loss 2.1449\r\n","iter 771: loss 2.1070, time 4596.98ms, mfu 0.00%\r\n","step 772: train loss 2.1075, val loss 2.1563\r\n","iter 772: loss 2.0007, time 5115.93ms, mfu 0.00%\r\n","step 773: train loss 2.1097, val loss 2.1557\r\n","iter 773: loss 2.0262, time 4540.69ms, mfu 0.00%\r\n","step 774: train loss 2.1116, val loss 2.1512\r\n","iter 774: loss 2.0819, time 4515.24ms, mfu 0.00%\r\n","step 775: train loss 2.1039, val loss 2.1500\r\n","iter 775: loss 2.0948, time 4498.38ms, mfu 0.00%\r\n","step 776: train loss 2.1070, val loss 2.1437\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 776: loss 2.1561, time 4545.62ms, mfu 0.00%\r\n","step 777: train loss 2.1083, val loss 2.1458\r\n","iter 777: loss 2.0689, time 4479.27ms, mfu 0.00%\r\n","step 778: train loss 2.1110, val loss 2.1599\r\n","iter 778: loss 2.0382, time 4432.27ms, mfu 0.00%\r\n","step 779: train loss 2.1089, val loss 2.1582\r\n","iter 779: loss 2.0807, time 5253.42ms, mfu 0.00%\r\n","step 780: train loss 2.1014, val loss 2.1539\r\n","iter 780: loss 2.1438, time 4499.91ms, mfu 0.00%\r\n","step 781: train loss 2.1073, val loss 2.1481\r\n","iter 781: loss 2.0689, time 4573.22ms, mfu 0.00%\r\n","step 782: train loss 2.1065, val loss 2.1404\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 782: loss 2.0511, time 4565.42ms, mfu 0.00%\r\n","step 783: train loss 2.0992, val loss 2.1432\r\n","iter 783: loss 2.1670, time 4448.26ms, mfu 0.00%\r\n","step 784: train loss 2.1070, val loss 2.1468\r\n","iter 784: loss 2.1800, time 4490.21ms, mfu 0.00%\r\n","step 785: train loss 2.1021, val loss 2.1545\r\n","iter 785: loss 2.0298, time 4371.54ms, mfu 0.00%\r\n","step 786: train loss 2.1066, val loss 2.1579\r\n","iter 786: loss 2.1082, time 5145.31ms, mfu 0.00%\r\n","step 787: train loss 2.1019, val loss 2.1542\r\n","iter 787: loss 2.0711, time 4427.30ms, mfu 0.00%\r\n","step 788: train loss 2.1024, val loss 2.1563\r\n","iter 788: loss 2.0667, time 4379.57ms, mfu 0.00%\r\n","step 789: train loss 2.1019, val loss 2.1637\r\n","iter 789: loss 2.0823, time 4545.16ms, mfu 0.00%\r\n","step 790: train loss 2.1059, val loss 2.1515\r\n","iter 790: loss 2.1256, time 4403.91ms, mfu 0.00%\r\n","step 791: train loss 2.0976, val loss 2.1394\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 791: loss 2.1635, time 4449.49ms, mfu 0.00%\r\n","step 792: train loss 2.0902, val loss 2.1389\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 792: loss 2.1443, time 4517.79ms, mfu 0.00%\r\n","step 793: train loss 2.0990, val loss 2.1409\r\n","iter 793: loss 2.1478, time 5083.97ms, mfu 0.00%\r\n","step 794: train loss 2.0984, val loss 2.1401\r\n","iter 794: loss 2.0779, time 4374.81ms, mfu 0.00%\r\n","step 795: train loss 2.0879, val loss 2.1418\r\n","iter 795: loss 2.0506, time 4525.28ms, mfu 0.00%\r\n","step 796: train loss 2.0923, val loss 2.1436\r\n","iter 796: loss 2.0986, time 4462.12ms, mfu 0.00%\r\n","step 797: train loss 2.1005, val loss 2.1507\r\n","iter 797: loss 2.0491, time 4568.97ms, mfu 0.00%\r\n","step 798: train loss 2.1036, val loss 2.1505\r\n","iter 798: loss 2.1586, time 4416.29ms, mfu 0.00%\r\n","step 799: train loss 2.1082, val loss 2.1571\r\n","iter 799: loss 2.2401, time 4620.96ms, mfu 0.00%\r\n","step 800: train loss 2.1049, val loss 2.1571\r\n","iter 800: loss 2.1353, time 5348.97ms, mfu 0.00%\r\n","step 801: train loss 2.1010, val loss 2.1555\r\n","iter 801: loss 2.0832, time 4533.51ms, mfu 0.00%\r\n","step 802: train loss 2.1033, val loss 2.1532\r\n","iter 802: loss 2.0485, time 4576.89ms, mfu 0.00%\r\n","step 803: train loss 2.1102, val loss 2.1538\r\n","iter 803: loss 2.1220, time 4510.12ms, mfu 0.00%\r\n","step 804: train loss 2.1056, val loss 2.1566\r\n","iter 804: loss 2.1112, time 4497.02ms, mfu 0.00%\r\n","step 805: train loss 2.1052, val loss 2.1601\r\n","iter 805: loss 2.1982, time 4533.09ms, mfu 0.00%\r\n","step 806: train loss 2.1132, val loss 2.1518\r\n","iter 806: loss 2.0784, time 4541.29ms, mfu 0.00%\r\n","step 807: train loss 2.1108, val loss 2.1540\r\n","iter 807: loss 2.0850, time 5066.88ms, mfu 0.00%\r\n","step 808: train loss 2.1111, val loss 2.1491\r\n","iter 808: loss 2.2238, time 4500.50ms, mfu 0.00%\r\n","step 809: train loss 2.1090, val loss 2.1500\r\n","iter 809: loss 2.1454, time 4436.62ms, mfu 0.00%\r\n","step 810: train loss 2.1063, val loss 2.1485\r\n","iter 810: loss 2.1295, time 4510.58ms, mfu 0.00%\r\n","step 811: train loss 2.1121, val loss 2.1448\r\n","iter 811: loss 2.1318, time 4458.01ms, mfu 0.00%\r\n","step 812: train loss 2.0989, val loss 2.1461\r\n","iter 812: loss 2.0872, time 4461.04ms, mfu 0.00%\r\n","step 813: train loss 2.1016, val loss 2.1365\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 813: loss 2.0420, time 4576.05ms, mfu 0.00%\r\n","step 814: train loss 2.0874, val loss 2.1349\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 814: loss 2.1005, time 5064.93ms, mfu 0.00%\r\n","step 815: train loss 2.0836, val loss 2.1342\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 815: loss 2.0496, time 4595.68ms, mfu 0.00%\r\n","step 816: train loss 2.0932, val loss 2.1354\r\n","iter 816: loss 2.0404, time 4505.35ms, mfu 0.00%\r\n","step 817: train loss 2.0983, val loss 2.1407\r\n","iter 817: loss 2.1074, time 4438.20ms, mfu 0.00%\r\n","step 818: train loss 2.0870, val loss 2.1454\r\n","iter 818: loss 2.1597, time 4450.63ms, mfu 0.00%\r\n","step 819: train loss 2.0873, val loss 2.1381\r\n","iter 819: loss 1.9890, time 4386.10ms, mfu 0.00%\r\n","step 820: train loss 2.0858, val loss 2.1337\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 820: loss 2.1036, time 4446.13ms, mfu 0.00%\r\n","step 821: train loss 2.0882, val loss 2.1333\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 821: loss 2.0430, time 5003.06ms, mfu 0.00%\r\n","step 822: train loss 2.0859, val loss 2.1428\r\n","iter 822: loss 2.1245, time 4727.78ms, mfu 0.00%\r\n","step 823: train loss 2.0790, val loss 2.1386\r\n","iter 823: loss 2.0990, time 4581.89ms, mfu 0.00%\r\n","step 824: train loss 2.0811, val loss 2.1271\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 824: loss 2.0456, time 4605.11ms, mfu 0.00%\r\n","step 825: train loss 2.0916, val loss 2.1356\r\n","iter 825: loss 2.0880, time 4491.18ms, mfu 0.00%\r\n","step 826: train loss 2.0926, val loss 2.1411\r\n","iter 826: loss 2.0803, time 4506.00ms, mfu 0.00%\r\n","step 827: train loss 2.0990, val loss 2.1376\r\n","iter 827: loss 2.1318, time 4483.57ms, mfu 0.00%\r\n","step 828: train loss 2.0928, val loss 2.1424\r\n","iter 828: loss 2.1021, time 4931.70ms, mfu 0.00%\r\n","step 829: train loss 2.0845, val loss 2.1401\r\n","iter 829: loss 2.1250, time 4789.26ms, mfu 0.00%\r\n","step 830: train loss 2.0922, val loss 2.1363\r\n","iter 830: loss 2.0993, time 4516.16ms, mfu 0.00%\r\n","step 831: train loss 2.0865, val loss 2.1390\r\n","iter 831: loss 2.1580, time 4590.86ms, mfu 0.00%\r\n","step 832: train loss 2.0936, val loss 2.1268\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 832: loss 2.1201, time 4572.77ms, mfu 0.00%\r\n","step 833: train loss 2.0847, val loss 2.1366\r\n","iter 833: loss 2.0726, time 4461.24ms, mfu 0.00%\r\n","step 834: train loss 2.0782, val loss 2.1285\r\n","iter 834: loss 2.1019, time 4732.15ms, mfu 0.00%\r\n","step 835: train loss 2.0776, val loss 2.1193\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 835: loss 2.1158, time 4779.49ms, mfu 0.00%\r\n","step 836: train loss 2.0758, val loss 2.1149\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 836: loss 2.0227, time 4867.85ms, mfu 0.00%\r\n","step 837: train loss 2.0718, val loss 2.1175\r\n","iter 837: loss 2.0994, time 4499.17ms, mfu 0.00%\r\n","step 838: train loss 2.0687, val loss 2.1195\r\n","iter 838: loss 2.0679, time 4602.95ms, mfu 0.00%\r\n","step 839: train loss 2.0751, val loss 2.1177\r\n","iter 839: loss 2.0865, time 4523.43ms, mfu 0.00%\r\n","step 840: train loss 2.0744, val loss 2.1267\r\n","iter 840: loss 1.9920, time 4411.55ms, mfu 0.00%\r\n","step 841: train loss 2.0744, val loss 2.1189\r\n","iter 841: loss 1.9827, time 4701.85ms, mfu 0.00%\r\n","step 842: train loss 2.0787, val loss 2.1269\r\n","iter 842: loss 1.9937, time 4728.82ms, mfu 0.00%\r\n","step 843: train loss 2.0714, val loss 2.1229\r\n","iter 843: loss 2.1075, time 5007.88ms, mfu 0.00%\r\n","step 844: train loss 2.0741, val loss 2.1287\r\n","iter 844: loss 1.9556, time 4522.86ms, mfu 0.00%\r\n","step 845: train loss 2.0813, val loss 2.1330\r\n","iter 845: loss 2.0375, time 4578.18ms, mfu 0.00%\r\n","step 846: train loss 2.0802, val loss 2.1383\r\n","iter 846: loss 2.1290, time 4547.30ms, mfu 0.00%\r\n","step 847: train loss 2.0875, val loss 2.1452\r\n","iter 847: loss 2.1353, time 4631.69ms, mfu 0.00%\r\n","step 848: train loss 2.0778, val loss 2.1443\r\n","iter 848: loss 2.0005, time 4545.34ms, mfu 0.00%\r\n","step 849: train loss 2.0858, val loss 2.1374\r\n","iter 849: loss 2.0232, time 4778.02ms, mfu 0.00%\r\n","step 850: train loss 2.0758, val loss 2.1292\r\n","iter 850: loss 1.9814, time 4883.69ms, mfu 0.00%\r\n","step 851: train loss 2.0676, val loss 2.1206\r\n","iter 851: loss 2.0949, time 4701.31ms, mfu 0.00%\r\n","step 852: train loss 2.0698, val loss 2.1201\r\n","iter 852: loss 2.0776, time 4495.98ms, mfu 0.00%\r\n","step 853: train loss 2.0686, val loss 2.1293\r\n","iter 853: loss 2.1009, time 4442.36ms, mfu 0.00%\r\n","step 854: train loss 2.0700, val loss 2.1216\r\n","iter 854: loss 2.0583, time 4606.53ms, mfu 0.00%\r\n","step 855: train loss 2.0713, val loss 2.1226\r\n","iter 855: loss 2.1147, time 4419.53ms, mfu 0.00%\r\n","step 856: train loss 2.0653, val loss 2.1169\r\n","iter 856: loss 1.9153, time 4762.88ms, mfu 0.00%\r\n","step 857: train loss 2.0686, val loss 2.1182\r\n","iter 857: loss 2.0499, time 5016.82ms, mfu 0.00%\r\n","step 858: train loss 2.0683, val loss 2.1302\r\n","iter 858: loss 2.0196, time 4500.72ms, mfu 0.00%\r\n","step 859: train loss 2.0645, val loss 2.1198\r\n","iter 859: loss 1.9952, time 4492.95ms, mfu 0.00%\r\n","step 860: train loss 2.0647, val loss 2.1180\r\n","iter 860: loss 1.9918, time 4460.94ms, mfu 0.00%\r\n","step 861: train loss 2.0681, val loss 2.1203\r\n","iter 861: loss 2.0505, time 4452.11ms, mfu 0.00%\r\n","step 862: train loss 2.0836, val loss 2.1275\r\n","iter 862: loss 2.0403, time 4411.04ms, mfu 0.00%\r\n","step 863: train loss 2.0808, val loss 2.1286\r\n","iter 863: loss 2.0424, time 4481.10ms, mfu 0.00%\r\n","step 864: train loss 2.0678, val loss 2.1206\r\n","iter 864: loss 2.0709, time 5121.95ms, mfu 0.00%\r\n","step 865: train loss 2.0689, val loss 2.1265\r\n","iter 865: loss 1.9869, time 4476.91ms, mfu 0.00%\r\n","step 866: train loss 2.0675, val loss 2.1231\r\n","iter 866: loss 2.0368, time 4478.66ms, mfu 0.00%\r\n","step 867: train loss 2.0745, val loss 2.1250\r\n","iter 867: loss 2.1399, time 4546.94ms, mfu 0.00%\r\n","step 868: train loss 2.0775, val loss 2.1302\r\n","iter 868: loss 2.0516, time 4497.10ms, mfu 0.00%\r\n","step 869: train loss 2.0670, val loss 2.1257\r\n","iter 869: loss 2.1320, time 4684.42ms, mfu 0.00%\r\n","step 870: train loss 2.0720, val loss 2.1262\r\n","iter 870: loss 2.0017, time 4444.81ms, mfu 0.00%\r\n","step 871: train loss 2.0724, val loss 2.1266\r\n","iter 871: loss 2.0494, time 5052.23ms, mfu 0.00%\r\n","step 872: train loss 2.0733, val loss 2.1246\r\n","iter 872: loss 2.1410, time 4446.71ms, mfu 0.00%\r\n","step 873: train loss 2.0729, val loss 2.1302\r\n","iter 873: loss 2.0970, time 4513.97ms, mfu 0.00%\r\n","step 874: train loss 2.0690, val loss 2.1123\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 874: loss 2.0424, time 4479.16ms, mfu 0.00%\r\n","step 875: train loss 2.0647, val loss 2.1144\r\n","iter 875: loss 2.1098, time 4493.31ms, mfu 0.00%\r\n","step 876: train loss 2.0658, val loss 2.1136\r\n","iter 876: loss 2.1065, time 4394.96ms, mfu 0.00%\r\n","step 877: train loss 2.0787, val loss 2.1203\r\n","iter 877: loss 2.0579, time 4561.83ms, mfu 0.00%\r\n","step 878: train loss 2.0758, val loss 2.1273\r\n","iter 878: loss 2.1609, time 4980.40ms, mfu 0.00%\r\n","step 879: train loss 2.0710, val loss 2.1237\r\n","iter 879: loss 2.0618, time 4495.54ms, mfu 0.00%\r\n","step 880: train loss 2.0675, val loss 2.1203\r\n","iter 880: loss 2.0956, time 4564.63ms, mfu 0.00%\r\n","step 881: train loss 2.0689, val loss 2.1177\r\n","iter 881: loss 2.1459, time 4432.51ms, mfu 0.00%\r\n","step 882: train loss 2.0624, val loss 2.1264\r\n","iter 882: loss 2.1272, time 4629.76ms, mfu 0.00%\r\n","step 883: train loss 2.0687, val loss 2.1240\r\n","iter 883: loss 2.0538, time 4513.22ms, mfu 0.00%\r\n","step 884: train loss 2.0658, val loss 2.1169\r\n","iter 884: loss 2.1075, time 4584.71ms, mfu 0.00%\r\n","step 885: train loss 2.0669, val loss 2.1235\r\n","iter 885: loss 2.1462, time 5080.44ms, mfu 0.00%\r\n","step 886: train loss 2.0660, val loss 2.1190\r\n","iter 886: loss 1.9860, time 4526.17ms, mfu 0.00%\r\n","step 887: train loss 2.0514, val loss 2.1144\r\n","iter 887: loss 2.0908, time 4626.88ms, mfu 0.00%\r\n","step 888: train loss 2.0600, val loss 2.1093\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 888: loss 2.0014, time 4435.32ms, mfu 0.00%\r\n","step 889: train loss 2.0590, val loss 2.1087\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 889: loss 2.0873, time 4462.99ms, mfu 0.00%\r\n","step 890: train loss 2.0605, val loss 2.1061\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 890: loss 2.0319, time 4528.05ms, mfu 0.00%\r\n","step 891: train loss 2.0553, val loss 2.1092\r\n","iter 891: loss 2.0142, time 4510.01ms, mfu 0.00%\r\n","step 892: train loss 2.0596, val loss 2.1193\r\n","iter 892: loss 2.0358, time 4980.19ms, mfu 0.00%\r\n","step 893: train loss 2.0579, val loss 2.1085\r\n","iter 893: loss 2.0083, time 4565.24ms, mfu 0.00%\r\n","step 894: train loss 2.0636, val loss 2.1222\r\n","iter 894: loss 2.0055, time 4468.90ms, mfu 0.00%\r\n","step 895: train loss 2.0528, val loss 2.1181\r\n","iter 895: loss 2.1147, time 4549.33ms, mfu 0.00%\r\n","step 896: train loss 2.0492, val loss 2.1212\r\n","iter 896: loss 2.0832, time 4478.39ms, mfu 0.00%\r\n","step 897: train loss 2.0447, val loss 2.1207\r\n","iter 897: loss 2.0650, time 4551.10ms, mfu 0.00%\r\n","step 898: train loss 2.0513, val loss 2.1174\r\n","iter 898: loss 2.1070, time 4506.51ms, mfu 0.00%\r\n","step 899: train loss 2.0509, val loss 2.1203\r\n","iter 899: loss 2.0185, time 5323.47ms, mfu 0.00%\r\n","step 900: train loss 2.0439, val loss 2.1133\r\n","iter 900: loss 2.0931, time 4630.61ms, mfu 0.00%\r\n","step 901: train loss 2.0569, val loss 2.1158\r\n","iter 901: loss 2.0369, time 4554.98ms, mfu 0.00%\r\n","step 902: train loss 2.0500, val loss 2.1127\r\n","iter 902: loss 2.0261, time 4793.37ms, mfu 0.00%\r\n","step 903: train loss 2.0397, val loss 2.1124\r\n","iter 903: loss 2.0608, time 4732.82ms, mfu 0.00%\r\n","step 904: train loss 2.0415, val loss 2.1014\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 904: loss 2.1055, time 5022.42ms, mfu 0.00%\r\n","step 905: train loss 2.0483, val loss 2.0989\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 905: loss 2.1110, time 4679.29ms, mfu 0.00%\r\n","step 906: train loss 2.0429, val loss 2.1079\r\n","iter 906: loss 2.0689, time 5266.67ms, mfu 0.00%\r\n","step 907: train loss 2.0519, val loss 2.1116\r\n","iter 907: loss 2.1037, time 4490.62ms, mfu 0.00%\r\n","step 908: train loss 2.0469, val loss 2.1149\r\n","iter 908: loss 2.0254, time 4403.04ms, mfu 0.00%\r\n","step 909: train loss 2.0507, val loss 2.1126\r\n","iter 909: loss 2.0559, time 4508.59ms, mfu 0.00%\r\n","step 910: train loss 2.0527, val loss 2.1176\r\n","iter 910: loss 1.9993, time 4516.00ms, mfu 0.00%\r\n","step 911: train loss 2.0463, val loss 2.1142\r\n","iter 911: loss 2.0821, time 4383.68ms, mfu 0.00%\r\n","step 912: train loss 2.0448, val loss 2.1064\r\n","iter 912: loss 1.9039, time 4587.10ms, mfu 0.00%\r\n","step 913: train loss 2.0446, val loss 2.1024\r\n","iter 913: loss 2.1143, time 5040.87ms, mfu 0.00%\r\n","step 914: train loss 2.0378, val loss 2.0974\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 914: loss 2.0587, time 4532.39ms, mfu 0.00%\r\n","step 915: train loss 2.0451, val loss 2.1086\r\n","iter 915: loss 2.0852, time 4534.78ms, mfu 0.00%\r\n","step 916: train loss 2.0376, val loss 2.1013\r\n","iter 916: loss 2.1209, time 4631.66ms, mfu 0.00%\r\n","step 917: train loss 2.0395, val loss 2.1048\r\n","iter 917: loss 2.0410, time 4468.30ms, mfu 0.00%\r\n","step 918: train loss 2.0465, val loss 2.1005\r\n","iter 918: loss 2.0874, time 4481.99ms, mfu 0.00%\r\n","step 919: train loss 2.0439, val loss 2.1044\r\n","iter 919: loss 2.0715, time 4599.51ms, mfu 0.00%\r\n","step 920: train loss 2.0464, val loss 2.1077\r\n","iter 920: loss 2.1183, time 5027.64ms, mfu 0.00%\r\n","step 921: train loss 2.0456, val loss 2.1101\r\n","iter 921: loss 2.0260, time 4613.67ms, mfu 0.00%\r\n","step 922: train loss 2.0356, val loss 2.1006\r\n","iter 922: loss 2.0483, time 4626.48ms, mfu 0.00%\r\n","step 923: train loss 2.0377, val loss 2.0976\r\n","iter 923: loss 2.0163, time 4472.78ms, mfu 0.00%\r\n","step 924: train loss 2.0335, val loss 2.0974\r\n","iter 924: loss 2.0545, time 4493.27ms, mfu 0.00%\r\n","step 925: train loss 2.0367, val loss 2.1006\r\n","iter 925: loss 2.0305, time 4437.47ms, mfu 0.00%\r\n","step 926: train loss 2.0302, val loss 2.0992\r\n","iter 926: loss 2.0665, time 4416.16ms, mfu 0.00%\r\n","step 927: train loss 2.0471, val loss 2.1012\r\n","iter 927: loss 2.0945, time 5103.43ms, mfu 0.00%\r\n","step 928: train loss 2.0317, val loss 2.1026\r\n","iter 928: loss 1.9666, time 4386.39ms, mfu 0.00%\r\n","step 929: train loss 2.0346, val loss 2.0983\r\n","iter 929: loss 2.0893, time 4461.69ms, mfu 0.00%\r\n","step 930: train loss 2.0451, val loss 2.1034\r\n","iter 930: loss 2.0638, time 4425.85ms, mfu 0.00%\r\n","step 931: train loss 2.0389, val loss 2.0959\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 931: loss 2.0055, time 4448.20ms, mfu 0.00%\r\n","step 932: train loss 2.0339, val loss 2.0934\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 932: loss 2.0807, time 4475.40ms, mfu 0.00%\r\n","step 933: train loss 2.0341, val loss 2.0869\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 933: loss 1.9553, time 4355.07ms, mfu 0.00%\r\n","step 934: train loss 2.0304, val loss 2.0961\r\n","iter 934: loss 1.9838, time 4945.54ms, mfu 0.00%\r\n","step 935: train loss 2.0367, val loss 2.0895\r\n","iter 935: loss 2.1108, time 4549.31ms, mfu 0.00%\r\n","step 936: train loss 2.0274, val loss 2.1036\r\n","iter 936: loss 2.1504, time 4431.99ms, mfu 0.00%\r\n","step 937: train loss 2.0301, val loss 2.0965\r\n","iter 937: loss 1.9619, time 4307.95ms, mfu 0.00%\r\n","step 938: train loss 2.0437, val loss 2.0941\r\n","iter 938: loss 2.0185, time 4521.88ms, mfu 0.00%\r\n","step 939: train loss 2.0329, val loss 2.0980\r\n","iter 939: loss 1.9554, time 4358.31ms, mfu 0.00%\r\n","step 940: train loss 2.0396, val loss 2.1023\r\n","iter 940: loss 2.0728, time 4369.61ms, mfu 0.00%\r\n","step 941: train loss 2.0316, val loss 2.0998\r\n","iter 941: loss 2.0451, time 4465.64ms, mfu 0.00%\r\n","step 942: train loss 2.0304, val loss 2.0944\r\n","iter 942: loss 2.0462, time 5132.38ms, mfu 0.00%\r\n","step 943: train loss 2.0240, val loss 2.0972\r\n","iter 943: loss 2.0556, time 4559.41ms, mfu 0.00%\r\n","step 944: train loss 2.0256, val loss 2.1018\r\n","iter 944: loss 1.9812, time 4369.56ms, mfu 0.00%\r\n","step 945: train loss 2.0253, val loss 2.0994\r\n","iter 945: loss 2.0987, time 4455.71ms, mfu 0.00%\r\n","step 946: train loss 2.0311, val loss 2.1005\r\n","iter 946: loss 2.0249, time 4363.61ms, mfu 0.00%\r\n","step 947: train loss 2.0295, val loss 2.0995\r\n","iter 947: loss 1.9992, time 4497.85ms, mfu 0.00%\r\n","step 948: train loss 2.0329, val loss 2.0985\r\n","iter 948: loss 1.8815, time 4413.68ms, mfu 0.00%\r\n","step 949: train loss 2.0302, val loss 2.1048\r\n","iter 949: loss 2.0987, time 5105.60ms, mfu 0.00%\r\n","step 950: train loss 2.0287, val loss 2.0883\r\n","iter 950: loss 2.0223, time 4476.93ms, mfu 0.00%\r\n","step 951: train loss 2.0237, val loss 2.0963\r\n","iter 951: loss 2.0259, time 4417.72ms, mfu 0.00%\r\n","step 952: train loss 2.0188, val loss 2.0937\r\n","iter 952: loss 2.0402, time 4373.81ms, mfu 0.00%\r\n","step 953: train loss 2.0197, val loss 2.0948\r\n","iter 953: loss 1.9755, time 4325.39ms, mfu 0.00%\r\n","step 954: train loss 2.0238, val loss 2.0984\r\n","iter 954: loss 2.0013, time 4444.00ms, mfu 0.00%\r\n","step 955: train loss 2.0205, val loss 2.0886\r\n","iter 955: loss 1.9782, time 4371.19ms, mfu 0.00%\r\n","step 956: train loss 2.0172, val loss 2.0942\r\n","iter 956: loss 2.0580, time 5087.93ms, mfu 0.00%\r\n","step 957: train loss 2.0141, val loss 2.0899\r\n","iter 957: loss 2.0218, time 4508.94ms, mfu 0.00%\r\n","step 958: train loss 2.0248, val loss 2.0987\r\n","iter 958: loss 2.0267, time 4455.15ms, mfu 0.00%\r\n","step 959: train loss 2.0227, val loss 2.0971\r\n","iter 959: loss 2.1014, time 4390.64ms, mfu 0.00%\r\n","step 960: train loss 2.0260, val loss 2.1037\r\n","iter 960: loss 2.0049, time 4480.55ms, mfu 0.00%\r\n","step 961: train loss 2.0318, val loss 2.0984\r\n","iter 961: loss 2.0129, time 4429.75ms, mfu 0.00%\r\n","step 962: train loss 2.0210, val loss 2.1041\r\n","iter 962: loss 2.0039, time 4458.86ms, mfu 0.00%\r\n","step 963: train loss 2.0250, val loss 2.1000\r\n","iter 963: loss 1.9328, time 5040.38ms, mfu 0.00%\r\n","step 964: train loss 2.0179, val loss 2.1034\r\n","iter 964: loss 1.9791, time 4394.54ms, mfu 0.00%\r\n","step 965: train loss 2.0208, val loss 2.0936\r\n","iter 965: loss 1.9886, time 4429.40ms, mfu 0.00%\r\n","step 966: train loss 2.0169, val loss 2.0921\r\n","iter 966: loss 2.0735, time 4352.61ms, mfu 0.00%\r\n","step 967: train loss 2.0218, val loss 2.0868\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 967: loss 2.0415, time 4514.20ms, mfu 0.00%\r\n","step 968: train loss 2.0153, val loss 2.0863\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 968: loss 2.0193, time 4445.64ms, mfu 0.00%\r\n","step 969: train loss 2.0155, val loss 2.0849\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 969: loss 2.0262, time 4470.28ms, mfu 0.00%\r\n","step 970: train loss 2.0198, val loss 2.1025\r\n","iter 970: loss 1.9985, time 4960.60ms, mfu 0.00%\r\n","step 971: train loss 2.0234, val loss 2.0987\r\n","iter 971: loss 2.1032, time 4549.81ms, mfu 0.00%\r\n","step 972: train loss 2.0058, val loss 2.0933\r\n","iter 972: loss 1.9380, time 4417.09ms, mfu 0.00%\r\n","step 973: train loss 2.0039, val loss 2.0855\r\n","iter 973: loss 2.0557, time 4351.37ms, mfu 0.00%\r\n","step 974: train loss 2.0083, val loss 2.0912\r\n","iter 974: loss 2.0424, time 4491.87ms, mfu 0.00%\r\n","step 975: train loss 2.0136, val loss 2.0797\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 975: loss 2.2150, time 4479.86ms, mfu 0.00%\r\n","step 976: train loss 2.0122, val loss 2.0864\r\n","iter 976: loss 1.9916, time 4619.94ms, mfu 0.00%\r\n","step 977: train loss 2.0183, val loss 2.0838\r\n","iter 977: loss 2.1349, time 4526.04ms, mfu 0.00%\r\n","step 978: train loss 2.0263, val loss 2.0872\r\n","iter 978: loss 1.9906, time 5018.83ms, mfu 0.00%\r\n","step 979: train loss 2.0208, val loss 2.0844\r\n","iter 979: loss 2.0605, time 4400.61ms, mfu 0.00%\r\n","step 980: train loss 2.0236, val loss 2.0888\r\n","iter 980: loss 1.9783, time 4334.74ms, mfu 0.00%\r\n","step 981: train loss 2.0222, val loss 2.0891\r\n","iter 981: loss 2.0519, time 4386.43ms, mfu 0.00%\r\n","step 982: train loss 2.0126, val loss 2.0866\r\n","iter 982: loss 1.9921, time 4445.06ms, mfu 0.00%\r\n","step 983: train loss 2.0078, val loss 2.0741\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 983: loss 2.0231, time 4431.66ms, mfu 0.00%\r\n","step 984: train loss 1.9991, val loss 2.0786\r\n","iter 984: loss 2.0622, time 4372.65ms, mfu 0.00%\r\n","step 985: train loss 2.0053, val loss 2.0798\r\n","iter 985: loss 2.0015, time 5097.86ms, mfu 0.00%\r\n","step 986: train loss 2.0102, val loss 2.0810\r\n","iter 986: loss 2.0133, time 4318.73ms, mfu 0.00%\r\n","step 987: train loss 2.0092, val loss 2.0819\r\n","iter 987: loss 1.9811, time 4421.58ms, mfu 0.00%\r\n","step 988: train loss 2.0091, val loss 2.0889\r\n","iter 988: loss 2.0923, time 4365.18ms, mfu 0.00%\r\n","step 989: train loss 2.0230, val loss 2.0791\r\n","iter 989: loss 2.1069, time 4521.41ms, mfu 0.00%\r\n","step 990: train loss 2.0268, val loss 2.0950\r\n","iter 990: loss 2.0872, time 4441.23ms, mfu 0.00%\r\n","step 991: train loss 2.0215, val loss 2.0939\r\n","iter 991: loss 2.0717, time 4354.27ms, mfu 0.00%\r\n","step 992: train loss 2.0178, val loss 2.0938\r\n","iter 992: loss 2.0064, time 5163.09ms, mfu 0.00%\r\n","step 993: train loss 2.0203, val loss 2.0816\r\n","iter 993: loss 1.9878, time 4542.23ms, mfu 0.00%\r\n","step 994: train loss 2.0131, val loss 2.0804\r\n","iter 994: loss 2.0072, time 4379.59ms, mfu 0.00%\r\n","step 995: train loss 2.0072, val loss 2.0894\r\n","iter 995: loss 2.0706, time 4354.44ms, mfu 0.00%\r\n","step 996: train loss 2.0153, val loss 2.0906\r\n","iter 996: loss 1.9921, time 4555.78ms, mfu 0.00%\r\n","step 997: train loss 2.0113, val loss 2.0733\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 997: loss 1.9203, time 4373.34ms, mfu 0.00%\r\n","step 998: train loss 1.9918, val loss 2.0755\r\n","iter 998: loss 1.9480, time 4465.29ms, mfu 0.00%\r\n","step 999: train loss 1.9961, val loss 2.0755\r\n","iter 999: loss 1.9979, time 4949.39ms, mfu 0.00%\r\n","step 1000: train loss 2.0132, val loss 2.0766\r\n","iter 1000: loss 1.9687, time 4321.73ms, mfu 0.00%\r\n","step 1001: train loss 2.0045, val loss 2.0808\r\n","iter 1001: loss 2.0228, time 4423.23ms, mfu 0.00%\r\n","step 1002: train loss 2.0022, val loss 2.0810\r\n","iter 1002: loss 2.0628, time 4387.85ms, mfu 0.00%\r\n","step 1003: train loss 2.0103, val loss 2.0788\r\n","iter 1003: loss 2.0095, time 4436.34ms, mfu 0.00%\r\n","step 1004: train loss 2.0033, val loss 2.0678\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 1004: loss 1.9738, time 4470.30ms, mfu 0.00%\r\n","step 1005: train loss 2.0064, val loss 2.0810\r\n","iter 1005: loss 2.0146, time 4397.76ms, mfu 0.00%\r\n","step 1006: train loss 2.0145, val loss 2.0760\r\n","iter 1006: loss 2.0346, time 5082.21ms, mfu 0.00%\r\n","step 1007: train loss 2.0227, val loss 2.0911\r\n","iter 1007: loss 1.9798, time 4480.57ms, mfu 0.00%\r\n","step 1008: train loss 2.0192, val loss 2.0868\r\n","iter 1008: loss 1.9897, time 4289.24ms, mfu 0.00%\r\n","step 1009: train loss 2.0135, val loss 2.0758\r\n","iter 1009: loss 1.9072, time 4501.58ms, mfu 0.00%\r\n","step 1010: train loss 2.0099, val loss 2.0740\r\n","iter 1010: loss 2.0641, time 4455.35ms, mfu 0.00%\r\n","step 1011: train loss 2.0124, val loss 2.0662\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 1011: loss 2.0459, time 4536.52ms, mfu 0.00%\r\n","step 1012: train loss 2.0059, val loss 2.0738\r\n","iter 1012: loss 1.9620, time 4516.13ms, mfu 0.00%\r\n","step 1013: train loss 2.0069, val loss 2.0779\r\n","iter 1013: loss 2.0148, time 4380.96ms, mfu 0.00%\r\n","step 1014: train loss 2.0022, val loss 2.0773\r\n","iter 1014: loss 1.9569, time 5067.02ms, mfu 0.00%\r\n","step 1015: train loss 1.9996, val loss 2.0712\r\n","iter 1015: loss 1.9687, time 4441.80ms, mfu 0.00%\r\n","step 1016: train loss 1.9924, val loss 2.0662\r\n","iter 1016: loss 2.0375, time 4546.73ms, mfu 0.00%\r\n","step 1017: train loss 1.9961, val loss 2.0655\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 1017: loss 1.9026, time 4445.17ms, mfu 0.00%\r\n","step 1018: train loss 1.9918, val loss 2.0732\r\n","iter 1018: loss 2.0189, time 4661.52ms, mfu 0.00%\r\n","step 1019: train loss 1.9976, val loss 2.0783\r\n","iter 1019: loss 1.9321, time 4372.25ms, mfu 0.00%\r\n","step 1020: train loss 2.0034, val loss 2.0770\r\n","iter 1020: loss 2.0646, time 4564.29ms, mfu 0.00%\r\n","step 1021: train loss 2.0144, val loss 2.0764\r\n","iter 1021: loss 2.0645, time 5040.18ms, mfu 0.00%\r\n","step 1022: train loss 2.0023, val loss 2.0810\r\n","iter 1022: loss 2.1159, time 4489.15ms, mfu 0.00%\r\n","step 1023: train loss 2.0022, val loss 2.0760\r\n","iter 1023: loss 2.0298, time 4465.90ms, mfu 0.00%\r\n","step 1024: train loss 2.0009, val loss 2.0774\r\n","iter 1024: loss 2.0175, time 4527.45ms, mfu 0.00%\r\n","step 1025: train loss 2.0123, val loss 2.0662\r\n","iter 1025: loss 1.9865, time 4559.75ms, mfu 0.00%\r\n","step 1026: train loss 2.0029, val loss 2.0686\r\n","iter 1026: loss 1.9132, time 4498.04ms, mfu 0.00%\r\n","step 1027: train loss 2.0021, val loss 2.0687\r\n","iter 1027: loss 2.0390, time 4405.79ms, mfu 0.00%\r\n","step 1028: train loss 1.9929, val loss 2.0688\r\n","iter 1028: loss 1.9321, time 5163.05ms, mfu 0.00%\r\n","step 1029: train loss 2.0001, val loss 2.0701\r\n","iter 1029: loss 1.9491, time 4580.01ms, mfu 0.00%\r\n","step 1030: train loss 1.9994, val loss 2.0731\r\n","iter 1030: loss 1.9809, time 4340.53ms, mfu 0.00%\r\n","step 1031: train loss 1.9952, val loss 2.0716\r\n","iter 1031: loss 1.9503, time 4523.48ms, mfu 0.00%\r\n","step 1032: train loss 1.9981, val loss 2.0736\r\n","iter 1032: loss 2.0237, time 4455.68ms, mfu 0.00%\r\n","step 1033: train loss 2.0004, val loss 2.0736\r\n","iter 1033: loss 1.9816, time 4424.57ms, mfu 0.00%\r\n","step 1034: train loss 1.9948, val loss 2.0676\r\n","iter 1034: loss 1.9776, time 4522.37ms, mfu 0.00%\r\n","step 1035: train loss 1.9986, val loss 2.0679\r\n","iter 1035: loss 2.0338, time 5122.71ms, mfu 0.00%\r\n","step 1036: train loss 1.9984, val loss 2.0680\r\n","iter 1036: loss 1.9540, time 4401.62ms, mfu 0.00%\r\n","step 1037: train loss 1.9947, val loss 2.0664\r\n","iter 1037: loss 2.0310, time 4412.82ms, mfu 0.00%\r\n","step 1038: train loss 1.9983, val loss 2.0688\r\n","iter 1038: loss 1.9924, time 4482.77ms, mfu 0.00%\r\n","step 1039: train loss 1.9995, val loss 2.0635\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 1039: loss 2.0900, time 4495.42ms, mfu 0.00%\r\n","step 1040: train loss 1.9925, val loss 2.0579\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 1040: loss 2.0806, time 4436.23ms, mfu 0.00%\r\n","step 1041: train loss 1.9841, val loss 2.0586\r\n","iter 1041: loss 1.8923, time 4380.50ms, mfu 0.00%\r\n","step 1042: train loss 1.9854, val loss 2.0407\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 1042: loss 1.9980, time 5138.56ms, mfu 0.00%\r\n","step 1043: train loss 1.9983, val loss 2.0501\r\n","iter 1043: loss 1.9073, time 4426.57ms, mfu 0.00%\r\n","step 1044: train loss 1.9922, val loss 2.0566\r\n","iter 1044: loss 1.9833, time 4440.51ms, mfu 0.00%\r\n","step 1045: train loss 1.9926, val loss 2.0557\r\n","iter 1045: loss 1.9738, time 4460.39ms, mfu 0.00%\r\n","step 1046: train loss 1.9894, val loss 2.0708\r\n","iter 1046: loss 2.0959, time 4351.10ms, mfu 0.00%\r\n","step 1047: train loss 2.0000, val loss 2.0604\r\n","iter 1047: loss 1.9413, time 4555.68ms, mfu 0.00%\r\n","step 1048: train loss 1.9880, val loss 2.0620\r\n","iter 1048: loss 2.0450, time 4464.30ms, mfu 0.00%\r\n","step 1049: train loss 1.9838, val loss 2.0559\r\n","iter 1049: loss 2.0222, time 4632.30ms, mfu 0.00%\r\n","step 1050: train loss 1.9853, val loss 2.0550\r\n","iter 1050: loss 2.0064, time 4872.45ms, mfu 0.00%\r\n","step 1051: train loss 1.9845, val loss 2.0512\r\n","iter 1051: loss 1.9876, time 4392.97ms, mfu 0.00%\r\n","step 1052: train loss 1.9835, val loss 2.0565\r\n","iter 1052: loss 1.9182, time 4422.87ms, mfu 0.00%\r\n","step 1053: train loss 1.9975, val loss 2.0587\r\n","iter 1053: loss 1.9089, time 4477.50ms, mfu 0.00%\r\n","step 1054: train loss 1.9931, val loss 2.0582\r\n","iter 1054: loss 1.9598, time 4392.60ms, mfu 0.00%\r\n","step 1055: train loss 1.9813, val loss 2.0573\r\n","iter 1055: loss 1.9994, time 4432.52ms, mfu 0.00%\r\n","step 1056: train loss 1.9885, val loss 2.0535\r\n","iter 1056: loss 1.9548, time 4493.22ms, mfu 0.00%\r\n","step 1057: train loss 1.9873, val loss 2.0539\r\n","iter 1057: loss 1.9396, time 4963.33ms, mfu 0.00%\r\n","step 1058: train loss 1.9851, val loss 2.0505\r\n","iter 1058: loss 2.0112, time 4406.91ms, mfu 0.00%\r\n","step 1059: train loss 1.9832, val loss 2.0527\r\n","iter 1059: loss 1.9755, time 4428.70ms, mfu 0.00%\r\n","step 1060: train loss 1.9720, val loss 2.0524\r\n","iter 1060: loss 1.9976, time 4468.85ms, mfu 0.00%\r\n","step 1061: train loss 1.9785, val loss 2.0464\r\n","iter 1061: loss 1.9693, time 4366.51ms, mfu 0.00%\r\n","step 1062: train loss 1.9777, val loss 2.0510\r\n","iter 1062: loss 2.0162, time 4495.43ms, mfu 0.00%\r\n","step 1063: train loss 1.9749, val loss 2.0486\r\n","iter 1063: loss 2.1041, time 4362.96ms, mfu 0.00%\r\n","step 1064: train loss 1.9839, val loss 2.0432\r\n","iter 1064: loss 2.0063, time 5183.64ms, mfu 0.00%\r\n","step 1065: train loss 1.9734, val loss 2.0509\r\n","iter 1065: loss 1.9278, time 4567.39ms, mfu 0.00%\r\n","step 1066: train loss 1.9889, val loss 2.0579\r\n","iter 1066: loss 1.9293, time 4415.16ms, mfu 0.00%\r\n","step 1067: train loss 1.9862, val loss 2.0624\r\n","iter 1067: loss 2.0469, time 4496.72ms, mfu 0.00%\r\n","step 1068: train loss 1.9819, val loss 2.0626\r\n","iter 1068: loss 2.0208, time 4332.01ms, mfu 0.00%\r\n","step 1069: train loss 1.9815, val loss 2.0601\r\n","iter 1069: loss 1.9485, time 4431.80ms, mfu 0.00%\r\n","step 1070: train loss 1.9746, val loss 2.0502\r\n","iter 1070: loss 2.0941, time 4393.42ms, mfu 0.00%\r\n","step 1071: train loss 1.9793, val loss 2.0557\r\n","iter 1071: loss 1.9011, time 4921.72ms, mfu 0.00%\r\n","step 1072: train loss 1.9796, val loss 2.0492\r\n","iter 1072: loss 2.0042, time 4321.34ms, mfu 0.00%\r\n","step 1073: train loss 1.9845, val loss 2.0544\r\n","iter 1073: loss 2.0494, time 4531.83ms, mfu 0.00%\r\n","step 1074: train loss 1.9777, val loss 2.0556\r\n","iter 1074: loss 1.9531, time 4416.76ms, mfu 0.00%\r\n","step 1075: train loss 1.9783, val loss 2.0552\r\n","iter 1075: loss 2.0023, time 4459.22ms, mfu 0.00%\r\n","step 1076: train loss 1.9718, val loss 2.0551\r\n","iter 1076: loss 1.9108, time 4409.86ms, mfu 0.00%\r\n","step 1077: train loss 1.9716, val loss 2.0659\r\n","iter 1077: loss 1.9787, time 4338.54ms, mfu 0.00%\r\n","step 1078: train loss 1.9852, val loss 2.0610\r\n","iter 1078: loss 1.9495, time 5204.30ms, mfu 0.00%\r\n","step 1079: train loss 1.9843, val loss 2.0598\r\n","iter 1079: loss 2.0044, time 4292.29ms, mfu 0.00%\r\n","step 1080: train loss 1.9833, val loss 2.0551\r\n","iter 1080: loss 2.0831, time 4403.64ms, mfu 0.00%\r\n","step 1081: train loss 1.9771, val loss 2.0456\r\n","iter 1081: loss 2.0471, time 4441.13ms, mfu 0.00%\r\n","step 1082: train loss 1.9794, val loss 2.0508\r\n","iter 1082: loss 2.0768, time 4350.34ms, mfu 0.00%\r\n","step 1083: train loss 1.9840, val loss 2.0485\r\n","iter 1083: loss 1.8750, time 4538.33ms, mfu 0.00%\r\n","step 1084: train loss 1.9802, val loss 2.0380\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 1084: loss 2.0555, time 4511.41ms, mfu 0.00%\r\n","step 1085: train loss 1.9701, val loss 2.0514\r\n","iter 1085: loss 2.0115, time 4640.32ms, mfu 0.00%\r\n","step 1086: train loss 1.9812, val loss 2.0434\r\n","iter 1086: loss 1.9972, time 4858.25ms, mfu 0.00%\r\n","step 1087: train loss 1.9859, val loss 2.0505\r\n","iter 1087: loss 2.0504, time 4464.03ms, mfu 0.00%\r\n","step 1088: train loss 1.9796, val loss 2.0446\r\n","iter 1088: loss 1.9634, time 4362.33ms, mfu 0.00%\r\n","step 1089: train loss 1.9821, val loss 2.0493\r\n","iter 1089: loss 2.0327, time 4377.87ms, mfu 0.00%\r\n","step 1090: train loss 1.9815, val loss 2.0523\r\n","iter 1090: loss 2.0056, time 4318.60ms, mfu 0.00%\r\n","step 1091: train loss 1.9847, val loss 2.0394\r\n","iter 1091: loss 2.0076, time 4404.15ms, mfu 0.00%\r\n","step 1092: train loss 1.9755, val loss 2.0354\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 1092: loss 1.9830, time 4541.62ms, mfu 0.00%\r\n","step 1093: train loss 1.9817, val loss 2.0420\r\n","iter 1093: loss 1.9320, time 5184.94ms, mfu 0.00%\r\n","step 1094: train loss 1.9665, val loss 2.0335\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 1094: loss 1.8896, time 4516.23ms, mfu 0.00%\r\n","step 1095: train loss 1.9761, val loss 2.0364\r\n","iter 1095: loss 2.0796, time 4582.98ms, mfu 0.00%\r\n","step 1096: train loss 1.9720, val loss 2.0478\r\n","iter 1096: loss 1.9668, time 4411.63ms, mfu 0.00%\r\n","step 1097: train loss 1.9760, val loss 2.0453\r\n","iter 1097: loss 1.9290, time 4462.43ms, mfu 0.00%\r\n","step 1098: train loss 1.9753, val loss 2.0504\r\n","iter 1098: loss 1.9174, time 4519.88ms, mfu 0.00%\r\n","step 1099: train loss 1.9755, val loss 2.0512\r\n","iter 1099: loss 1.8994, time 4370.17ms, mfu 0.00%\r\n","step 1100: train loss 1.9663, val loss 2.0432\r\n","iter 1100: loss 1.9706, time 5160.79ms, mfu 0.00%\r\n","step 1101: train loss 1.9684, val loss 2.0434\r\n","iter 1101: loss 1.9099, time 4548.80ms, mfu 0.00%\r\n","step 1102: train loss 1.9749, val loss 2.0407\r\n","iter 1102: loss 1.8412, time 4512.05ms, mfu 0.00%\r\n","step 1103: train loss 1.9632, val loss 2.0476\r\n","iter 1103: loss 1.9732, time 4475.97ms, mfu 0.00%\r\n","step 1104: train loss 1.9645, val loss 2.0526\r\n","iter 1104: loss 1.9058, time 4593.99ms, mfu 0.00%\r\n","step 1105: train loss 1.9671, val loss 2.0517\r\n","iter 1105: loss 1.9421, time 4459.53ms, mfu 0.00%\r\n","step 1106: train loss 1.9699, val loss 2.0541\r\n","iter 1106: loss 1.9878, time 4476.63ms, mfu 0.00%\r\n","step 1107: train loss 1.9786, val loss 2.0601\r\n","iter 1107: loss 1.9331, time 4907.31ms, mfu 0.00%\r\n","step 1108: train loss 1.9756, val loss 2.0564\r\n","iter 1108: loss 1.9108, time 4535.70ms, mfu 0.00%\r\n","step 1109: train loss 1.9707, val loss 2.0576\r\n","iter 1109: loss 2.0342, time 4383.81ms, mfu 0.00%\r\n","step 1110: train loss 1.9658, val loss 2.0579\r\n","iter 1110: loss 1.9465, time 4384.31ms, mfu 0.00%\r\n","step 1111: train loss 1.9705, val loss 2.0515\r\n","iter 1111: loss 2.1161, time 4452.71ms, mfu 0.00%\r\n","step 1112: train loss 1.9643, val loss 2.0483\r\n","iter 1112: loss 2.0016, time 4263.32ms, mfu 0.00%\r\n","step 1113: train loss 1.9655, val loss 2.0509\r\n","iter 1113: loss 1.9947, time 4338.11ms, mfu 0.00%\r\n","step 1114: train loss 1.9698, val loss 2.0503\r\n","iter 1114: loss 1.9046, time 5054.51ms, mfu 0.00%\r\n","step 1115: train loss 1.9606, val loss 2.0412\r\n","iter 1115: loss 1.9304, time 4376.95ms, mfu 0.00%\r\n","step 1116: train loss 1.9593, val loss 2.0414\r\n","iter 1116: loss 2.0343, time 4296.62ms, mfu 0.00%\r\n","step 1117: train loss 1.9687, val loss 2.0392\r\n","iter 1117: loss 1.9459, time 4389.41ms, mfu 0.00%\r\n","step 1118: train loss 1.9506, val loss 2.0413\r\n","iter 1118: loss 2.0685, time 4337.48ms, mfu 0.00%\r\n","step 1119: train loss 1.9637, val loss 2.0410\r\n","iter 1119: loss 1.9894, time 4458.47ms, mfu 0.00%\r\n","step 1120: train loss 1.9664, val loss 2.0376\r\n","iter 1120: loss 1.9710, time 4547.57ms, mfu 0.00%\r\n","step 1121: train loss 1.9673, val loss 2.0456\r\n","iter 1121: loss 2.0633, time 4650.74ms, mfu 0.00%\r\n","step 1122: train loss 1.9634, val loss 2.0383\r\n","iter 1122: loss 2.0609, time 4834.81ms, mfu 0.00%\r\n","step 1123: train loss 1.9687, val loss 2.0431\r\n","iter 1123: loss 1.9204, time 4557.22ms, mfu 0.00%\r\n","step 1124: train loss 1.9569, val loss 2.0351\r\n","iter 1124: loss 1.9600, time 4500.30ms, mfu 0.00%\r\n","step 1125: train loss 1.9568, val loss 2.0361\r\n","iter 1125: loss 2.0343, time 4628.43ms, mfu 0.00%\r\n","step 1126: train loss 1.9645, val loss 2.0401\r\n","iter 1126: loss 1.9095, time 4370.83ms, mfu 0.00%\r\n","step 1127: train loss 1.9607, val loss 2.0271\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 1127: loss 2.0064, time 4485.49ms, mfu 0.00%\r\n","step 1128: train loss 1.9551, val loss 2.0366\r\n","iter 1128: loss 1.9514, time 4948.97ms, mfu 0.00%\r\n","step 1129: train loss 1.9605, val loss 2.0394\r\n","iter 1129: loss 1.9008, time 4756.91ms, mfu 0.00%\r\n","step 1130: train loss 1.9552, val loss 2.0407\r\n","iter 1130: loss 2.0591, time 4597.41ms, mfu 0.00%\r\n","step 1131: train loss 1.9605, val loss 2.0352\r\n","iter 1131: loss 1.8364, time 4669.39ms, mfu 0.00%\r\n","step 1132: train loss 1.9589, val loss 2.0363\r\n","iter 1132: loss 2.0278, time 4433.12ms, mfu 0.00%\r\n","step 1133: train loss 1.9520, val loss 2.0307\r\n","iter 1133: loss 2.0280, time 4558.69ms, mfu 0.00%\r\n","step 1134: train loss 1.9576, val loss 2.0318\r\n","iter 1134: loss 1.9591, time 4529.21ms, mfu 0.00%\r\n","step 1135: train loss 1.9580, val loss 2.0389\r\n","iter 1135: loss 1.8772, time 4955.07ms, mfu 0.00%\r\n","step 1136: train loss 1.9597, val loss 2.0460\r\n","iter 1136: loss 1.9892, time 4939.15ms, mfu 0.00%\r\n","step 1137: train loss 1.9650, val loss 2.0422\r\n","iter 1137: loss 1.9472, time 4495.95ms, mfu 0.00%\r\n","step 1138: train loss 1.9576, val loss 2.0330\r\n","iter 1138: loss 1.9251, time 4462.73ms, mfu 0.00%\r\n","step 1139: train loss 1.9430, val loss 2.0209\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 1139: loss 2.0067, time 4502.98ms, mfu 0.00%\r\n","step 1140: train loss 1.9558, val loss 2.0380\r\n","iter 1140: loss 1.9439, time 4424.48ms, mfu 0.00%\r\n","step 1141: train loss 1.9579, val loss 2.0359\r\n","iter 1141: loss 1.9946, time 4529.28ms, mfu 0.00%\r\n","step 1142: train loss 1.9578, val loss 2.0440\r\n","iter 1142: loss 1.8954, time 4617.66ms, mfu 0.00%\r\n","step 1143: train loss 1.9504, val loss 2.0287\r\n","iter 1143: loss 2.0027, time 4873.17ms, mfu 0.00%\r\n","step 1144: train loss 1.9490, val loss 2.0288\r\n","iter 1144: loss 1.9728, time 4534.67ms, mfu 0.00%\r\n","step 1145: train loss 1.9592, val loss 2.0266\r\n","iter 1145: loss 1.9597, time 4365.72ms, mfu 0.00%\r\n","step 1146: train loss 1.9525, val loss 2.0327\r\n","iter 1146: loss 1.9995, time 4401.62ms, mfu 0.00%\r\n","step 1147: train loss 1.9526, val loss 2.0397\r\n","iter 1147: loss 1.9057, time 4482.19ms, mfu 0.00%\r\n","step 1148: train loss 1.9516, val loss 2.0287\r\n","iter 1148: loss 1.9069, time 4687.89ms, mfu 0.00%\r\n","step 1149: train loss 1.9538, val loss 2.0217\r\n","iter 1149: loss 2.0132, time 4530.18ms, mfu 0.00%\r\n","step 1150: train loss 1.9415, val loss 2.0266\r\n","iter 1150: loss 1.9711, time 5214.03ms, mfu 0.00%\r\n","step 1151: train loss 1.9509, val loss 2.0224\r\n","iter 1151: loss 1.9229, time 4993.52ms, mfu 0.00%\r\n","step 1152: train loss 1.9540, val loss 2.0223\r\n","iter 1152: loss 1.9090, time 4723.60ms, mfu 0.00%\r\n","step 1153: train loss 1.9527, val loss 2.0280\r\n","iter 1153: loss 1.9809, time 4660.87ms, mfu 0.00%\r\n","step 1154: train loss 1.9467, val loss 2.0225\r\n","iter 1154: loss 2.0045, time 4914.07ms, mfu 0.00%\r\n","step 1155: train loss 1.9459, val loss 2.0268\r\n","iter 1155: loss 1.9098, time 4618.07ms, mfu 0.00%\r\n","step 1156: train loss 1.9461, val loss 2.0277\r\n","iter 1156: loss 1.9947, time 5253.53ms, mfu 0.00%\r\n","step 1157: train loss 1.9457, val loss 2.0231\r\n","iter 1157: loss 1.9258, time 4663.85ms, mfu 0.00%\r\n","step 1158: train loss 1.9458, val loss 2.0314\r\n","iter 1158: loss 1.9142, time 4607.04ms, mfu 0.00%\r\n","step 1159: train loss 1.9578, val loss 2.0311\r\n","iter 1159: loss 1.9624, time 4442.36ms, mfu 0.00%\r\n","step 1160: train loss 1.9530, val loss 2.0361\r\n","iter 1160: loss 1.9580, time 4631.68ms, mfu 0.00%\r\n","step 1161: train loss 1.9484, val loss 2.0325\r\n","iter 1161: loss 1.9381, time 4716.17ms, mfu 0.00%\r\n","step 1162: train loss 1.9528, val loss 2.0342\r\n","iter 1162: loss 1.8442, time 4691.75ms, mfu 0.00%\r\n","step 1163: train loss 1.9565, val loss 2.0336\r\n","iter 1163: loss 1.8890, time 5395.07ms, mfu 0.00%\r\n","step 1164: train loss 1.9501, val loss 2.0230\r\n","iter 1164: loss 2.0017, time 4797.84ms, mfu 0.00%\r\n","step 1165: train loss 1.9523, val loss 2.0307\r\n","iter 1165: loss 1.9772, time 4724.86ms, mfu 0.00%\r\n","step 1166: train loss 1.9521, val loss 2.0370\r\n","iter 1166: loss 1.9566, time 4770.75ms, mfu 0.00%\r\n","step 1167: train loss 1.9508, val loss 2.0336\r\n","iter 1167: loss 1.9366, time 4786.77ms, mfu 0.00%\r\n","step 1168: train loss 1.9420, val loss 2.0275\r\n","iter 1168: loss 1.9042, time 4761.32ms, mfu 0.00%\r\n","step 1169: train loss 1.9398, val loss 2.0232\r\n","iter 1169: loss 1.8396, time 4776.96ms, mfu 0.00%\r\n","step 1170: train loss 1.9498, val loss 2.0234\r\n","iter 1170: loss 1.8732, time 5246.71ms, mfu 0.00%\r\n","step 1171: train loss 1.9511, val loss 2.0284\r\n","iter 1171: loss 1.9570, time 4939.97ms, mfu 0.00%\r\n","step 1172: train loss 1.9471, val loss 2.0351\r\n","iter 1172: loss 1.9803, time 4743.07ms, mfu 0.00%\r\n","step 1173: train loss 1.9480, val loss 2.0387\r\n","iter 1173: loss 1.8971, time 4754.51ms, mfu 0.00%\r\n","step 1174: train loss 1.9548, val loss 2.0381\r\n","iter 1174: loss 1.9692, time 4775.85ms, mfu 0.00%\r\n","step 1175: train loss 1.9522, val loss 2.0320\r\n","iter 1175: loss 1.8675, time 4732.29ms, mfu 0.00%\r\n","step 1176: train loss 1.9487, val loss 2.0333\r\n","iter 1176: loss 1.8923, time 4889.30ms, mfu 0.00%\r\n","step 1177: train loss 1.9507, val loss 2.0264\r\n","iter 1177: loss 1.9773, time 5093.00ms, mfu 0.00%\r\n","step 1178: train loss 1.9519, val loss 2.0320\r\n","iter 1178: loss 1.9497, time 4629.25ms, mfu 0.00%\r\n","step 1179: train loss 1.9408, val loss 2.0280\r\n","iter 1179: loss 1.9695, time 4592.56ms, mfu 0.00%\r\n","step 1180: train loss 1.9390, val loss 2.0315\r\n","iter 1180: loss 2.0446, time 4461.31ms, mfu 0.00%\r\n","step 1181: train loss 1.9402, val loss 2.0261\r\n","iter 1181: loss 1.8711, time 4372.63ms, mfu 0.00%\r\n","step 1182: train loss 1.9456, val loss 2.0281\r\n","iter 1182: loss 1.9768, time 4419.86ms, mfu 0.00%\r\n","step 1183: train loss 1.9458, val loss 2.0229\r\n","iter 1183: loss 2.0186, time 4432.14ms, mfu 0.00%\r\n","step 1184: train loss 1.9454, val loss 2.0387\r\n","iter 1184: loss 1.8611, time 4905.04ms, mfu 0.00%\r\n","step 1185: train loss 1.9593, val loss 2.0303\r\n","iter 1185: loss 2.0187, time 4315.73ms, mfu 0.00%\r\n","step 1186: train loss 1.9535, val loss 2.0269\r\n","iter 1186: loss 2.0069, time 4350.94ms, mfu 0.00%\r\n","step 1187: train loss 1.9511, val loss 2.0246\r\n","iter 1187: loss 1.9255, time 4367.51ms, mfu 0.00%\r\n","step 1188: train loss 1.9411, val loss 2.0253\r\n","iter 1188: loss 1.9956, time 4631.98ms, mfu 0.00%\r\n","step 1189: train loss 1.9341, val loss 2.0189\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 1189: loss 1.9300, time 4332.64ms, mfu 0.00%\r\n","step 1190: train loss 1.9332, val loss 2.0135\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 1190: loss 2.0130, time 4390.58ms, mfu 0.00%\r\n","step 1191: train loss 1.9371, val loss 2.0132\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 1191: loss 1.8804, time 4998.86ms, mfu 0.00%\r\n","step 1192: train loss 1.9424, val loss 2.0211\r\n","iter 1192: loss 1.9404, time 4356.41ms, mfu 0.00%\r\n","step 1193: train loss 1.9504, val loss 2.0282\r\n","iter 1193: loss 2.0541, time 4443.99ms, mfu 0.00%\r\n","step 1194: train loss 1.9415, val loss 2.0207\r\n","iter 1194: loss 1.9156, time 4322.98ms, mfu 0.00%\r\n","step 1195: train loss 1.9437, val loss 2.0257\r\n","iter 1195: loss 1.9316, time 4547.03ms, mfu 0.00%\r\n","step 1196: train loss 1.9433, val loss 2.0208\r\n","iter 1196: loss 1.9412, time 4407.31ms, mfu 0.00%\r\n","step 1197: train loss 1.9422, val loss 2.0290\r\n","iter 1197: loss 1.9575, time 4408.78ms, mfu 0.00%\r\n","step 1198: train loss 1.9508, val loss 2.0199\r\n","iter 1198: loss 1.9895, time 5205.58ms, mfu 0.00%\r\n","step 1199: train loss 1.9433, val loss 2.0141\r\n","iter 1199: loss 1.9108, time 4550.98ms, mfu 0.00%\r\n","step 1200: train loss 1.9376, val loss 2.0170\r\n","iter 1200: loss 1.9999, time 4415.75ms, mfu 0.00%\r\n","step 1201: train loss 1.9361, val loss 2.0057\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 1201: loss 1.9453, time 4583.68ms, mfu 0.00%\r\n","step 1202: train loss 1.9386, val loss 2.0215\r\n","iter 1202: loss 1.9015, time 4403.28ms, mfu 0.00%\r\n","step 1203: train loss 1.9311, val loss 2.0231\r\n","iter 1203: loss 1.9022, time 4428.01ms, mfu 0.00%\r\n","step 1204: train loss 1.9378, val loss 2.0277\r\n","iter 1204: loss 1.9734, time 4540.28ms, mfu 0.00%\r\n","step 1205: train loss 1.9374, val loss 2.0142\r\n","iter 1205: loss 1.9164, time 5105.09ms, mfu 0.00%\r\n","step 1206: train loss 1.9280, val loss 2.0187\r\n","iter 1206: loss 1.8596, time 4759.87ms, mfu 0.00%\r\n","step 1207: train loss 1.9337, val loss 2.0226\r\n","iter 1207: loss 1.9383, time 4359.13ms, mfu 0.00%\r\n","step 1208: train loss 1.9451, val loss 2.0314\r\n","iter 1208: loss 1.8537, time 4460.62ms, mfu 0.00%\r\n","step 1209: train loss 1.9368, val loss 2.0385\r\n","iter 1209: loss 1.9651, time 4503.60ms, mfu 0.00%\r\n","step 1210: train loss 1.9427, val loss 2.0317\r\n","iter 1210: loss 2.0016, time 4402.93ms, mfu 0.00%\r\n","step 1211: train loss 1.9448, val loss 2.0324\r\n","iter 1211: loss 1.9111, time 4441.32ms, mfu 0.00%\r\n","step 1212: train loss 1.9350, val loss 2.0297\r\n","iter 1212: loss 1.9280, time 5231.30ms, mfu 0.00%\r\n","step 1213: train loss 1.9445, val loss 2.0233\r\n","iter 1213: loss 1.9673, time 4446.15ms, mfu 0.00%\r\n","step 1214: train loss 1.9418, val loss 2.0275\r\n","iter 1214: loss 1.9558, time 4491.74ms, mfu 0.00%\r\n","step 1215: train loss 1.9375, val loss 2.0342\r\n","iter 1215: loss 2.0133, time 4465.34ms, mfu 0.00%\r\n","step 1216: train loss 1.9380, val loss 2.0279\r\n","iter 1216: loss 2.0222, time 4464.07ms, mfu 0.00%\r\n","step 1217: train loss 1.9346, val loss 2.0227\r\n","iter 1217: loss 1.9084, time 4493.41ms, mfu 0.00%\r\n","step 1218: train loss 1.9356, val loss 2.0286\r\n","iter 1218: loss 1.9957, time 4389.14ms, mfu 0.00%\r\n","step 1219: train loss 1.9277, val loss 2.0291\r\n","iter 1219: loss 1.8735, time 5139.02ms, mfu 0.00%\r\n","step 1220: train loss 1.9319, val loss 2.0216\r\n","iter 1220: loss 1.9021, time 4577.47ms, mfu 0.00%\r\n","step 1221: train loss 1.9349, val loss 2.0232\r\n","iter 1221: loss 1.8936, time 4608.18ms, mfu 0.00%\r\n","step 1222: train loss 1.9388, val loss 2.0127\r\n","iter 1222: loss 2.0473, time 4478.20ms, mfu 0.00%\r\n","step 1223: train loss 1.9275, val loss 2.0170\r\n","iter 1223: loss 1.9846, time 4590.93ms, mfu 0.00%\r\n","step 1224: train loss 1.9267, val loss 2.0186\r\n","iter 1224: loss 1.8924, time 4592.03ms, mfu 0.00%\r\n","step 1225: train loss 1.9265, val loss 2.0198\r\n","iter 1225: loss 1.9221, time 4585.46ms, mfu 0.00%\r\n","step 1226: train loss 1.9302, val loss 2.0110\r\n","iter 1226: loss 2.0200, time 5003.28ms, mfu 0.00%\r\n","step 1227: train loss 1.9225, val loss 2.0122\r\n","iter 1227: loss 1.9832, time 4573.23ms, mfu 0.00%\r\n","step 1228: train loss 1.9293, val loss 2.0015\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 1228: loss 1.9453, time 4527.76ms, mfu 0.00%\r\n","step 1229: train loss 1.9273, val loss 2.0178\r\n","iter 1229: loss 1.9816, time 4508.22ms, mfu 0.00%\r\n","step 1230: train loss 1.9275, val loss 2.0157\r\n","iter 1230: loss 1.9133, time 4478.30ms, mfu 0.00%\r\n","step 1231: train loss 1.9243, val loss 2.0275\r\n","iter 1231: loss 1.9319, time 4426.46ms, mfu 0.00%\r\n","step 1232: train loss 1.9282, val loss 2.0147\r\n","iter 1232: loss 1.9140, time 4591.94ms, mfu 0.00%\r\n","step 1233: train loss 1.9308, val loss 2.0183\r\n","iter 1233: loss 2.0494, time 4991.64ms, mfu 0.00%\r\n","step 1234: train loss 1.9222, val loss 2.0107\r\n","iter 1234: loss 1.8637, time 4667.88ms, mfu 0.00%\r\n","step 1235: train loss 1.9228, val loss 2.0078\r\n","iter 1235: loss 1.8721, time 4431.86ms, mfu 0.00%\r\n","step 1236: train loss 1.9127, val loss 2.0047\r\n","iter 1236: loss 1.9307, time 4540.80ms, mfu 0.00%\r\n","step 1237: train loss 1.9221, val loss 1.9990\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 1237: loss 2.0151, time 4514.20ms, mfu 0.00%\r\n","step 1238: train loss 1.9183, val loss 2.0040\r\n","iter 1238: loss 2.0104, time 4465.24ms, mfu 0.00%\r\n","step 1239: train loss 1.9195, val loss 2.0112\r\n","iter 1239: loss 1.9114, time 4486.97ms, mfu 0.00%\r\n","step 1240: train loss 1.9197, val loss 1.9994\r\n","iter 1240: loss 1.9252, time 5108.15ms, mfu 0.00%\r\n","step 1241: train loss 1.9169, val loss 1.9983\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 1241: loss 1.9891, time 4722.78ms, mfu 0.00%\r\n","step 1242: train loss 1.9149, val loss 1.9985\r\n","iter 1242: loss 1.9640, time 4403.13ms, mfu 0.00%\r\n","step 1243: train loss 1.9208, val loss 2.0106\r\n","iter 1243: loss 1.9735, time 4440.81ms, mfu 0.00%\r\n","step 1244: train loss 1.9173, val loss 2.0029\r\n","iter 1244: loss 1.9179, time 4513.46ms, mfu 0.00%\r\n","step 1245: train loss 1.9146, val loss 2.0101\r\n","iter 1245: loss 1.9524, time 4490.21ms, mfu 0.00%\r\n","step 1246: train loss 1.9187, val loss 2.0075\r\n","iter 1246: loss 1.8562, time 4409.11ms, mfu 0.00%\r\n","step 1247: train loss 1.9069, val loss 2.0037\r\n","iter 1247: loss 1.9211, time 4994.07ms, mfu 0.00%\r\n","step 1248: train loss 1.9247, val loss 2.0088\r\n","iter 1248: loss 1.9550, time 4548.64ms, mfu 0.00%\r\n","step 1249: train loss 1.9221, val loss 2.0122\r\n","iter 1249: loss 2.0045, time 4442.93ms, mfu 0.00%\r\n","step 1250: train loss 1.9265, val loss 2.0163\r\n","iter 1250: loss 1.9945, time 4484.95ms, mfu 0.00%\r\n","step 1251: train loss 1.9237, val loss 2.0063\r\n","iter 1251: loss 1.9676, time 4465.16ms, mfu 0.00%\r\n","step 1252: train loss 1.9206, val loss 2.0061\r\n","iter 1252: loss 1.9393, time 4455.22ms, mfu 0.00%\r\n","step 1253: train loss 1.9224, val loss 2.0218\r\n","iter 1253: loss 1.8852, time 4496.25ms, mfu 0.00%\r\n","step 1254: train loss 1.9250, val loss 2.0112\r\n","iter 1254: loss 1.8804, time 4691.22ms, mfu 0.00%\r\n","step 1255: train loss 1.9289, val loss 2.0108\r\n","iter 1255: loss 1.9129, time 4955.02ms, mfu 0.00%\r\n","step 1256: train loss 1.9243, val loss 2.0080\r\n","iter 1256: loss 1.8690, time 4422.15ms, mfu 0.00%\r\n","step 1257: train loss 1.9222, val loss 2.0076\r\n","iter 1257: loss 1.9387, time 4437.96ms, mfu 0.00%\r\n","step 1258: train loss 1.9210, val loss 2.0025\r\n","iter 1258: loss 1.9230, time 4605.44ms, mfu 0.00%\r\n","step 1259: train loss 1.9171, val loss 1.9938\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 1259: loss 1.8041, time 4591.90ms, mfu 0.00%\r\n","step 1260: train loss 1.9256, val loss 2.0023\r\n","iter 1260: loss 1.8992, time 4503.43ms, mfu 0.00%\r\n","step 1261: train loss 1.9160, val loss 2.0175\r\n","iter 1261: loss 1.8611, time 4785.53ms, mfu 0.00%\r\n","step 1262: train loss 1.9172, val loss 2.0193\r\n","iter 1262: loss 1.9059, time 5141.23ms, mfu 0.00%\r\n","step 1263: train loss 1.9219, val loss 2.0161\r\n","iter 1263: loss 1.8844, time 4508.06ms, mfu 0.00%\r\n","step 1264: train loss 1.9263, val loss 2.0135\r\n","iter 1264: loss 2.0544, time 4714.29ms, mfu 0.00%\r\n","step 1265: train loss 1.9199, val loss 2.0107\r\n","iter 1265: loss 1.9350, time 4471.85ms, mfu 0.00%\r\n","step 1266: train loss 1.9204, val loss 2.0060\r\n","iter 1266: loss 2.0199, time 4532.28ms, mfu 0.00%\r\n","step 1267: train loss 1.9158, val loss 2.0058\r\n","iter 1267: loss 1.9275, time 4526.61ms, mfu 0.00%\r\n","step 1268: train loss 1.9163, val loss 1.9996\r\n","iter 1268: loss 1.8793, time 4424.60ms, mfu 0.00%\r\n","step 1269: train loss 1.9204, val loss 2.0029\r\n","iter 1269: loss 1.9075, time 5103.87ms, mfu 0.00%\r\n","step 1270: train loss 1.9091, val loss 2.0060\r\n","iter 1270: loss 1.9627, time 4361.49ms, mfu 0.00%\r\n","step 1271: train loss 1.9124, val loss 2.0023\r\n","iter 1271: loss 1.8595, time 4632.92ms, mfu 0.00%\r\n","step 1272: train loss 1.9164, val loss 2.0015\r\n","iter 1272: loss 1.8866, time 4526.59ms, mfu 0.00%\r\n","step 1273: train loss 1.9159, val loss 2.0037\r\n","iter 1273: loss 1.9824, time 4467.24ms, mfu 0.00%\r\n","step 1274: train loss 1.9095, val loss 2.0163\r\n","iter 1274: loss 1.9988, time 4492.76ms, mfu 0.00%\r\n","step 1275: train loss 1.9137, val loss 2.0049\r\n","iter 1275: loss 1.9861, time 4485.50ms, mfu 0.00%\r\n","step 1276: train loss 1.9140, val loss 2.0003\r\n","iter 1276: loss 1.9379, time 5052.18ms, mfu 0.00%\r\n","step 1277: train loss 1.9021, val loss 2.0112\r\n","iter 1277: loss 1.8540, time 4591.51ms, mfu 0.00%\r\n","step 1278: train loss 1.9076, val loss 2.0009\r\n","iter 1278: loss 1.8404, time 4460.13ms, mfu 0.00%\r\n","step 1279: train loss 1.8992, val loss 2.0037\r\n","iter 1279: loss 2.0574, time 4453.64ms, mfu 0.00%\r\n","step 1280: train loss 1.9159, val loss 1.9993\r\n","iter 1280: loss 1.8711, time 4459.36ms, mfu 0.00%\r\n","step 1281: train loss 1.9052, val loss 2.0086\r\n","iter 1281: loss 1.8858, time 4446.96ms, mfu 0.00%\r\n","step 1282: train loss 1.9111, val loss 1.9984\r\n","iter 1282: loss 1.9179, time 4413.68ms, mfu 0.00%\r\n","step 1283: train loss 1.9053, val loss 2.0016\r\n","iter 1283: loss 1.8547, time 5104.65ms, mfu 0.00%\r\n","step 1284: train loss 1.9055, val loss 1.9989\r\n","iter 1284: loss 1.8539, time 4459.25ms, mfu 0.00%\r\n","step 1285: train loss 1.8993, val loss 2.0036\r\n","iter 1285: loss 1.8995, time 4423.92ms, mfu 0.00%\r\n","step 1286: train loss 1.9094, val loss 1.9991\r\n","iter 1286: loss 1.9759, time 4432.27ms, mfu 0.00%\r\n","step 1287: train loss 1.9114, val loss 1.9865\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 1287: loss 1.8910, time 4582.17ms, mfu 0.00%\r\n","step 1288: train loss 1.9077, val loss 1.9995\r\n","iter 1288: loss 1.9217, time 4522.80ms, mfu 0.00%\r\n","step 1289: train loss 1.9065, val loss 2.0036\r\n","iter 1289: loss 1.9188, time 4470.62ms, mfu 0.00%\r\n","step 1290: train loss 1.9146, val loss 2.0062\r\n","iter 1290: loss 1.9552, time 5001.98ms, mfu 0.00%\r\n","step 1291: train loss 1.9117, val loss 2.0117\r\n","iter 1291: loss 1.9291, time 4430.07ms, mfu 0.00%\r\n","step 1292: train loss 1.9126, val loss 2.0119\r\n","iter 1292: loss 1.9541, time 4401.17ms, mfu 0.00%\r\n","step 1293: train loss 1.9172, val loss 2.0037\r\n","iter 1293: loss 1.9867, time 4429.23ms, mfu 0.00%\r\n","step 1294: train loss 1.9072, val loss 2.0045\r\n","iter 1294: loss 1.9799, time 4352.14ms, mfu 0.00%\r\n","step 1295: train loss 1.9114, val loss 2.0188\r\n","iter 1295: loss 1.9651, time 4653.76ms, mfu 0.00%\r\n","step 1296: train loss 1.9103, val loss 2.0034\r\n","iter 1296: loss 1.8925, time 4453.59ms, mfu 0.00%\r\n","step 1297: train loss 1.9039, val loss 2.0060\r\n","iter 1297: loss 1.8686, time 5196.28ms, mfu 0.00%\r\n","step 1298: train loss 1.9054, val loss 2.0044\r\n","iter 1298: loss 1.8167, time 4405.17ms, mfu 0.00%\r\n","step 1299: train loss 1.9136, val loss 2.0035\r\n","iter 1299: loss 1.9502, time 4524.36ms, mfu 0.00%\r\n","step 1300: train loss 1.9043, val loss 2.0153\r\n","iter 1300: loss 1.9784, time 4447.51ms, mfu 0.00%\r\n","step 1301: train loss 1.9162, val loss 2.0122\r\n","iter 1301: loss 1.8929, time 4288.00ms, mfu 0.00%\r\n","step 1302: train loss 1.9102, val loss 2.0002\r\n","iter 1302: loss 1.9051, time 4483.44ms, mfu 0.00%\r\n","step 1303: train loss 1.9059, val loss 1.9967\r\n","iter 1303: loss 1.8050, time 4460.64ms, mfu 0.00%\r\n","step 1304: train loss 1.9097, val loss 1.9931\r\n","iter 1304: loss 1.8202, time 5040.74ms, mfu 0.00%\r\n","step 1305: train loss 1.9052, val loss 1.9856\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 1305: loss 1.8921, time 4467.72ms, mfu 0.00%\r\n","step 1306: train loss 1.9060, val loss 1.9909\r\n","iter 1306: loss 1.9743, time 4475.86ms, mfu 0.00%\r\n","step 1307: train loss 1.9139, val loss 1.9926\r\n","iter 1307: loss 1.9077, time 4538.64ms, mfu 0.00%\r\n","step 1308: train loss 1.9018, val loss 2.0028\r\n","iter 1308: loss 1.8150, time 4501.89ms, mfu 0.00%\r\n","step 1309: train loss 1.9018, val loss 1.9972\r\n","iter 1309: loss 1.9748, time 4385.10ms, mfu 0.00%\r\n","step 1310: train loss 1.8981, val loss 1.9991\r\n","iter 1310: loss 1.8481, time 4454.33ms, mfu 0.00%\r\n","step 1311: train loss 1.9017, val loss 1.9929\r\n","iter 1311: loss 1.8857, time 5214.82ms, mfu 0.00%\r\n","step 1312: train loss 1.8949, val loss 1.9940\r\n","iter 1312: loss 1.8736, time 4462.95ms, mfu 0.00%\r\n","step 1313: train loss 1.9011, val loss 1.9956\r\n","iter 1313: loss 1.8387, time 4592.35ms, mfu 0.00%\r\n","step 1314: train loss 1.8965, val loss 1.9999\r\n","iter 1314: loss 1.8835, time 4413.25ms, mfu 0.00%\r\n","step 1315: train loss 1.8918, val loss 2.0026\r\n","iter 1315: loss 1.9106, time 4627.47ms, mfu 0.00%\r\n","step 1316: train loss 1.8987, val loss 2.0013\r\n","iter 1316: loss 1.9010, time 4413.28ms, mfu 0.00%\r\n","step 1317: train loss 1.8960, val loss 1.9890\r\n","iter 1317: loss 1.9433, time 4401.95ms, mfu 0.00%\r\n","step 1318: train loss 1.8904, val loss 1.9940\r\n","iter 1318: loss 1.8019, time 5084.10ms, mfu 0.00%\r\n","step 1319: train loss 1.8938, val loss 1.9978\r\n","iter 1319: loss 1.8612, time 4530.94ms, mfu 0.00%\r\n","step 1320: train loss 1.8954, val loss 1.9963\r\n","iter 1320: loss 1.9265, time 4500.85ms, mfu 0.00%\r\n","step 1321: train loss 1.8908, val loss 1.9979\r\n","iter 1321: loss 1.9446, time 4679.17ms, mfu 0.00%\r\n","step 1322: train loss 1.8905, val loss 1.9962\r\n","iter 1322: loss 1.8340, time 4554.70ms, mfu 0.00%\r\n","step 1323: train loss 1.8891, val loss 1.9993\r\n","iter 1323: loss 1.7567, time 4450.85ms, mfu 0.00%\r\n","step 1324: train loss 1.8935, val loss 1.9866\r\n","iter 1324: loss 1.8626, time 4582.79ms, mfu 0.00%\r\n","step 1325: train loss 1.8944, val loss 2.0044\r\n","iter 1325: loss 1.8855, time 5147.75ms, mfu 0.00%\r\n","step 1326: train loss 1.8857, val loss 1.9947\r\n","iter 1326: loss 1.8706, time 4547.57ms, mfu 0.00%\r\n","step 1327: train loss 1.8964, val loss 2.0072\r\n","iter 1327: loss 1.9653, time 4421.16ms, mfu 0.00%\r\n","step 1328: train loss 1.8941, val loss 2.0043\r\n","iter 1328: loss 1.9077, time 4540.67ms, mfu 0.00%\r\n","step 1329: train loss 1.8882, val loss 1.9983\r\n","iter 1329: loss 1.8170, time 4605.44ms, mfu 0.00%\r\n","step 1330: train loss 1.8949, val loss 2.0093\r\n","iter 1330: loss 1.8650, time 4609.51ms, mfu 0.00%\r\n","step 1331: train loss 1.8971, val loss 2.0128\r\n","iter 1331: loss 1.9555, time 4516.85ms, mfu 0.00%\r\n","step 1332: train loss 1.9077, val loss 2.0112\r\n","iter 1332: loss 2.0086, time 5275.20ms, mfu 0.00%\r\n","step 1333: train loss 1.8929, val loss 2.0093\r\n","iter 1333: loss 1.8855, time 4558.70ms, mfu 0.00%\r\n","step 1334: train loss 1.8941, val loss 2.0011\r\n","iter 1334: loss 1.8376, time 4619.73ms, mfu 0.00%\r\n","step 1335: train loss 1.8966, val loss 1.9960\r\n","iter 1335: loss 1.7858, time 4486.82ms, mfu 0.00%\r\n","step 1336: train loss 1.8913, val loss 2.0015\r\n","iter 1336: loss 1.8074, time 4558.24ms, mfu 0.00%\r\n","step 1337: train loss 1.8965, val loss 1.9997\r\n","iter 1337: loss 1.8590, time 4661.56ms, mfu 0.00%\r\n","step 1338: train loss 1.8980, val loss 2.0085\r\n","iter 1338: loss 1.9035, time 4471.95ms, mfu 0.00%\r\n","step 1339: train loss 1.9013, val loss 1.9981\r\n","iter 1339: loss 1.8599, time 5399.62ms, mfu 0.00%\r\n","step 1340: train loss 1.8931, val loss 2.0020\r\n","iter 1340: loss 1.9243, time 4636.97ms, mfu 0.00%\r\n","step 1341: train loss 1.9002, val loss 2.0036\r\n","iter 1341: loss 1.9581, time 4567.60ms, mfu 0.00%\r\n","step 1342: train loss 1.8902, val loss 1.9974\r\n","iter 1342: loss 1.9530, time 4588.62ms, mfu 0.00%\r\n","step 1343: train loss 1.8936, val loss 1.9915\r\n","iter 1343: loss 1.8768, time 4612.07ms, mfu 0.00%\r\n","step 1344: train loss 1.8955, val loss 1.9913\r\n","iter 1344: loss 1.8830, time 4525.39ms, mfu 0.00%\r\n","step 1345: train loss 1.8923, val loss 1.9934\r\n","iter 1345: loss 1.7893, time 4589.23ms, mfu 0.00%\r\n","step 1346: train loss 1.8947, val loss 1.9900\r\n","iter 1346: loss 1.8668, time 5201.53ms, mfu 0.00%\r\n","step 1347: train loss 1.8963, val loss 1.9911\r\n","iter 1347: loss 1.8687, time 4630.58ms, mfu 0.00%\r\n","step 1348: train loss 1.8968, val loss 1.9904\r\n","iter 1348: loss 1.8908, time 4631.57ms, mfu 0.00%\r\n","step 1349: train loss 1.8913, val loss 1.9878\r\n","iter 1349: loss 1.9195, time 4644.88ms, mfu 0.00%\r\n","step 1350: train loss 1.8921, val loss 1.9935\r\n","iter 1350: loss 1.9288, time 4469.69ms, mfu 0.00%\r\n","step 1351: train loss 1.8868, val loss 1.9964\r\n","iter 1351: loss 1.8373, time 4408.76ms, mfu 0.00%\r\n","step 1352: train loss 1.8890, val loss 1.9902\r\n","iter 1352: loss 1.8918, time 4499.22ms, mfu 0.00%\r\n","step 1353: train loss 1.8949, val loss 1.9928\r\n","iter 1353: loss 1.8128, time 5288.43ms, mfu 0.00%\r\n","step 1354: train loss 1.8981, val loss 1.9971\r\n","iter 1354: loss 1.9015, time 4622.95ms, mfu 0.00%\r\n","step 1355: train loss 1.8953, val loss 1.9956\r\n","iter 1355: loss 1.9902, time 4608.22ms, mfu 0.00%\r\n","step 1356: train loss 1.8952, val loss 2.0060\r\n","iter 1356: loss 1.8850, time 4779.30ms, mfu 0.00%\r\n","step 1357: train loss 1.8879, val loss 1.9985\r\n","iter 1357: loss 1.8131, time 4531.12ms, mfu 0.00%\r\n","step 1358: train loss 1.8916, val loss 1.9971\r\n","iter 1358: loss 1.8698, time 4527.76ms, mfu 0.00%\r\n","step 1359: train loss 1.8901, val loss 1.9919\r\n","iter 1359: loss 1.8428, time 4623.19ms, mfu 0.00%\r\n","step 1360: train loss 1.8878, val loss 2.0047\r\n","iter 1360: loss 1.8625, time 5284.36ms, mfu 0.00%\r\n","step 1361: train loss 1.8949, val loss 2.0071\r\n","iter 1361: loss 1.8807, time 4658.95ms, mfu 0.00%\r\n","step 1362: train loss 1.8956, val loss 2.0037\r\n","iter 1362: loss 1.8544, time 4718.30ms, mfu 0.00%\r\n","step 1363: train loss 1.8923, val loss 2.0011\r\n","iter 1363: loss 1.8922, time 4729.54ms, mfu 0.00%\r\n","step 1364: train loss 1.8842, val loss 2.0073\r\n","iter 1364: loss 1.9092, time 5024.50ms, mfu 0.00%\r\n","step 1365: train loss 1.8895, val loss 2.0066\r\n","iter 1365: loss 1.8835, time 4863.80ms, mfu 0.00%\r\n","step 1366: train loss 1.8901, val loss 2.0073\r\n","iter 1366: loss 1.8716, time 4839.43ms, mfu 0.00%\r\n","step 1367: train loss 1.8817, val loss 1.9941\r\n","iter 1367: loss 1.9026, time 5489.62ms, mfu 0.00%\r\n","step 1368: train loss 1.8832, val loss 1.9989\r\n","iter 1368: loss 1.8530, time 4661.60ms, mfu 0.00%\r\n","step 1369: train loss 1.8754, val loss 1.9960\r\n","iter 1369: loss 1.8218, time 4800.58ms, mfu 0.00%\r\n","step 1370: train loss 1.8814, val loss 1.9935\r\n","iter 1370: loss 1.8492, time 4648.65ms, mfu 0.00%\r\n","step 1371: train loss 1.8824, val loss 1.9961\r\n","iter 1371: loss 1.9131, time 4353.96ms, mfu 0.00%\r\n","step 1372: train loss 1.8822, val loss 1.9884\r\n","iter 1372: loss 1.9104, time 4553.67ms, mfu 0.00%\r\n","step 1373: train loss 1.8885, val loss 1.9882\r\n","iter 1373: loss 2.0400, time 4549.42ms, mfu 0.00%\r\n","step 1374: train loss 1.8849, val loss 1.9908\r\n","iter 1374: loss 1.8756, time 5184.85ms, mfu 0.00%\r\n","step 1375: train loss 1.8846, val loss 1.9890\r\n","iter 1375: loss 1.8853, time 4898.96ms, mfu 0.00%\r\n","step 1376: train loss 1.8774, val loss 1.9866\r\n","iter 1376: loss 1.7883, time 4833.63ms, mfu 0.00%\r\n","step 1377: train loss 1.8863, val loss 1.9857\r\n","iter 1377: loss 1.8605, time 4772.58ms, mfu 0.00%\r\n","step 1378: train loss 1.8786, val loss 1.9789\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 1378: loss 1.9397, time 4567.12ms, mfu 0.00%\r\n","step 1379: train loss 1.8812, val loss 1.9856\r\n","iter 1379: loss 1.8715, time 4626.95ms, mfu 0.00%\r\n","step 1380: train loss 1.8799, val loss 1.9854\r\n","iter 1380: loss 1.7288, time 5030.50ms, mfu 0.00%\r\n","step 1381: train loss 1.8745, val loss 1.9894\r\n","iter 1381: loss 1.8072, time 4864.30ms, mfu 0.00%\r\n","step 1382: train loss 1.8755, val loss 1.9779\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 1382: loss 1.8894, time 4601.31ms, mfu 0.00%\r\n","step 1383: train loss 1.8799, val loss 1.9836\r\n","iter 1383: loss 1.8589, time 4717.70ms, mfu 0.00%\r\n","step 1384: train loss 1.8821, val loss 1.9770\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 1384: loss 1.7848, time 4601.58ms, mfu 0.00%\r\n","step 1385: train loss 1.8807, val loss 1.9786\r\n","iter 1385: loss 1.9296, time 4652.29ms, mfu 0.00%\r\n","step 1386: train loss 1.8758, val loss 1.9865\r\n","iter 1386: loss 1.8316, time 4512.03ms, mfu 0.00%\r\n","step 1387: train loss 1.8806, val loss 1.9823\r\n","iter 1387: loss 1.8516, time 5233.23ms, mfu 0.00%\r\n","step 1388: train loss 1.8883, val loss 1.9799\r\n","iter 1388: loss 1.8623, time 4696.19ms, mfu 0.00%\r\n","step 1389: train loss 1.8765, val loss 1.9850\r\n","iter 1389: loss 1.7768, time 4638.36ms, mfu 0.00%\r\n","step 1390: train loss 1.8809, val loss 1.9815\r\n","iter 1390: loss 1.8844, time 4804.40ms, mfu 0.00%\r\n","step 1391: train loss 1.8807, val loss 1.9774\r\n","iter 1391: loss 1.9050, time 4666.27ms, mfu 0.00%\r\n","step 1392: train loss 1.8688, val loss 1.9857\r\n","iter 1392: loss 1.9084, time 4722.08ms, mfu 0.00%\r\n","step 1393: train loss 1.8838, val loss 1.9786\r\n","iter 1393: loss 1.7931, time 4776.64ms, mfu 0.00%\r\n","step 1394: train loss 1.8771, val loss 1.9761\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 1394: loss 1.9390, time 5497.06ms, mfu 0.00%\r\n","step 1395: train loss 1.8747, val loss 1.9897\r\n","iter 1395: loss 1.9436, time 4792.42ms, mfu 0.00%\r\n","step 1396: train loss 1.8758, val loss 1.9753\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 1396: loss 1.8255, time 4742.89ms, mfu 0.00%\r\n","step 1397: train loss 1.8758, val loss 1.9737\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 1397: loss 1.8717, time 4495.13ms, mfu 0.00%\r\n","step 1398: train loss 1.8818, val loss 1.9774\r\n","iter 1398: loss 1.8603, time 4697.69ms, mfu 0.00%\r\n","step 1399: train loss 1.8778, val loss 1.9782\r\n","iter 1399: loss 1.9027, time 4697.24ms, mfu 0.00%\r\n","step 1400: train loss 1.8712, val loss 1.9770\r\n","iter 1400: loss 1.8203, time 4691.87ms, mfu 0.00%\r\n","step 1401: train loss 1.8788, val loss 1.9880\r\n","iter 1401: loss 1.8358, time 5421.34ms, mfu 0.00%\r\n","step 1402: train loss 1.8752, val loss 1.9786\r\n","iter 1402: loss 2.0269, time 4794.58ms, mfu 0.00%\r\n","step 1403: train loss 1.8794, val loss 1.9803\r\n","iter 1403: loss 1.7995, time 4586.58ms, mfu 0.00%\r\n","step 1404: train loss 1.8834, val loss 1.9881\r\n","iter 1404: loss 1.8443, time 4596.83ms, mfu 0.00%\r\n","step 1405: train loss 1.8896, val loss 1.9853\r\n","iter 1405: loss 1.8673, time 4415.07ms, mfu 0.00%\r\n","step 1406: train loss 1.8848, val loss 1.9953\r\n","iter 1406: loss 1.9772, time 4544.30ms, mfu 0.00%\r\n","step 1407: train loss 1.8820, val loss 1.9830\r\n","iter 1407: loss 1.9389, time 4520.70ms, mfu 0.00%\r\n","step 1408: train loss 1.8770, val loss 1.9794\r\n","iter 1408: loss 1.8662, time 5304.22ms, mfu 0.00%\r\n","step 1409: train loss 1.8790, val loss 1.9782\r\n","iter 1409: loss 1.9093, time 4591.36ms, mfu 0.00%\r\n","step 1410: train loss 1.8732, val loss 1.9883\r\n","iter 1410: loss 1.8488, time 4543.93ms, mfu 0.00%\r\n","step 1411: train loss 1.8758, val loss 1.9773\r\n","iter 1411: loss 1.7627, time 4505.84ms, mfu 0.00%\r\n","step 1412: train loss 1.8801, val loss 1.9805\r\n","iter 1412: loss 1.8655, time 4532.51ms, mfu 0.00%\r\n","step 1413: train loss 1.8738, val loss 1.9757\r\n","iter 1413: loss 1.8649, time 4535.75ms, mfu 0.00%\r\n","step 1414: train loss 1.8710, val loss 1.9778\r\n","iter 1414: loss 1.9009, time 4487.32ms, mfu 0.00%\r\n","step 1415: train loss 1.8697, val loss 1.9760\r\n","iter 1415: loss 1.8987, time 5266.02ms, mfu 0.00%\r\n","step 1416: train loss 1.8785, val loss 1.9759\r\n","iter 1416: loss 1.9340, time 4701.38ms, mfu 0.00%\r\n","step 1417: train loss 1.8802, val loss 1.9670\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 1417: loss 1.8151, time 4607.12ms, mfu 0.00%\r\n","step 1418: train loss 1.8710, val loss 1.9843\r\n","iter 1418: loss 1.8481, time 4531.67ms, mfu 0.00%\r\n","step 1419: train loss 1.8704, val loss 1.9686\r\n","iter 1419: loss 1.9022, time 4560.91ms, mfu 0.00%\r\n","step 1420: train loss 1.8807, val loss 1.9711\r\n","iter 1420: loss 1.8348, time 4575.24ms, mfu 0.00%\r\n","step 1421: train loss 1.8658, val loss 1.9668\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 1421: loss 1.9359, time 4681.80ms, mfu 0.00%\r\n","step 1422: train loss 1.8800, val loss 1.9753\r\n","iter 1422: loss 1.9697, time 5223.34ms, mfu 0.00%\r\n","step 1423: train loss 1.8728, val loss 1.9836\r\n","iter 1423: loss 1.7938, time 4497.47ms, mfu 0.00%\r\n","step 1424: train loss 1.8719, val loss 1.9867\r\n","iter 1424: loss 1.9287, time 4298.80ms, mfu 0.00%\r\n","step 1425: train loss 1.8726, val loss 1.9780\r\n","iter 1425: loss 1.9960, time 4482.74ms, mfu 0.00%\r\n","step 1426: train loss 1.8713, val loss 1.9777\r\n","iter 1426: loss 1.8784, time 4344.56ms, mfu 0.00%\r\n","step 1427: train loss 1.8736, val loss 1.9884\r\n","iter 1427: loss 1.8816, time 4306.96ms, mfu 0.00%\r\n","step 1428: train loss 1.8658, val loss 1.9781\r\n","iter 1428: loss 1.9366, time 4414.28ms, mfu 0.00%\r\n","step 1429: train loss 1.8703, val loss 1.9778\r\n","iter 1429: loss 1.8192, time 4863.34ms, mfu 0.00%\r\n","step 1430: train loss 1.8632, val loss 1.9750\r\n","iter 1430: loss 1.8443, time 4456.99ms, mfu 0.00%\r\n","step 1431: train loss 1.8653, val loss 1.9753\r\n","iter 1431: loss 1.8416, time 4510.14ms, mfu 0.00%\r\n","step 1432: train loss 1.8727, val loss 1.9791\r\n","iter 1432: loss 1.8207, time 4407.63ms, mfu 0.00%\r\n","step 1433: train loss 1.8703, val loss 1.9801\r\n","iter 1433: loss 1.8765, time 4333.92ms, mfu 0.00%\r\n","step 1434: train loss 1.8666, val loss 1.9851\r\n","iter 1434: loss 1.8467, time 4431.50ms, mfu 0.00%\r\n","step 1435: train loss 1.8782, val loss 1.9839\r\n","iter 1435: loss 1.8987, time 4349.61ms, mfu 0.00%\r\n","step 1436: train loss 1.8711, val loss 1.9758\r\n","iter 1436: loss 1.9400, time 4936.94ms, mfu 0.00%\r\n","step 1437: train loss 1.8744, val loss 1.9859\r\n","iter 1437: loss 1.9327, time 4568.98ms, mfu 0.00%\r\n","step 1438: train loss 1.8725, val loss 1.9846\r\n","iter 1438: loss 1.9229, time 4381.28ms, mfu 0.00%\r\n","step 1439: train loss 1.8623, val loss 1.9769\r\n","iter 1439: loss 1.8583, time 4473.65ms, mfu 0.00%\r\n","step 1440: train loss 1.8564, val loss 1.9700\r\n","iter 1440: loss 1.8929, time 4352.44ms, mfu 0.00%\r\n","step 1441: train loss 1.8613, val loss 1.9764\r\n","iter 1441: loss 1.8263, time 4375.16ms, mfu 0.00%\r\n","step 1442: train loss 1.8633, val loss 1.9793\r\n","iter 1442: loss 1.8098, time 4398.12ms, mfu 0.00%\r\n","step 1443: train loss 1.8686, val loss 1.9694\r\n","iter 1443: loss 1.8526, time 4815.36ms, mfu 0.00%\r\n","step 1444: train loss 1.8699, val loss 1.9754\r\n","iter 1444: loss 1.8159, time 4567.76ms, mfu 0.00%\r\n","step 1445: train loss 1.8671, val loss 1.9636\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 1445: loss 1.8682, time 4388.24ms, mfu 0.00%\r\n","step 1446: train loss 1.8637, val loss 1.9758\r\n","iter 1446: loss 1.8756, time 4289.61ms, mfu 0.00%\r\n","step 1447: train loss 1.8590, val loss 1.9623\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 1447: loss 1.8621, time 4343.08ms, mfu 0.00%\r\n","step 1448: train loss 1.8695, val loss 1.9704\r\n","iter 1448: loss 1.9193, time 4397.38ms, mfu 0.00%\r\n","step 1449: train loss 1.8708, val loss 1.9661\r\n","iter 1449: loss 1.7522, time 4288.02ms, mfu 0.00%\r\n","step 1450: train loss 1.8649, val loss 1.9702\r\n","iter 1450: loss 1.7985, time 4383.23ms, mfu 0.00%\r\n","step 1451: train loss 1.8670, val loss 1.9689\r\n","iter 1451: loss 1.8476, time 5098.34ms, mfu 0.00%\r\n","step 1452: train loss 1.8650, val loss 1.9555\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 1452: loss 1.7994, time 4471.26ms, mfu 0.00%\r\n","step 1453: train loss 1.8568, val loss 1.9657\r\n","iter 1453: loss 1.7737, time 4289.15ms, mfu 0.00%\r\n","step 1454: train loss 1.8605, val loss 1.9642\r\n","iter 1454: loss 1.9078, time 4378.47ms, mfu 0.00%\r\n","step 1455: train loss 1.8627, val loss 1.9658\r\n","iter 1455: loss 1.8399, time 4282.55ms, mfu 0.00%\r\n","step 1456: train loss 1.8605, val loss 1.9628\r\n","iter 1456: loss 1.9099, time 4315.66ms, mfu 0.00%\r\n","step 1457: train loss 1.8631, val loss 1.9568\r\n","iter 1457: loss 1.8522, time 4431.17ms, mfu 0.00%\r\n","step 1458: train loss 1.8662, val loss 1.9596\r\n","iter 1458: loss 1.8200, time 4982.89ms, mfu 0.00%\r\n","step 1459: train loss 1.8525, val loss 1.9641\r\n","iter 1459: loss 1.8269, time 4377.95ms, mfu 0.00%\r\n","step 1460: train loss 1.8632, val loss 1.9641\r\n","iter 1460: loss 1.8796, time 4323.80ms, mfu 0.00%\r\n","step 1461: train loss 1.8566, val loss 1.9629\r\n","iter 1461: loss 1.8282, time 4299.00ms, mfu 0.00%\r\n","step 1462: train loss 1.8551, val loss 1.9668\r\n","iter 1462: loss 1.9044, time 4393.11ms, mfu 0.00%\r\n","step 1463: train loss 1.8549, val loss 1.9631\r\n","iter 1463: loss 1.7759, time 4370.92ms, mfu 0.00%\r\n","step 1464: train loss 1.8544, val loss 1.9688\r\n","iter 1464: loss 1.8203, time 4319.17ms, mfu 0.00%\r\n","step 1465: train loss 1.8652, val loss 1.9697\r\n","iter 1465: loss 1.8612, time 4930.97ms, mfu 0.00%\r\n","step 1466: train loss 1.8566, val loss 1.9708\r\n","iter 1466: loss 1.8127, time 4502.05ms, mfu 0.00%\r\n","step 1467: train loss 1.8573, val loss 1.9737\r\n","iter 1467: loss 1.7789, time 4294.33ms, mfu 0.00%\r\n","step 1468: train loss 1.8658, val loss 1.9745\r\n","iter 1468: loss 1.9324, time 4436.32ms, mfu 0.00%\r\n","step 1469: train loss 1.8581, val loss 1.9786\r\n","iter 1469: loss 1.7308, time 4324.17ms, mfu 0.00%\r\n","step 1470: train loss 1.8543, val loss 1.9680\r\n","iter 1470: loss 1.8201, time 4489.38ms, mfu 0.00%\r\n","step 1471: train loss 1.8558, val loss 1.9700\r\n","iter 1471: loss 1.9050, time 4286.32ms, mfu 0.00%\r\n","step 1472: train loss 1.8481, val loss 1.9613\r\n","iter 1472: loss 1.7168, time 4402.41ms, mfu 0.00%\r\n","step 1473: train loss 1.8545, val loss 1.9653\r\n","iter 1473: loss 1.8698, time 5007.06ms, mfu 0.00%\r\n","step 1474: train loss 1.8569, val loss 1.9565\r\n","iter 1474: loss 1.8467, time 4302.12ms, mfu 0.00%\r\n","step 1475: train loss 1.8523, val loss 1.9674\r\n","iter 1475: loss 1.9231, time 4289.92ms, mfu 0.00%\r\n","step 1476: train loss 1.8560, val loss 1.9688\r\n","iter 1476: loss 1.8246, time 4294.31ms, mfu 0.00%\r\n","step 1477: train loss 1.8556, val loss 1.9623\r\n","iter 1477: loss 1.7842, time 4364.60ms, mfu 0.00%\r\n","step 1478: train loss 1.8548, val loss 1.9686\r\n","iter 1478: loss 1.9320, time 4298.67ms, mfu 0.00%\r\n","step 1479: train loss 1.8553, val loss 1.9633\r\n","iter 1479: loss 1.8447, time 4476.90ms, mfu 0.00%\r\n","step 1480: train loss 1.8494, val loss 1.9681\r\n","iter 1480: loss 1.8931, time 4924.23ms, mfu 0.00%\r\n","step 1481: train loss 1.8449, val loss 1.9775\r\n","iter 1481: loss 1.8269, time 4224.91ms, mfu 0.00%\r\n","step 1482: train loss 1.8585, val loss 1.9712\r\n","iter 1482: loss 1.7680, time 4418.36ms, mfu 0.00%\r\n","step 1483: train loss 1.8565, val loss 1.9657\r\n","iter 1483: loss 1.8736, time 4350.72ms, mfu 0.00%\r\n","step 1484: train loss 1.8610, val loss 1.9684\r\n","iter 1484: loss 1.7155, time 4412.44ms, mfu 0.00%\r\n","step 1485: train loss 1.8563, val loss 1.9692\r\n","iter 1485: loss 1.9263, time 4512.44ms, mfu 0.00%\r\n","step 1486: train loss 1.8655, val loss 1.9763\r\n","iter 1486: loss 1.9377, time 4639.24ms, mfu 0.00%\r\n","step 1487: train loss 1.8651, val loss 1.9716\r\n","iter 1487: loss 1.9340, time 4999.98ms, mfu 0.00%\r\n","step 1488: train loss 1.8542, val loss 1.9601\r\n","iter 1488: loss 1.7834, time 4432.55ms, mfu 0.00%\r\n","step 1489: train loss 1.8545, val loss 1.9675\r\n","iter 1489: loss 1.8499, time 4402.55ms, mfu 0.00%\r\n","step 1490: train loss 1.8570, val loss 1.9643\r\n","iter 1490: loss 1.9087, time 4514.82ms, mfu 0.00%\r\n","step 1491: train loss 1.8552, val loss 1.9631\r\n","iter 1491: loss 1.9179, time 4510.15ms, mfu 0.00%\r\n","step 1492: train loss 1.8533, val loss 1.9637\r\n","iter 1492: loss 1.7955, time 4532.66ms, mfu 0.00%\r\n","step 1493: train loss 1.8517, val loss 1.9608\r\n","iter 1493: loss 1.8512, time 4700.18ms, mfu 0.00%\r\n","step 1494: train loss 1.8417, val loss 1.9601\r\n","iter 1494: loss 1.7539, time 4985.65ms, mfu 0.00%\r\n","step 1495: train loss 1.8533, val loss 1.9610\r\n","iter 1495: loss 1.8233, time 4542.80ms, mfu 0.00%\r\n","step 1496: train loss 1.8536, val loss 1.9686\r\n","iter 1496: loss 1.9404, time 4464.16ms, mfu 0.00%\r\n","step 1497: train loss 1.8522, val loss 1.9571\r\n","iter 1497: loss 1.7924, time 4482.84ms, mfu 0.00%\r\n","step 1498: train loss 1.8536, val loss 1.9543\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 1498: loss 1.9228, time 4491.90ms, mfu 0.00%\r\n","step 1499: train loss 1.8448, val loss 1.9546\r\n","iter 1499: loss 1.8117, time 4512.21ms, mfu 0.00%\r\n","step 1500: train loss 1.8444, val loss 1.9608\r\n","iter 1500: loss 1.7957, time 4478.78ms, mfu 0.00%\r\n","step 1501: train loss 1.8459, val loss 1.9566\r\n","iter 1501: loss 1.7881, time 5095.97ms, mfu 0.00%\r\n","step 1502: train loss 1.8554, val loss 1.9586\r\n","iter 1502: loss 1.8861, time 4393.18ms, mfu 0.00%\r\n","step 1503: train loss 1.8454, val loss 1.9570\r\n","iter 1503: loss 1.8902, time 4403.36ms, mfu 0.00%\r\n","step 1504: train loss 1.8594, val loss 1.9651\r\n","iter 1504: loss 1.9029, time 4438.58ms, mfu 0.00%\r\n","step 1505: train loss 1.8571, val loss 1.9674\r\n","iter 1505: loss 1.8517, time 4399.59ms, mfu 0.00%\r\n","step 1506: train loss 1.8563, val loss 1.9618\r\n","iter 1506: loss 1.8681, time 4554.00ms, mfu 0.00%\r\n","step 1507: train loss 1.8563, val loss 1.9660\r\n","iter 1507: loss 1.9119, time 4533.82ms, mfu 0.00%\r\n","step 1508: train loss 1.8536, val loss 1.9633\r\n","iter 1508: loss 1.8495, time 4679.74ms, mfu 0.00%\r\n","step 1509: train loss 1.8494, val loss 1.9645\r\n","iter 1509: loss 1.9093, time 4708.35ms, mfu 0.00%\r\n","step 1510: train loss 1.8512, val loss 1.9598\r\n","iter 1510: loss 1.9285, time 4512.48ms, mfu 0.00%\r\n","step 1511: train loss 1.8505, val loss 1.9607\r\n","iter 1511: loss 1.9212, time 4331.91ms, mfu 0.00%\r\n","step 1512: train loss 1.8470, val loss 1.9717\r\n","iter 1512: loss 1.8396, time 4486.80ms, mfu 0.00%\r\n","step 1513: train loss 1.8539, val loss 1.9620\r\n","iter 1513: loss 1.9148, time 4447.02ms, mfu 0.00%\r\n","step 1514: train loss 1.8532, val loss 1.9597\r\n","iter 1514: loss 1.7818, time 4529.22ms, mfu 0.00%\r\n","step 1515: train loss 1.8520, val loss 1.9645\r\n","iter 1515: loss 1.9366, time 4531.78ms, mfu 0.00%\r\n","step 1516: train loss 1.8544, val loss 1.9625\r\n","iter 1516: loss 1.8971, time 4880.78ms, mfu 0.00%\r\n","step 1517: train loss 1.8460, val loss 1.9616\r\n","iter 1517: loss 1.7295, time 4423.64ms, mfu 0.00%\r\n","step 1518: train loss 1.8506, val loss 1.9567\r\n","iter 1518: loss 1.7852, time 4420.57ms, mfu 0.00%\r\n","step 1519: train loss 1.8482, val loss 1.9550\r\n","iter 1519: loss 1.9075, time 4429.64ms, mfu 0.00%\r\n","step 1520: train loss 1.8411, val loss 1.9642\r\n","iter 1520: loss 1.8132, time 4484.83ms, mfu 0.00%\r\n","step 1521: train loss 1.8431, val loss 1.9578\r\n","iter 1521: loss 1.9470, time 4545.81ms, mfu 0.00%\r\n","step 1522: train loss 1.8484, val loss 1.9476\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 1522: loss 1.8805, time 4316.60ms, mfu 0.00%\r\n","step 1523: train loss 1.8412, val loss 1.9575\r\n","iter 1523: loss 1.8660, time 4909.41ms, mfu 0.00%\r\n","step 1524: train loss 1.8453, val loss 1.9644\r\n","iter 1524: loss 1.7651, time 4523.88ms, mfu 0.00%\r\n","step 1525: train loss 1.8467, val loss 1.9570\r\n","iter 1525: loss 1.8756, time 4340.39ms, mfu 0.00%\r\n","step 1526: train loss 1.8562, val loss 1.9627\r\n","iter 1526: loss 1.8617, time 4383.47ms, mfu 0.00%\r\n","step 1527: train loss 1.8489, val loss 1.9648\r\n","iter 1527: loss 1.7304, time 4468.21ms, mfu 0.00%\r\n","step 1528: train loss 1.8472, val loss 1.9637\r\n","iter 1528: loss 1.8671, time 4469.64ms, mfu 0.00%\r\n","step 1529: train loss 1.8458, val loss 1.9721\r\n","iter 1529: loss 1.8534, time 4393.17ms, mfu 0.00%\r\n","step 1530: train loss 1.8432, val loss 1.9637\r\n","iter 1530: loss 1.8107, time 4904.91ms, mfu 0.00%\r\n","step 1531: train loss 1.8418, val loss 1.9577\r\n","iter 1531: loss 1.8918, time 4306.55ms, mfu 0.00%\r\n","step 1532: train loss 1.8417, val loss 1.9561\r\n","iter 1532: loss 1.7784, time 4459.84ms, mfu 0.00%\r\n","step 1533: train loss 1.8411, val loss 1.9539\r\n","iter 1533: loss 1.8485, time 4254.73ms, mfu 0.00%\r\n","step 1534: train loss 1.8477, val loss 1.9582\r\n","iter 1534: loss 1.9214, time 4284.18ms, mfu 0.00%\r\n","step 1535: train loss 1.8564, val loss 1.9611\r\n","iter 1535: loss 1.8900, time 4483.75ms, mfu 0.00%\r\n","step 1536: train loss 1.8536, val loss 1.9572\r\n","iter 1536: loss 1.6667, time 4342.65ms, mfu 0.00%\r\n","step 1537: train loss 1.8415, val loss 1.9637\r\n","iter 1537: loss 1.8385, time 4801.93ms, mfu 0.00%\r\n","step 1538: train loss 1.8406, val loss 1.9532\r\n","iter 1538: loss 1.8086, time 4466.18ms, mfu 0.00%\r\n","step 1539: train loss 1.8392, val loss 1.9507\r\n","iter 1539: loss 1.7834, time 4305.50ms, mfu 0.00%\r\n","step 1540: train loss 1.8403, val loss 1.9552\r\n","iter 1540: loss 1.9318, time 4282.28ms, mfu 0.00%\r\n","step 1541: train loss 1.8380, val loss 1.9434\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 1541: loss 1.7995, time 4531.54ms, mfu 0.00%\r\n","step 1542: train loss 1.8438, val loss 1.9565\r\n","iter 1542: loss 1.8071, time 4590.38ms, mfu 0.00%\r\n","step 1543: train loss 1.8303, val loss 1.9518\r\n","iter 1543: loss 1.8999, time 4448.61ms, mfu 0.00%\r\n","step 1544: train loss 1.8370, val loss 1.9416\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 1544: loss 1.7976, time 4434.74ms, mfu 0.00%\r\n","step 1545: train loss 1.8306, val loss 1.9529\r\n","iter 1545: loss 1.7722, time 4894.08ms, mfu 0.00%\r\n","step 1546: train loss 1.8354, val loss 1.9609\r\n","iter 1546: loss 1.7760, time 4538.69ms, mfu 0.00%\r\n","step 1547: train loss 1.8319, val loss 1.9515\r\n","iter 1547: loss 1.8530, time 4306.74ms, mfu 0.00%\r\n","step 1548: train loss 1.8364, val loss 1.9520\r\n","iter 1548: loss 1.8766, time 4482.81ms, mfu 0.00%\r\n","step 1549: train loss 1.8351, val loss 1.9473\r\n","iter 1549: loss 1.8122, time 4556.21ms, mfu 0.00%\r\n","step 1550: train loss 1.8359, val loss 1.9678\r\n","iter 1550: loss 1.7673, time 4430.68ms, mfu 0.00%\r\n","step 1551: train loss 1.8358, val loss 1.9604\r\n","iter 1551: loss 1.8856, time 4267.14ms, mfu 0.00%\r\n","step 1552: train loss 1.8359, val loss 1.9498\r\n","iter 1552: loss 1.7441, time 4979.96ms, mfu 0.00%\r\n","step 1553: train loss 1.8443, val loss 1.9591\r\n","iter 1553: loss 1.8998, time 4258.48ms, mfu 0.00%\r\n","step 1554: train loss 1.8431, val loss 1.9501\r\n","iter 1554: loss 1.8577, time 4260.60ms, mfu 0.00%\r\n","step 1555: train loss 1.8387, val loss 1.9505\r\n","iter 1555: loss 1.8368, time 4389.10ms, mfu 0.00%\r\n","step 1556: train loss 1.8375, val loss 1.9569\r\n","iter 1556: loss 1.8806, time 4381.91ms, mfu 0.00%\r\n","step 1557: train loss 1.8326, val loss 1.9550\r\n","iter 1557: loss 1.9063, time 4469.46ms, mfu 0.00%\r\n","step 1558: train loss 1.8434, val loss 1.9568\r\n","iter 1558: loss 1.9069, time 4350.02ms, mfu 0.00%\r\n","step 1559: train loss 1.8326, val loss 1.9581\r\n","iter 1559: loss 1.7650, time 4924.69ms, mfu 0.00%\r\n","step 1560: train loss 1.8267, val loss 1.9530\r\n","iter 1560: loss 1.9466, time 4635.01ms, mfu 0.00%\r\n","step 1561: train loss 1.8340, val loss 1.9512\r\n","iter 1561: loss 1.7976, time 4539.64ms, mfu 0.00%\r\n","step 1562: train loss 1.8454, val loss 1.9537\r\n","iter 1562: loss 1.8935, time 4409.81ms, mfu 0.00%\r\n","step 1563: train loss 1.8305, val loss 1.9595\r\n","iter 1563: loss 1.9700, time 4516.82ms, mfu 0.00%\r\n","step 1564: train loss 1.8323, val loss 1.9437\r\n","iter 1564: loss 1.7481, time 4464.08ms, mfu 0.00%\r\n","step 1565: train loss 1.8412, val loss 1.9477\r\n","iter 1565: loss 1.8982, time 4353.39ms, mfu 0.00%\r\n","step 1566: train loss 1.8399, val loss 1.9440\r\n","iter 1566: loss 1.8325, time 5156.67ms, mfu 0.00%\r\n","step 1567: train loss 1.8389, val loss 1.9451\r\n","iter 1567: loss 1.8400, time 4253.86ms, mfu 0.00%\r\n","step 1568: train loss 1.8356, val loss 1.9465\r\n","iter 1568: loss 1.8632, time 4305.42ms, mfu 0.00%\r\n","step 1569: train loss 1.8379, val loss 1.9439\r\n","iter 1569: loss 1.8058, time 4385.54ms, mfu 0.00%\r\n","step 1570: train loss 1.8389, val loss 1.9472\r\n","iter 1570: loss 1.7534, time 4457.42ms, mfu 0.00%\r\n","step 1571: train loss 1.8400, val loss 1.9437\r\n","iter 1571: loss 1.7786, time 4386.56ms, mfu 0.00%\r\n","step 1572: train loss 1.8347, val loss 1.9507\r\n","iter 1572: loss 1.8223, time 4312.40ms, mfu 0.00%\r\n","step 1573: train loss 1.8409, val loss 1.9426\r\n","iter 1573: loss 1.7721, time 4497.52ms, mfu 0.00%\r\n","step 1574: train loss 1.8386, val loss 1.9391\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 1574: loss 1.8764, time 4890.10ms, mfu 0.00%\r\n","step 1575: train loss 1.8394, val loss 1.9442\r\n","iter 1575: loss 1.8098, time 4449.12ms, mfu 0.00%\r\n","step 1576: train loss 1.8397, val loss 1.9499\r\n","iter 1576: loss 1.8122, time 4358.02ms, mfu 0.00%\r\n","step 1577: train loss 1.8387, val loss 1.9440\r\n","iter 1577: loss 1.7751, time 4512.93ms, mfu 0.00%\r\n","step 1578: train loss 1.8349, val loss 1.9550\r\n","iter 1578: loss 1.8105, time 4569.43ms, mfu 0.00%\r\n","step 1579: train loss 1.8374, val loss 1.9607\r\n","iter 1579: loss 1.7965, time 4391.52ms, mfu 0.00%\r\n","step 1580: train loss 1.8356, val loss 1.9549\r\n","iter 1580: loss 1.8383, time 4404.79ms, mfu 0.00%\r\n","step 1581: train loss 1.8336, val loss 1.9533\r\n","iter 1581: loss 2.0217, time 4998.12ms, mfu 0.00%\r\n","step 1582: train loss 1.8341, val loss 1.9462\r\n","iter 1582: loss 1.8006, time 4313.27ms, mfu 0.00%\r\n","step 1583: train loss 1.8415, val loss 1.9385\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 1583: loss 1.8777, time 4418.15ms, mfu 0.00%\r\n","step 1584: train loss 1.8254, val loss 1.9516\r\n","iter 1584: loss 1.8582, time 4488.01ms, mfu 0.00%\r\n","step 1585: train loss 1.8356, val loss 1.9456\r\n","iter 1585: loss 1.8700, time 4490.62ms, mfu 0.00%\r\n","step 1586: train loss 1.8364, val loss 1.9503\r\n","iter 1586: loss 1.9294, time 4381.75ms, mfu 0.00%\r\n","step 1587: train loss 1.8311, val loss 1.9435\r\n","iter 1587: loss 1.8635, time 4312.53ms, mfu 0.00%\r\n","step 1588: train loss 1.8334, val loss 1.9438\r\n","iter 1588: loss 1.7805, time 4913.45ms, mfu 0.00%\r\n","step 1589: train loss 1.8346, val loss 1.9458\r\n","iter 1589: loss 1.8595, time 4190.40ms, mfu 0.00%\r\n","step 1590: train loss 1.8291, val loss 1.9399\r\n","iter 1590: loss 1.8069, time 4319.76ms, mfu 0.00%\r\n","step 1591: train loss 1.8304, val loss 1.9379\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 1591: loss 1.8207, time 4489.77ms, mfu 0.00%\r\n","step 1592: train loss 1.8252, val loss 1.9409\r\n","iter 1592: loss 1.8027, time 4374.18ms, mfu 0.00%\r\n","step 1593: train loss 1.8267, val loss 1.9295\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 1593: loss 1.8343, time 4479.20ms, mfu 0.00%\r\n","step 1594: train loss 1.8306, val loss 1.9428\r\n","iter 1594: loss 1.8016, time 4431.75ms, mfu 0.00%\r\n","step 1595: train loss 1.8306, val loss 1.9516\r\n","iter 1595: loss 1.7657, time 4969.12ms, mfu 0.00%\r\n","step 1596: train loss 1.8379, val loss 1.9442\r\n","iter 1596: loss 1.8420, time 4559.50ms, mfu 0.00%\r\n","step 1597: train loss 1.8360, val loss 1.9467\r\n","iter 1597: loss 1.8396, time 4496.49ms, mfu 0.00%\r\n","step 1598: train loss 1.8291, val loss 1.9533\r\n","iter 1598: loss 1.8585, time 4541.27ms, mfu 0.00%\r\n","step 1599: train loss 1.8369, val loss 1.9499\r\n","iter 1599: loss 1.8752, time 4570.11ms, mfu 0.00%\r\n","step 1600: train loss 1.8357, val loss 1.9529\r\n","iter 1600: loss 1.7869, time 4452.69ms, mfu 0.00%\r\n","step 1601: train loss 1.8327, val loss 1.9517\r\n","iter 1601: loss 1.7817, time 4549.58ms, mfu 0.00%\r\n","step 1602: train loss 1.8275, val loss 1.9556\r\n","iter 1602: loss 1.7937, time 5172.86ms, mfu 0.00%\r\n","step 1603: train loss 1.8375, val loss 1.9525\r\n","iter 1603: loss 1.7843, time 4472.61ms, mfu 0.00%\r\n","step 1604: train loss 1.8389, val loss 1.9582\r\n","iter 1604: loss 1.8119, time 4409.21ms, mfu 0.00%\r\n","step 1605: train loss 1.8340, val loss 1.9555\r\n","iter 1605: loss 1.8014, time 4610.25ms, mfu 0.00%\r\n","step 1606: train loss 1.8320, val loss 1.9506\r\n","iter 1606: loss 1.8564, time 4517.16ms, mfu 0.00%\r\n","step 1607: train loss 1.8383, val loss 1.9477\r\n","iter 1607: loss 1.9138, time 4522.08ms, mfu 0.00%\r\n","step 1608: train loss 1.8316, val loss 1.9467\r\n","iter 1608: loss 1.9369, time 4451.52ms, mfu 0.00%\r\n","step 1609: train loss 1.8310, val loss 1.9555\r\n","iter 1609: loss 1.9282, time 4642.78ms, mfu 0.00%\r\n","step 1610: train loss 1.8403, val loss 1.9461\r\n","iter 1610: loss 1.7995, time 4717.73ms, mfu 0.00%\r\n","step 1611: train loss 1.8327, val loss 1.9487\r\n","iter 1611: loss 1.8684, time 4359.22ms, mfu 0.00%\r\n","step 1612: train loss 1.8302, val loss 1.9475\r\n","iter 1612: loss 1.8892, time 4610.73ms, mfu 0.00%\r\n","step 1613: train loss 1.8393, val loss 1.9537\r\n","iter 1613: loss 1.8723, time 4462.74ms, mfu 0.00%\r\n","step 1614: train loss 1.8352, val loss 1.9550\r\n","iter 1614: loss 1.7806, time 4514.28ms, mfu 0.00%\r\n","step 1615: train loss 1.8420, val loss 1.9459\r\n","iter 1615: loss 1.8766, time 4411.84ms, mfu 0.00%\r\n","step 1616: train loss 1.8319, val loss 1.9475\r\n","iter 1616: loss 1.8132, time 4470.04ms, mfu 0.00%\r\n","step 1617: train loss 1.8288, val loss 1.9480\r\n","iter 1617: loss 1.7720, time 4936.03ms, mfu 0.00%\r\n","step 1618: train loss 1.8293, val loss 1.9510\r\n","iter 1618: loss 1.8088, time 4433.53ms, mfu 0.00%\r\n","step 1619: train loss 1.8334, val loss 1.9513\r\n","iter 1619: loss 1.8545, time 4604.99ms, mfu 0.00%\r\n","step 1620: train loss 1.8210, val loss 1.9364\r\n","iter 1620: loss 1.9384, time 4391.42ms, mfu 0.00%\r\n","step 1621: train loss 1.8278, val loss 1.9396\r\n","iter 1621: loss 1.8518, time 4392.38ms, mfu 0.00%\r\n","step 1622: train loss 1.8260, val loss 1.9454\r\n","iter 1622: loss 1.8566, time 4363.77ms, mfu 0.00%\r\n","step 1623: train loss 1.8256, val loss 1.9457\r\n","iter 1623: loss 1.9116, time 4392.49ms, mfu 0.00%\r\n","step 1624: train loss 1.8212, val loss 1.9472\r\n","iter 1624: loss 1.8727, time 4979.15ms, mfu 0.00%\r\n","step 1625: train loss 1.8250, val loss 1.9485\r\n","iter 1625: loss 1.8928, time 4388.43ms, mfu 0.00%\r\n","step 1626: train loss 1.8244, val loss 1.9424\r\n","iter 1626: loss 1.8705, time 4539.94ms, mfu 0.00%\r\n","step 1627: train loss 1.8362, val loss 1.9508\r\n","iter 1627: loss 1.8438, time 4439.86ms, mfu 0.00%\r\n","step 1628: train loss 1.8297, val loss 1.9420\r\n","iter 1628: loss 1.8468, time 4535.86ms, mfu 0.00%\r\n","step 1629: train loss 1.8195, val loss 1.9430\r\n","iter 1629: loss 1.7477, time 4302.65ms, mfu 0.00%\r\n","step 1630: train loss 1.8244, val loss 1.9476\r\n","iter 1630: loss 1.8399, time 4518.97ms, mfu 0.00%\r\n","step 1631: train loss 1.8213, val loss 1.9448\r\n","iter 1631: loss 1.8154, time 4935.41ms, mfu 0.00%\r\n","step 1632: train loss 1.8186, val loss 1.9506\r\n","iter 1632: loss 1.7063, time 4541.37ms, mfu 0.00%\r\n","step 1633: train loss 1.8179, val loss 1.9342\r\n","iter 1633: loss 1.8609, time 4491.24ms, mfu 0.00%\r\n","step 1634: train loss 1.8243, val loss 1.9413\r\n","iter 1634: loss 1.8209, time 4405.77ms, mfu 0.00%\r\n","step 1635: train loss 1.8204, val loss 1.9270\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 1635: loss 1.6879, time 4370.65ms, mfu 0.00%\r\n","step 1636: train loss 1.8282, val loss 1.9401\r\n","iter 1636: loss 1.8312, time 4447.81ms, mfu 0.00%\r\n","step 1637: train loss 1.8247, val loss 1.9382\r\n","iter 1637: loss 1.8319, time 4352.60ms, mfu 0.00%\r\n","step 1638: train loss 1.8155, val loss 1.9276\r\n","iter 1638: loss 1.8462, time 5133.94ms, mfu 0.00%\r\n","step 1639: train loss 1.8243, val loss 1.9369\r\n","iter 1639: loss 1.7853, time 4368.01ms, mfu 0.00%\r\n","step 1640: train loss 1.8285, val loss 1.9393\r\n","iter 1640: loss 1.8085, time 4556.82ms, mfu 0.00%\r\n","step 1641: train loss 1.8266, val loss 1.9234\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 1641: loss 1.7009, time 4537.72ms, mfu 0.00%\r\n","step 1642: train loss 1.8228, val loss 1.9380\r\n","iter 1642: loss 1.7151, time 4380.00ms, mfu 0.00%\r\n","step 1643: train loss 1.8247, val loss 1.9338\r\n","iter 1643: loss 1.8197, time 4419.80ms, mfu 0.00%\r\n","step 1644: train loss 1.8300, val loss 1.9253\r\n","iter 1644: loss 1.9383, time 4419.94ms, mfu 0.00%\r\n","step 1645: train loss 1.8206, val loss 1.9321\r\n","iter 1645: loss 1.8356, time 4692.64ms, mfu 0.00%\r\n","step 1646: train loss 1.8262, val loss 1.9423\r\n","iter 1646: loss 1.6533, time 4772.51ms, mfu 0.00%\r\n","step 1647: train loss 1.8280, val loss 1.9308\r\n","iter 1647: loss 1.8084, time 4508.08ms, mfu 0.00%\r\n","step 1648: train loss 1.8176, val loss 1.9319\r\n","iter 1648: loss 1.8494, time 4438.29ms, mfu 0.00%\r\n","step 1649: train loss 1.8162, val loss 1.9310\r\n","iter 1649: loss 1.7978, time 4437.00ms, mfu 0.00%\r\n","step 1650: train loss 1.8163, val loss 1.9285\r\n","iter 1650: loss 1.7568, time 4513.85ms, mfu 0.00%\r\n","step 1651: train loss 1.8164, val loss 1.9298\r\n","iter 1651: loss 1.7903, time 4305.63ms, mfu 0.00%\r\n","step 1652: train loss 1.8217, val loss 1.9387\r\n","iter 1652: loss 1.9186, time 4475.38ms, mfu 0.00%\r\n","step 1653: train loss 1.8232, val loss 1.9297\r\n","iter 1653: loss 1.8333, time 4888.18ms, mfu 0.00%\r\n","step 1654: train loss 1.8235, val loss 1.9280\r\n","iter 1654: loss 1.7717, time 4562.48ms, mfu 0.00%\r\n","step 1655: train loss 1.8212, val loss 1.9411\r\n","iter 1655: loss 1.8197, time 4435.62ms, mfu 0.00%\r\n","step 1656: train loss 1.8125, val loss 1.9447\r\n","iter 1656: loss 1.7931, time 4315.81ms, mfu 0.00%\r\n","step 1657: train loss 1.8292, val loss 1.9399\r\n","iter 1657: loss 1.8747, time 4333.31ms, mfu 0.00%\r\n","step 1658: train loss 1.8160, val loss 1.9317\r\n","iter 1658: loss 1.8448, time 4344.75ms, mfu 0.00%\r\n","step 1659: train loss 1.8160, val loss 1.9344\r\n","iter 1659: loss 1.7688, time 4364.02ms, mfu 0.00%\r\n","step 1660: train loss 1.8224, val loss 1.9464\r\n","iter 1660: loss 1.8009, time 4951.10ms, mfu 0.00%\r\n","step 1661: train loss 1.8200, val loss 1.9367\r\n","iter 1661: loss 1.8187, time 4571.58ms, mfu 0.00%\r\n","step 1662: train loss 1.8234, val loss 1.9342\r\n","iter 1662: loss 1.8387, time 4350.79ms, mfu 0.00%\r\n","step 1663: train loss 1.8241, val loss 1.9422\r\n","iter 1663: loss 1.9943, time 4461.73ms, mfu 0.00%\r\n","step 1664: train loss 1.8214, val loss 1.9397\r\n","iter 1664: loss 1.8081, time 4327.75ms, mfu 0.00%\r\n","step 1665: train loss 1.8134, val loss 1.9316\r\n","iter 1665: loss 1.8028, time 4381.97ms, mfu 0.00%\r\n","step 1666: train loss 1.8157, val loss 1.9411\r\n","iter 1666: loss 1.8343, time 4481.95ms, mfu 0.00%\r\n","step 1667: train loss 1.8127, val loss 1.9356\r\n","iter 1667: loss 1.9602, time 5000.14ms, mfu 0.00%\r\n","step 1668: train loss 1.8171, val loss 1.9352\r\n","iter 1668: loss 1.7633, time 4717.21ms, mfu 0.00%\r\n","step 1669: train loss 1.8155, val loss 1.9365\r\n","iter 1669: loss 1.8044, time 4461.27ms, mfu 0.00%\r\n","step 1670: train loss 1.8158, val loss 1.9332\r\n","iter 1670: loss 1.8733, time 4410.82ms, mfu 0.00%\r\n","step 1671: train loss 1.8185, val loss 1.9333\r\n","iter 1671: loss 1.7676, time 4370.00ms, mfu 0.00%\r\n","step 1672: train loss 1.8158, val loss 1.9362\r\n","iter 1672: loss 1.8679, time 4532.50ms, mfu 0.00%\r\n","step 1673: train loss 1.8173, val loss 1.9290\r\n","iter 1673: loss 1.8433, time 4346.58ms, mfu 0.00%\r\n","step 1674: train loss 1.8084, val loss 1.9253\r\n","iter 1674: loss 1.8780, time 5033.18ms, mfu 0.00%\r\n","step 1675: train loss 1.8207, val loss 1.9295\r\n","iter 1675: loss 1.8108, time 4529.64ms, mfu 0.00%\r\n","step 1676: train loss 1.8087, val loss 1.9324\r\n","iter 1676: loss 1.7372, time 4395.22ms, mfu 0.00%\r\n","step 1677: train loss 1.8101, val loss 1.9313\r\n","iter 1677: loss 1.9348, time 4368.91ms, mfu 0.00%\r\n","step 1678: train loss 1.8093, val loss 1.9264\r\n","iter 1678: loss 1.9088, time 4309.70ms, mfu 0.00%\r\n","step 1679: train loss 1.8146, val loss 1.9263\r\n","iter 1679: loss 1.8977, time 4391.91ms, mfu 0.00%\r\n","step 1680: train loss 1.8154, val loss 1.9320\r\n","iter 1680: loss 1.7966, time 4399.68ms, mfu 0.00%\r\n","step 1681: train loss 1.8122, val loss 1.9198\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 1681: loss 1.8163, time 4853.19ms, mfu 0.00%\r\n","step 1682: train loss 1.8112, val loss 1.9317\r\n","iter 1682: loss 1.9463, time 4670.01ms, mfu 0.00%\r\n","step 1683: train loss 1.8013, val loss 1.9274\r\n","iter 1683: loss 1.8769, time 4512.58ms, mfu 0.00%\r\n","step 1684: train loss 1.8125, val loss 1.9234\r\n","iter 1684: loss 1.9043, time 4312.11ms, mfu 0.00%\r\n","step 1685: train loss 1.8144, val loss 1.9311\r\n","iter 1685: loss 1.7021, time 4402.02ms, mfu 0.00%\r\n","step 1686: train loss 1.8118, val loss 1.9357\r\n","iter 1686: loss 1.8270, time 4543.88ms, mfu 0.00%\r\n","step 1687: train loss 1.8131, val loss 1.9282\r\n","iter 1687: loss 1.7587, time 4292.13ms, mfu 0.00%\r\n","step 1688: train loss 1.8171, val loss 1.9296\r\n","iter 1688: loss 1.9220, time 4420.49ms, mfu 0.00%\r\n","step 1689: train loss 1.8183, val loss 1.9274\r\n","iter 1689: loss 1.8856, time 5089.84ms, mfu 0.00%\r\n","step 1690: train loss 1.8185, val loss 1.9243\r\n","iter 1690: loss 1.8448, time 4392.53ms, mfu 0.00%\r\n","step 1691: train loss 1.8092, val loss 1.9277\r\n","iter 1691: loss 1.7444, time 4448.77ms, mfu 0.00%\r\n","step 1692: train loss 1.8197, val loss 1.9345\r\n","iter 1692: loss 1.8775, time 4341.83ms, mfu 0.00%\r\n","step 1693: train loss 1.8096, val loss 1.9281\r\n","iter 1693: loss 1.9010, time 4311.33ms, mfu 0.00%\r\n","step 1694: train loss 1.8187, val loss 1.9263\r\n","iter 1694: loss 1.7866, time 4445.56ms, mfu 0.00%\r\n","step 1695: train loss 1.8027, val loss 1.9277\r\n","iter 1695: loss 1.8525, time 4291.36ms, mfu 0.00%\r\n","step 1696: train loss 1.8155, val loss 1.9282\r\n","iter 1696: loss 1.8459, time 5253.30ms, mfu 0.00%\r\n","step 1697: train loss 1.8143, val loss 1.9334\r\n","iter 1697: loss 1.8432, time 4490.94ms, mfu 0.00%\r\n","step 1698: train loss 1.8200, val loss 1.9287\r\n","iter 1698: loss 1.8473, time 4404.60ms, mfu 0.00%\r\n","step 1699: train loss 1.8138, val loss 1.9238\r\n","iter 1699: loss 1.7101, time 4289.86ms, mfu 0.00%\r\n","step 1700: train loss 1.8090, val loss 1.9334\r\n","iter 1700: loss 1.8649, time 4396.36ms, mfu 0.00%\r\n","step 1701: train loss 1.8099, val loss 1.9306\r\n","iter 1701: loss 1.8329, time 4334.43ms, mfu 0.00%\r\n","step 1702: train loss 1.8136, val loss 1.9228\r\n","iter 1702: loss 1.6989, time 4331.54ms, mfu 0.00%\r\n","step 1703: train loss 1.8127, val loss 1.9348\r\n","iter 1703: loss 1.8513, time 5254.20ms, mfu 0.00%\r\n","step 1704: train loss 1.8049, val loss 1.9356\r\n","iter 1704: loss 1.8298, time 4478.73ms, mfu 0.00%\r\n","step 1705: train loss 1.8046, val loss 1.9255\r\n","iter 1705: loss 1.7835, time 4446.78ms, mfu 0.00%\r\n","step 1706: train loss 1.8101, val loss 1.9251\r\n","iter 1706: loss 1.7643, time 4374.11ms, mfu 0.00%\r\n","step 1707: train loss 1.8096, val loss 1.9297\r\n","iter 1707: loss 1.8640, time 4327.10ms, mfu 0.00%\r\n","step 1708: train loss 1.8059, val loss 1.9178\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 1708: loss 1.7673, time 4560.81ms, mfu 0.00%\r\n","step 1709: train loss 1.8124, val loss 1.9244\r\n","iter 1709: loss 1.8523, time 4324.16ms, mfu 0.00%\r\n","step 1710: train loss 1.8071, val loss 1.9280\r\n","iter 1710: loss 1.6891, time 5123.40ms, mfu 0.00%\r\n","step 1711: train loss 1.8112, val loss 1.9178\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 1711: loss 1.8152, time 4485.61ms, mfu 0.00%\r\n","step 1712: train loss 1.8086, val loss 1.9159\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 1712: loss 1.6768, time 4343.92ms, mfu 0.00%\r\n","step 1713: train loss 1.8079, val loss 1.9258\r\n","iter 1713: loss 1.8860, time 4274.07ms, mfu 0.00%\r\n","step 1714: train loss 1.8061, val loss 1.9281\r\n","iter 1714: loss 1.8280, time 4353.00ms, mfu 0.00%\r\n","step 1715: train loss 1.8114, val loss 1.9280\r\n","iter 1715: loss 1.9072, time 4215.57ms, mfu 0.00%\r\n","step 1716: train loss 1.8005, val loss 1.9373\r\n","iter 1716: loss 1.9110, time 4304.44ms, mfu 0.00%\r\n","step 1717: train loss 1.8027, val loss 1.9328\r\n","iter 1717: loss 1.8432, time 4431.98ms, mfu 0.00%\r\n","step 1718: train loss 1.8083, val loss 1.9253\r\n","iter 1718: loss 1.8555, time 5032.38ms, mfu 0.00%\r\n","step 1719: train loss 1.8033, val loss 1.9321\r\n","iter 1719: loss 1.7963, time 4425.18ms, mfu 0.00%\r\n","step 1720: train loss 1.8069, val loss 1.9322\r\n","iter 1720: loss 1.7717, time 4273.70ms, mfu 0.00%\r\n","step 1721: train loss 1.8100, val loss 1.9204\r\n","iter 1721: loss 1.8223, time 4309.44ms, mfu 0.00%\r\n","step 1722: train loss 1.8117, val loss 1.9424\r\n","iter 1722: loss 1.7606, time 4458.57ms, mfu 0.00%\r\n","step 1723: train loss 1.8083, val loss 1.9351\r\n","iter 1723: loss 1.7299, time 4403.26ms, mfu 0.00%\r\n","step 1724: train loss 1.8108, val loss 1.9278\r\n","iter 1724: loss 1.7128, time 4431.92ms, mfu 0.00%\r\n","step 1725: train loss 1.8183, val loss 1.9272\r\n","iter 1725: loss 1.8153, time 5173.09ms, mfu 0.00%\r\n","step 1726: train loss 1.8169, val loss 1.9250\r\n","iter 1726: loss 1.8393, time 4380.61ms, mfu 0.00%\r\n","step 1727: train loss 1.8138, val loss 1.9276\r\n","iter 1727: loss 1.8578, time 4280.45ms, mfu 0.00%\r\n","step 1728: train loss 1.8081, val loss 1.9316\r\n","iter 1728: loss 1.8933, time 4423.47ms, mfu 0.00%\r\n","step 1729: train loss 1.8003, val loss 1.9363\r\n","iter 1729: loss 1.8446, time 4249.13ms, mfu 0.00%\r\n","step 1730: train loss 1.8155, val loss 1.9288\r\n","iter 1730: loss 1.7991, time 4346.01ms, mfu 0.00%\r\n","step 1731: train loss 1.8109, val loss 1.9273\r\n","iter 1731: loss 1.7266, time 4413.07ms, mfu 0.00%\r\n","step 1732: train loss 1.8057, val loss 1.9266\r\n","iter 1732: loss 1.7724, time 4953.74ms, mfu 0.00%\r\n","step 1733: train loss 1.8061, val loss 1.9236\r\n","iter 1733: loss 1.8139, time 4439.23ms, mfu 0.00%\r\n","step 1734: train loss 1.8144, val loss 1.9277\r\n","iter 1734: loss 1.8608, time 4338.58ms, mfu 0.00%\r\n","step 1735: train loss 1.8049, val loss 1.9221\r\n","iter 1735: loss 1.8330, time 4425.99ms, mfu 0.00%\r\n","step 1736: train loss 1.8139, val loss 1.9210\r\n","iter 1736: loss 1.7541, time 4437.22ms, mfu 0.00%\r\n","step 1737: train loss 1.8038, val loss 1.9123\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 1737: loss 1.7918, time 4429.86ms, mfu 0.00%\r\n","step 1738: train loss 1.8080, val loss 1.9207\r\n","iter 1738: loss 1.8241, time 4471.24ms, mfu 0.00%\r\n","step 1739: train loss 1.8054, val loss 1.9223\r\n","iter 1739: loss 1.8753, time 4809.29ms, mfu 0.00%\r\n","step 1740: train loss 1.8079, val loss 1.9180\r\n","iter 1740: loss 1.7532, time 4839.16ms, mfu 0.00%\r\n","step 1741: train loss 1.8097, val loss 1.9219\r\n","iter 1741: loss 1.7079, time 4323.29ms, mfu 0.00%\r\n","step 1742: train loss 1.7985, val loss 1.9252\r\n","iter 1742: loss 1.8372, time 4343.59ms, mfu 0.00%\r\n","step 1743: train loss 1.8002, val loss 1.9206\r\n","iter 1743: loss 1.7341, time 4260.91ms, mfu 0.00%\r\n","step 1744: train loss 1.8119, val loss 1.9160\r\n","iter 1744: loss 1.7735, time 4241.02ms, mfu 0.00%\r\n","step 1745: train loss 1.8067, val loss 1.9212\r\n","iter 1745: loss 1.7888, time 4413.38ms, mfu 0.00%\r\n","step 1746: train loss 1.8128, val loss 1.9288\r\n","iter 1746: loss 1.8587, time 4399.73ms, mfu 0.00%\r\n","step 1747: train loss 1.8122, val loss 1.9186\r\n","iter 1747: loss 1.7678, time 5109.90ms, mfu 0.00%\r\n","step 1748: train loss 1.8135, val loss 1.9210\r\n","iter 1748: loss 1.8173, time 4339.56ms, mfu 0.00%\r\n","step 1749: train loss 1.8069, val loss 1.9223\r\n","iter 1749: loss 1.8148, time 4312.27ms, mfu 0.00%\r\n","step 1750: train loss 1.8015, val loss 1.9237\r\n","iter 1750: loss 1.7158, time 4424.99ms, mfu 0.00%\r\n","step 1751: train loss 1.8107, val loss 1.9258\r\n","iter 1751: loss 1.8221, time 4252.38ms, mfu 0.00%\r\n","step 1752: train loss 1.8079, val loss 1.9313\r\n","iter 1752: loss 1.8844, time 4452.56ms, mfu 0.00%\r\n","step 1753: train loss 1.8064, val loss 1.9273\r\n","iter 1753: loss 1.7897, time 4399.28ms, mfu 0.00%\r\n","step 1754: train loss 1.8063, val loss 1.9322\r\n","iter 1754: loss 1.8384, time 5024.14ms, mfu 0.00%\r\n","step 1755: train loss 1.8044, val loss 1.9262\r\n","iter 1755: loss 1.8573, time 4292.99ms, mfu 0.00%\r\n","step 1756: train loss 1.8030, val loss 1.9239\r\n","iter 1756: loss 1.7963, time 4285.72ms, mfu 0.00%\r\n","step 1757: train loss 1.8021, val loss 1.9220\r\n","iter 1757: loss 1.7569, time 4249.21ms, mfu 0.00%\r\n","step 1758: train loss 1.8098, val loss 1.9132\r\n","iter 1758: loss 1.8376, time 4387.70ms, mfu 0.00%\r\n","step 1759: train loss 1.8038, val loss 1.9238\r\n","iter 1759: loss 1.7443, time 4594.58ms, mfu 0.00%\r\n","step 1760: train loss 1.8031, val loss 1.9219\r\n","iter 1760: loss 1.7927, time 4368.11ms, mfu 0.00%\r\n","step 1761: train loss 1.8030, val loss 1.9230\r\n","iter 1761: loss 1.8323, time 4779.92ms, mfu 0.00%\r\n","step 1762: train loss 1.8037, val loss 1.9140\r\n","iter 1762: loss 1.7413, time 4842.31ms, mfu 0.00%\r\n","step 1763: train loss 1.8084, val loss 1.9088\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 1763: loss 1.8529, time 4431.78ms, mfu 0.00%\r\n","step 1764: train loss 1.7953, val loss 1.9202\r\n","iter 1764: loss 1.7594, time 4485.35ms, mfu 0.00%\r\n","step 1765: train loss 1.8092, val loss 1.9168\r\n","iter 1765: loss 1.8641, time 4419.72ms, mfu 0.00%\r\n","step 1766: train loss 1.7971, val loss 1.9222\r\n","iter 1766: loss 1.8382, time 4574.27ms, mfu 0.00%\r\n","step 1767: train loss 1.8074, val loss 1.9144\r\n","iter 1767: loss 1.8340, time 4529.62ms, mfu 0.00%\r\n","step 1768: train loss 1.8041, val loss 1.9162\r\n","iter 1768: loss 1.8912, time 4439.08ms, mfu 0.00%\r\n","step 1769: train loss 1.7987, val loss 1.9119\r\n","iter 1769: loss 1.7323, time 4905.21ms, mfu 0.00%\r\n","step 1770: train loss 1.7999, val loss 1.9182\r\n","iter 1770: loss 1.8224, time 4456.68ms, mfu 0.00%\r\n","step 1771: train loss 1.7961, val loss 1.9212\r\n","iter 1771: loss 1.7460, time 4291.28ms, mfu 0.00%\r\n","step 1772: train loss 1.8059, val loss 1.9166\r\n","iter 1772: loss 1.8927, time 4428.00ms, mfu 0.00%\r\n","step 1773: train loss 1.8024, val loss 1.9199\r\n","iter 1773: loss 1.6879, time 4541.68ms, mfu 0.00%\r\n","step 1774: train loss 1.8091, val loss 1.9170\r\n","iter 1774: loss 1.7314, time 4404.10ms, mfu 0.00%\r\n","step 1775: train loss 1.8006, val loss 1.9232\r\n","iter 1775: loss 1.7895, time 4440.48ms, mfu 0.00%\r\n","step 1776: train loss 1.8028, val loss 1.9228\r\n","iter 1776: loss 1.7784, time 5219.83ms, mfu 0.00%\r\n","step 1777: train loss 1.8006, val loss 1.9259\r\n","iter 1777: loss 1.8075, time 4369.30ms, mfu 0.00%\r\n","step 1778: train loss 1.8004, val loss 1.9236\r\n","iter 1778: loss 1.7972, time 4411.22ms, mfu 0.00%\r\n","step 1779: train loss 1.8028, val loss 1.9260\r\n","iter 1779: loss 1.8342, time 4355.43ms, mfu 0.00%\r\n","step 1780: train loss 1.8099, val loss 1.9205\r\n","iter 1780: loss 1.7508, time 4388.54ms, mfu 0.00%\r\n","step 1781: train loss 1.7968, val loss 1.9196\r\n","iter 1781: loss 1.7934, time 4529.84ms, mfu 0.00%\r\n","step 1782: train loss 1.7992, val loss 1.9292\r\n","iter 1782: loss 1.7289, time 4318.49ms, mfu 0.00%\r\n","step 1783: train loss 1.7985, val loss 1.9268\r\n","iter 1783: loss 1.8168, time 4975.61ms, mfu 0.00%\r\n","step 1784: train loss 1.7933, val loss 1.9178\r\n","iter 1784: loss 1.8069, time 4424.34ms, mfu 0.00%\r\n","step 1785: train loss 1.8064, val loss 1.9274\r\n","iter 1785: loss 1.8314, time 4375.49ms, mfu 0.00%\r\n","step 1786: train loss 1.8037, val loss 1.9268\r\n","iter 1786: loss 1.8313, time 4399.81ms, mfu 0.00%\r\n","step 1787: train loss 1.8002, val loss 1.9268\r\n","iter 1787: loss 1.7437, time 4510.77ms, mfu 0.00%\r\n","step 1788: train loss 1.8055, val loss 1.9241\r\n","iter 1788: loss 1.7836, time 4510.34ms, mfu 0.00%\r\n","step 1789: train loss 1.8010, val loss 1.9189\r\n","iter 1789: loss 1.8519, time 4439.47ms, mfu 0.00%\r\n","step 1790: train loss 1.7991, val loss 1.9132\r\n","iter 1790: loss 1.6461, time 4883.22ms, mfu 0.00%\r\n","step 1791: train loss 1.7995, val loss 1.9161\r\n","iter 1791: loss 1.7880, time 4443.43ms, mfu 0.00%\r\n","step 1792: train loss 1.7935, val loss 1.9042\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 1792: loss 1.8426, time 4542.72ms, mfu 0.00%\r\n","step 1793: train loss 1.7946, val loss 1.9142\r\n","iter 1793: loss 1.8400, time 4345.51ms, mfu 0.00%\r\n","step 1794: train loss 1.7993, val loss 1.9161\r\n","iter 1794: loss 1.8435, time 4622.97ms, mfu 0.00%\r\n","step 1795: train loss 1.7985, val loss 1.9170\r\n","iter 1795: loss 1.7604, time 4467.58ms, mfu 0.00%\r\n","step 1796: train loss 1.8050, val loss 1.9234\r\n","iter 1796: loss 1.7629, time 4329.97ms, mfu 0.00%\r\n","step 1797: train loss 1.8023, val loss 1.9224\r\n","iter 1797: loss 1.8575, time 4941.58ms, mfu 0.00%\r\n","step 1798: train loss 1.7977, val loss 1.9222\r\n","iter 1798: loss 1.8373, time 4477.93ms, mfu 0.00%\r\n","step 1799: train loss 1.7930, val loss 1.9173\r\n","iter 1799: loss 1.8347, time 4396.64ms, mfu 0.00%\r\n","step 1800: train loss 1.8033, val loss 1.9171\r\n","iter 1800: loss 1.7459, time 4309.22ms, mfu 0.00%\r\n","step 1801: train loss 1.7953, val loss 1.9169\r\n","iter 1801: loss 1.7945, time 4521.72ms, mfu 0.00%\r\n","step 1802: train loss 1.8002, val loss 1.9214\r\n","iter 1802: loss 1.7433, time 4361.09ms, mfu 0.00%\r\n","step 1803: train loss 1.7953, val loss 1.9214\r\n","iter 1803: loss 1.9123, time 4413.23ms, mfu 0.00%\r\n","step 1804: train loss 1.7904, val loss 1.9196\r\n","iter 1804: loss 1.6891, time 4530.15ms, mfu 0.00%\r\n","step 1805: train loss 1.7894, val loss 1.9192\r\n","iter 1805: loss 1.8725, time 4834.23ms, mfu 0.00%\r\n","step 1806: train loss 1.7963, val loss 1.9222\r\n","iter 1806: loss 1.7877, time 4437.90ms, mfu 0.00%\r\n","step 1807: train loss 1.8006, val loss 1.9165\r\n","iter 1807: loss 1.7998, time 4337.15ms, mfu 0.00%\r\n","step 1808: train loss 1.7867, val loss 1.9202\r\n","iter 1808: loss 1.8362, time 4431.82ms, mfu 0.00%\r\n","step 1809: train loss 1.7978, val loss 1.9240\r\n","iter 1809: loss 1.7980, time 4527.03ms, mfu 0.00%\r\n","step 1810: train loss 1.7933, val loss 1.9151\r\n","iter 1810: loss 1.8425, time 4449.99ms, mfu 0.00%\r\n","step 1811: train loss 1.7917, val loss 1.9173\r\n","iter 1811: loss 1.7611, time 4368.83ms, mfu 0.00%\r\n","step 1812: train loss 1.7825, val loss 1.9158\r\n","iter 1812: loss 1.7884, time 5252.94ms, mfu 0.00%\r\n","step 1813: train loss 1.7968, val loss 1.9151\r\n","iter 1813: loss 1.8392, time 4408.04ms, mfu 0.00%\r\n","step 1814: train loss 1.7917, val loss 1.9250\r\n","iter 1814: loss 1.8806, time 4466.77ms, mfu 0.00%\r\n","step 1815: train loss 1.7940, val loss 1.9193\r\n","iter 1815: loss 1.8393, time 4593.96ms, mfu 0.00%\r\n","step 1816: train loss 1.7953, val loss 1.9200\r\n","iter 1816: loss 1.7572, time 4436.37ms, mfu 0.00%\r\n","step 1817: train loss 1.7913, val loss 1.9129\r\n","iter 1817: loss 1.7780, time 4507.11ms, mfu 0.00%\r\n","step 1818: train loss 1.7915, val loss 1.9183\r\n","iter 1818: loss 1.7586, time 4432.32ms, mfu 0.00%\r\n","step 1819: train loss 1.8017, val loss 1.9176\r\n","iter 1819: loss 1.7238, time 5078.94ms, mfu 0.00%\r\n","step 1820: train loss 1.7967, val loss 1.9221\r\n","iter 1820: loss 1.8203, time 4485.42ms, mfu 0.00%\r\n","step 1821: train loss 1.7994, val loss 1.9142\r\n","iter 1821: loss 1.8089, time 4393.87ms, mfu 0.00%\r\n","step 1822: train loss 1.7955, val loss 1.9138\r\n","iter 1822: loss 1.7872, time 4446.54ms, mfu 0.00%\r\n","step 1823: train loss 1.8026, val loss 1.9208\r\n","iter 1823: loss 1.7301, time 4517.27ms, mfu 0.00%\r\n","step 1824: train loss 1.7994, val loss 1.9170\r\n","iter 1824: loss 1.7667, time 4296.19ms, mfu 0.00%\r\n","step 1825: train loss 1.7979, val loss 1.9163\r\n","iter 1825: loss 1.7916, time 4370.58ms, mfu 0.00%\r\n","step 1826: train loss 1.7953, val loss 1.9203\r\n","iter 1826: loss 1.8548, time 5024.79ms, mfu 0.00%\r\n","step 1827: train loss 1.7940, val loss 1.9076\r\n","iter 1827: loss 1.7817, time 4340.45ms, mfu 0.00%\r\n","step 1828: train loss 1.7984, val loss 1.9192\r\n","iter 1828: loss 1.7134, time 4388.73ms, mfu 0.00%\r\n","step 1829: train loss 1.7983, val loss 1.9159\r\n","iter 1829: loss 1.7792, time 4427.82ms, mfu 0.00%\r\n","step 1830: train loss 1.7966, val loss 1.9091\r\n","iter 1830: loss 1.9131, time 4578.09ms, mfu 0.00%\r\n","step 1831: train loss 1.7981, val loss 1.9127\r\n","iter 1831: loss 1.8063, time 4401.83ms, mfu 0.00%\r\n","step 1832: train loss 1.7921, val loss 1.9101\r\n","iter 1832: loss 1.6924, time 4425.07ms, mfu 0.00%\r\n","step 1833: train loss 1.7986, val loss 1.9179\r\n","iter 1833: loss 1.7641, time 4900.10ms, mfu 0.00%\r\n","step 1834: train loss 1.7972, val loss 1.9163\r\n","iter 1834: loss 1.7520, time 4791.60ms, mfu 0.00%\r\n","step 1835: train loss 1.7923, val loss 1.9139\r\n","iter 1835: loss 1.8012, time 4493.70ms, mfu 0.00%\r\n","step 1836: train loss 1.8076, val loss 1.9123\r\n","iter 1836: loss 1.7979, time 4577.03ms, mfu 0.00%\r\n","step 1837: train loss 1.7960, val loss 1.9256\r\n","iter 1837: loss 1.7714, time 4433.54ms, mfu 0.00%\r\n","step 1838: train loss 1.7995, val loss 1.9164\r\n","iter 1838: loss 1.8206, time 4332.96ms, mfu 0.00%\r\n","step 1839: train loss 1.7969, val loss 1.9200\r\n","iter 1839: loss 1.8657, time 4312.35ms, mfu 0.00%\r\n","step 1840: train loss 1.7921, val loss 1.9155\r\n","iter 1840: loss 1.7997, time 4540.24ms, mfu 0.00%\r\n","step 1841: train loss 1.7935, val loss 1.9110\r\n","iter 1841: loss 1.9179, time 4921.13ms, mfu 0.00%\r\n","step 1842: train loss 1.7951, val loss 1.9099\r\n","iter 1842: loss 1.7702, time 4383.06ms, mfu 0.00%\r\n","step 1843: train loss 1.7880, val loss 1.9162\r\n","iter 1843: loss 1.7335, time 4429.29ms, mfu 0.00%\r\n","step 1844: train loss 1.7925, val loss 1.9205\r\n","iter 1844: loss 1.6527, time 4310.13ms, mfu 0.00%\r\n","step 1845: train loss 1.8070, val loss 1.9145\r\n","iter 1845: loss 1.8745, time 4500.01ms, mfu 0.00%\r\n","step 1846: train loss 1.7905, val loss 1.9115\r\n","iter 1846: loss 1.7604, time 4296.01ms, mfu 0.00%\r\n","step 1847: train loss 1.7945, val loss 1.9200\r\n","iter 1847: loss 1.8247, time 4261.56ms, mfu 0.00%\r\n","step 1848: train loss 1.7972, val loss 1.9197\r\n","iter 1848: loss 1.8210, time 5043.69ms, mfu 0.00%\r\n","step 1849: train loss 1.7975, val loss 1.9123\r\n","iter 1849: loss 1.7836, time 4322.96ms, mfu 0.00%\r\n","step 1850: train loss 1.7895, val loss 1.9129\r\n","iter 1850: loss 1.8160, time 4490.38ms, mfu 0.00%\r\n","step 1851: train loss 1.7910, val loss 1.9175\r\n","iter 1851: loss 1.8273, time 4363.91ms, mfu 0.00%\r\n","step 1852: train loss 1.7922, val loss 1.9055\r\n","iter 1852: loss 1.8272, time 4397.20ms, mfu 0.00%\r\n","step 1853: train loss 1.7952, val loss 1.9141\r\n","iter 1853: loss 1.9362, time 4388.56ms, mfu 0.00%\r\n","step 1854: train loss 1.7832, val loss 1.9142\r\n","iter 1854: loss 1.9047, time 4450.12ms, mfu 0.00%\r\n","step 1855: train loss 1.7835, val loss 1.9137\r\n","iter 1855: loss 1.7684, time 4910.37ms, mfu 0.00%\r\n","step 1856: train loss 1.7941, val loss 1.9128\r\n","iter 1856: loss 1.8077, time 4375.21ms, mfu 0.00%\r\n","step 1857: train loss 1.7850, val loss 1.9039\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 1857: loss 1.9313, time 4477.39ms, mfu 0.00%\r\n","step 1858: train loss 1.7918, val loss 1.9136\r\n","iter 1858: loss 1.7396, time 4388.26ms, mfu 0.00%\r\n","step 1859: train loss 1.7902, val loss 1.9134\r\n","iter 1859: loss 1.8348, time 4457.33ms, mfu 0.00%\r\n","step 1860: train loss 1.7883, val loss 1.9037\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 1860: loss 1.7550, time 4304.90ms, mfu 0.00%\r\n","step 1861: train loss 1.7925, val loss 1.9096\r\n","iter 1861: loss 1.7228, time 4289.01ms, mfu 0.00%\r\n","step 1862: train loss 1.7854, val loss 1.9129\r\n","iter 1862: loss 1.6676, time 4797.94ms, mfu 0.00%\r\n","step 1863: train loss 1.7972, val loss 1.9151\r\n","iter 1863: loss 1.8230, time 4444.02ms, mfu 0.00%\r\n","step 1864: train loss 1.7865, val loss 1.9144\r\n","iter 1864: loss 1.8106, time 4420.82ms, mfu 0.00%\r\n","step 1865: train loss 1.7866, val loss 1.9054\r\n","iter 1865: loss 1.8082, time 4392.96ms, mfu 0.00%\r\n","step 1866: train loss 1.7904, val loss 1.9106\r\n","iter 1866: loss 1.7339, time 4532.82ms, mfu 0.00%\r\n","step 1867: train loss 1.7866, val loss 1.9063\r\n","iter 1867: loss 1.7581, time 4342.32ms, mfu 0.00%\r\n","step 1868: train loss 1.7877, val loss 1.9182\r\n","iter 1868: loss 1.8347, time 4474.61ms, mfu 0.00%\r\n","step 1869: train loss 1.7992, val loss 1.9097\r\n","iter 1869: loss 1.8132, time 4340.46ms, mfu 0.00%\r\n","step 1870: train loss 1.7883, val loss 1.9112\r\n","iter 1870: loss 1.7878, time 4930.93ms, mfu 0.00%\r\n","step 1871: train loss 1.7946, val loss 1.9128\r\n","iter 1871: loss 1.6889, time 4471.09ms, mfu 0.00%\r\n","step 1872: train loss 1.7916, val loss 1.9144\r\n","iter 1872: loss 1.6565, time 4349.56ms, mfu 0.00%\r\n","step 1873: train loss 1.7917, val loss 1.9095\r\n","iter 1873: loss 1.7922, time 4428.23ms, mfu 0.00%\r\n","step 1874: train loss 1.7921, val loss 1.9125\r\n","iter 1874: loss 1.6912, time 4316.85ms, mfu 0.00%\r\n","step 1875: train loss 1.7869, val loss 1.9119\r\n","iter 1875: loss 1.7545, time 4282.41ms, mfu 0.00%\r\n","step 1876: train loss 1.7886, val loss 1.9077\r\n","iter 1876: loss 1.8827, time 4408.72ms, mfu 0.00%\r\n","step 1877: train loss 1.7850, val loss 1.9127\r\n","iter 1877: loss 1.7760, time 4842.97ms, mfu 0.00%\r\n","step 1878: train loss 1.7932, val loss 1.9182\r\n","iter 1878: loss 1.9184, time 4481.73ms, mfu 0.00%\r\n","step 1879: train loss 1.7926, val loss 1.9193\r\n","iter 1879: loss 1.9419, time 4496.27ms, mfu 0.00%\r\n","step 1880: train loss 1.7911, val loss 1.9185\r\n","iter 1880: loss 1.7859, time 4426.01ms, mfu 0.00%\r\n","step 1881: train loss 1.7827, val loss 1.9153\r\n","iter 1881: loss 1.8362, time 4438.56ms, mfu 0.00%\r\n","step 1882: train loss 1.7965, val loss 1.9072\r\n","iter 1882: loss 1.7354, time 4389.68ms, mfu 0.00%\r\n","step 1883: train loss 1.7982, val loss 1.9108\r\n","iter 1883: loss 1.8201, time 4365.48ms, mfu 0.00%\r\n","step 1884: train loss 1.7867, val loss 1.9257\r\n","iter 1884: loss 1.8319, time 5080.36ms, mfu 0.00%\r\n","step 1885: train loss 1.7887, val loss 1.9108\r\n","iter 1885: loss 1.8170, time 4476.19ms, mfu 0.00%\r\n","step 1886: train loss 1.7875, val loss 1.9099\r\n","iter 1886: loss 1.8414, time 4342.45ms, mfu 0.00%\r\n","step 1887: train loss 1.7852, val loss 1.9139\r\n","iter 1887: loss 1.7650, time 4430.38ms, mfu 0.00%\r\n","step 1888: train loss 1.7877, val loss 1.9098\r\n","iter 1888: loss 1.7639, time 4386.79ms, mfu 0.00%\r\n","step 1889: train loss 1.7956, val loss 1.9061\r\n","iter 1889: loss 1.8001, time 4296.80ms, mfu 0.00%\r\n","step 1890: train loss 1.7980, val loss 1.9228\r\n","iter 1890: loss 1.7487, time 4373.65ms, mfu 0.00%\r\n","step 1891: train loss 1.7914, val loss 1.9135\r\n","iter 1891: loss 1.7394, time 4959.49ms, mfu 0.00%\r\n","step 1892: train loss 1.7940, val loss 1.9150\r\n","iter 1892: loss 1.6462, time 4488.06ms, mfu 0.00%\r\n","step 1893: train loss 1.7873, val loss 1.9201\r\n","iter 1893: loss 1.6895, time 4421.95ms, mfu 0.00%\r\n","step 1894: train loss 1.7923, val loss 1.9167\r\n","iter 1894: loss 1.7987, time 4457.28ms, mfu 0.00%\r\n","step 1895: train loss 1.7909, val loss 1.9134\r\n","iter 1895: loss 1.8003, time 4457.03ms, mfu 0.00%\r\n","step 1896: train loss 1.7948, val loss 1.9101\r\n","iter 1896: loss 1.6916, time 4306.28ms, mfu 0.00%\r\n","step 1897: train loss 1.7877, val loss 1.9128\r\n","iter 1897: loss 1.7562, time 4286.33ms, mfu 0.00%\r\n","step 1898: train loss 1.7968, val loss 1.9212\r\n","iter 1898: loss 1.6997, time 4652.87ms, mfu 0.00%\r\n","step 1899: train loss 1.7918, val loss 1.9225\r\n","iter 1899: loss 1.7803, time 4684.81ms, mfu 0.00%\r\n","step 1900: train loss 1.7915, val loss 1.9229\r\n","iter 1900: loss 1.8432, time 4281.81ms, mfu 0.00%\r\n","step 1901: train loss 1.7787, val loss 1.9144\r\n","iter 1901: loss 1.7627, time 4463.21ms, mfu 0.00%\r\n","step 1902: train loss 1.7855, val loss 1.9064\r\n","iter 1902: loss 1.7923, time 4378.33ms, mfu 0.00%\r\n","step 1903: train loss 1.7943, val loss 1.9115\r\n","iter 1903: loss 1.7957, time 4320.95ms, mfu 0.00%\r\n","step 1904: train loss 1.7884, val loss 1.9148\r\n","iter 1904: loss 1.7782, time 4359.87ms, mfu 0.00%\r\n","step 1905: train loss 1.7806, val loss 1.9129\r\n","iter 1905: loss 1.8340, time 4362.22ms, mfu 0.00%\r\n","step 1906: train loss 1.7820, val loss 1.9046\r\n","iter 1906: loss 1.7941, time 4958.03ms, mfu 0.00%\r\n","step 1907: train loss 1.7777, val loss 1.9010\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 1907: loss 1.8179, time 4501.12ms, mfu 0.00%\r\n","step 1908: train loss 1.7911, val loss 1.9033\r\n","iter 1908: loss 1.8110, time 4466.75ms, mfu 0.00%\r\n","step 1909: train loss 1.7810, val loss 1.9108\r\n","iter 1909: loss 1.7928, time 4336.33ms, mfu 0.00%\r\n","step 1910: train loss 1.7854, val loss 1.8975\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 1910: loss 1.7635, time 4481.48ms, mfu 0.00%\r\n","step 1911: train loss 1.7891, val loss 1.9130\r\n","iter 1911: loss 1.7579, time 4313.43ms, mfu 0.00%\r\n","step 1912: train loss 1.7827, val loss 1.9102\r\n","iter 1912: loss 1.7843, time 4299.06ms, mfu 0.00%\r\n","step 1913: train loss 1.7925, val loss 1.9090\r\n","iter 1913: loss 1.8254, time 5013.27ms, mfu 0.00%\r\n","step 1914: train loss 1.7855, val loss 1.9044\r\n","iter 1914: loss 1.7188, time 4253.39ms, mfu 0.00%\r\n","step 1915: train loss 1.7938, val loss 1.9079\r\n","iter 1915: loss 1.6972, time 4453.21ms, mfu 0.00%\r\n","step 1916: train loss 1.7862, val loss 1.9098\r\n","iter 1916: loss 1.8112, time 4419.26ms, mfu 0.00%\r\n","step 1917: train loss 1.7727, val loss 1.9020\r\n","iter 1917: loss 1.7135, time 4315.68ms, mfu 0.00%\r\n","step 1918: train loss 1.7864, val loss 1.9011\r\n","iter 1918: loss 1.7315, time 4345.50ms, mfu 0.00%\r\n","step 1919: train loss 1.7790, val loss 1.9084\r\n","iter 1919: loss 1.7147, time 4364.51ms, mfu 0.00%\r\n","step 1920: train loss 1.7832, val loss 1.9110\r\n","iter 1920: loss 1.7523, time 5198.20ms, mfu 0.00%\r\n","step 1921: train loss 1.7853, val loss 1.9056\r\n","iter 1921: loss 1.7555, time 4517.87ms, mfu 0.00%\r\n","step 1922: train loss 1.7914, val loss 1.9147\r\n","iter 1922: loss 1.6119, time 4455.25ms, mfu 0.00%\r\n","step 1923: train loss 1.7889, val loss 1.8994\r\n","iter 1923: loss 1.8623, time 4445.43ms, mfu 0.00%\r\n","step 1924: train loss 1.7834, val loss 1.9093\r\n","iter 1924: loss 1.7233, time 4388.88ms, mfu 0.00%\r\n","step 1925: train loss 1.7860, val loss 1.9104\r\n","iter 1925: loss 1.7549, time 4303.47ms, mfu 0.00%\r\n","step 1926: train loss 1.7838, val loss 1.9097\r\n","iter 1926: loss 1.7317, time 4256.19ms, mfu 0.00%\r\n","step 1927: train loss 1.7820, val loss 1.9115\r\n","iter 1927: loss 1.7619, time 4762.60ms, mfu 0.00%\r\n","step 1928: train loss 1.7910, val loss 1.8970\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 1928: loss 1.7165, time 4736.12ms, mfu 0.00%\r\n","step 1929: train loss 1.7869, val loss 1.9170\r\n","iter 1929: loss 1.8729, time 4404.91ms, mfu 0.00%\r\n","step 1930: train loss 1.7875, val loss 1.9091\r\n","iter 1930: loss 1.7120, time 4437.57ms, mfu 0.00%\r\n","step 1931: train loss 1.7806, val loss 1.9067\r\n","iter 1931: loss 1.8063, time 4288.00ms, mfu 0.00%\r\n","step 1932: train loss 1.7866, val loss 1.9045\r\n","iter 1932: loss 1.7638, time 4414.71ms, mfu 0.00%\r\n","step 1933: train loss 1.7811, val loss 1.9057\r\n","iter 1933: loss 1.7095, time 4265.88ms, mfu 0.00%\r\n","step 1934: train loss 1.7821, val loss 1.9041\r\n","iter 1934: loss 1.7212, time 4395.88ms, mfu 0.00%\r\n","step 1935: train loss 1.7835, val loss 1.9050\r\n","iter 1935: loss 1.7552, time 5001.26ms, mfu 0.00%\r\n","step 1936: train loss 1.7901, val loss 1.9103\r\n","iter 1936: loss 1.9215, time 4303.45ms, mfu 0.00%\r\n","step 1937: train loss 1.7807, val loss 1.9122\r\n","iter 1937: loss 1.6971, time 4422.75ms, mfu 0.00%\r\n","step 1938: train loss 1.7741, val loss 1.9041\r\n","iter 1938: loss 1.7714, time 4342.73ms, mfu 0.00%\r\n","step 1939: train loss 1.7906, val loss 1.9085\r\n","iter 1939: loss 1.8687, time 4586.76ms, mfu 0.00%\r\n","step 1940: train loss 1.7856, val loss 1.9096\r\n","iter 1940: loss 1.8169, time 4369.96ms, mfu 0.00%\r\n","step 1941: train loss 1.7875, val loss 1.9115\r\n","iter 1941: loss 1.7131, time 4519.65ms, mfu 0.00%\r\n","step 1942: train loss 1.7832, val loss 1.9017\r\n","iter 1942: loss 1.7710, time 5060.90ms, mfu 0.00%\r\n","step 1943: train loss 1.7837, val loss 1.8997\r\n","iter 1943: loss 1.7462, time 4386.10ms, mfu 0.00%\r\n","step 1944: train loss 1.7867, val loss 1.9089\r\n","iter 1944: loss 1.6723, time 4381.52ms, mfu 0.00%\r\n","step 1945: train loss 1.7825, val loss 1.9001\r\n","iter 1945: loss 1.8526, time 4432.32ms, mfu 0.00%\r\n","step 1946: train loss 1.7794, val loss 1.9108\r\n","iter 1946: loss 1.8295, time 4495.41ms, mfu 0.00%\r\n","step 1947: train loss 1.7818, val loss 1.9052\r\n","iter 1947: loss 1.6755, time 4265.83ms, mfu 0.00%\r\n","step 1948: train loss 1.7799, val loss 1.8970\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 1948: loss 1.7548, time 4574.77ms, mfu 0.00%\r\n","step 1949: train loss 1.7789, val loss 1.9062\r\n","iter 1949: loss 1.8270, time 5084.77ms, mfu 0.00%\r\n","step 1950: train loss 1.7803, val loss 1.9034\r\n","iter 1950: loss 1.8144, time 4395.19ms, mfu 0.00%\r\n","step 1951: train loss 1.7770, val loss 1.9016\r\n","iter 1951: loss 1.8272, time 4331.70ms, mfu 0.00%\r\n","step 1952: train loss 1.7791, val loss 1.9038\r\n","iter 1952: loss 1.7614, time 4428.71ms, mfu 0.00%\r\n","step 1953: train loss 1.7817, val loss 1.8987\r\n","iter 1953: loss 1.6741, time 4387.91ms, mfu 0.00%\r\n","step 1954: train loss 1.7875, val loss 1.9117\r\n","iter 1954: loss 1.6065, time 4369.48ms, mfu 0.00%\r\n","step 1955: train loss 1.7814, val loss 1.9079\r\n","iter 1955: loss 1.8177, time 4538.71ms, mfu 0.00%\r\n","step 1956: train loss 1.7766, val loss 1.9184\r\n","iter 1956: loss 1.7756, time 4695.59ms, mfu 0.00%\r\n","step 1957: train loss 1.7853, val loss 1.9136\r\n","iter 1957: loss 1.7855, time 4795.93ms, mfu 0.00%\r\n","step 1958: train loss 1.7859, val loss 1.9099\r\n","iter 1958: loss 1.7398, time 4357.53ms, mfu 0.00%\r\n","step 1959: train loss 1.7874, val loss 1.9111\r\n","iter 1959: loss 1.7911, time 4498.02ms, mfu 0.00%\r\n","step 1960: train loss 1.7729, val loss 1.9199\r\n","iter 1960: loss 1.7062, time 4460.11ms, mfu 0.00%\r\n","step 1961: train loss 1.7789, val loss 1.9096\r\n","iter 1961: loss 1.7851, time 4367.83ms, mfu 0.00%\r\n","step 1962: train loss 1.7854, val loss 1.9219\r\n","iter 1962: loss 1.7563, time 4449.95ms, mfu 0.00%\r\n","step 1963: train loss 1.7879, val loss 1.9059\r\n","iter 1963: loss 1.6755, time 4310.60ms, mfu 0.00%\r\n","step 1964: train loss 1.7839, val loss 1.9030\r\n","iter 1964: loss 1.7909, time 4804.68ms, mfu 0.00%\r\n","step 1965: train loss 1.7792, val loss 1.9134\r\n","iter 1965: loss 1.8912, time 4479.51ms, mfu 0.00%\r\n","step 1966: train loss 1.7832, val loss 1.9118\r\n","iter 1966: loss 1.7235, time 4400.47ms, mfu 0.00%\r\n","step 1967: train loss 1.7765, val loss 1.9028\r\n","iter 1967: loss 1.7835, time 4260.84ms, mfu 0.00%\r\n","step 1968: train loss 1.7714, val loss 1.9116\r\n","iter 1968: loss 1.8458, time 4349.63ms, mfu 0.00%\r\n","step 1969: train loss 1.7842, val loss 1.9070\r\n","iter 1969: loss 1.7543, time 4493.01ms, mfu 0.00%\r\n","step 1970: train loss 1.7799, val loss 1.9095\r\n","iter 1970: loss 1.7182, time 4334.75ms, mfu 0.00%\r\n","step 1971: train loss 1.7782, val loss 1.9115\r\n","iter 1971: loss 1.7313, time 5010.83ms, mfu 0.00%\r\n","step 1972: train loss 1.7864, val loss 1.9098\r\n","iter 1972: loss 1.7741, time 4430.33ms, mfu 0.00%\r\n","step 1973: train loss 1.7827, val loss 1.9027\r\n","iter 1973: loss 1.8090, time 4396.50ms, mfu 0.00%\r\n","step 1974: train loss 1.7790, val loss 1.9106\r\n","iter 1974: loss 1.7067, time 4477.47ms, mfu 0.00%\r\n","step 1975: train loss 1.7800, val loss 1.9083\r\n","iter 1975: loss 1.8772, time 4611.43ms, mfu 0.00%\r\n","step 1976: train loss 1.7720, val loss 1.9035\r\n","iter 1976: loss 1.7146, time 4369.39ms, mfu 0.00%\r\n","step 1977: train loss 1.7726, val loss 1.9086\r\n","iter 1977: loss 1.7656, time 4373.91ms, mfu 0.00%\r\n","step 1978: train loss 1.7757, val loss 1.9047\r\n","iter 1978: loss 1.7416, time 4919.94ms, mfu 0.00%\r\n","step 1979: train loss 1.7836, val loss 1.9072\r\n","iter 1979: loss 1.8011, time 4367.81ms, mfu 0.00%\r\n","step 1980: train loss 1.7711, val loss 1.9049\r\n","iter 1980: loss 1.7919, time 4281.41ms, mfu 0.00%\r\n","step 1981: train loss 1.7868, val loss 1.8953\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 1981: loss 1.8516, time 4298.12ms, mfu 0.00%\r\n","step 1982: train loss 1.7731, val loss 1.9005\r\n","iter 1982: loss 1.7952, time 4311.12ms, mfu 0.00%\r\n","step 1983: train loss 1.7782, val loss 1.9062\r\n","iter 1983: loss 1.7364, time 4400.30ms, mfu 0.00%\r\n","step 1984: train loss 1.7758, val loss 1.9005\r\n","iter 1984: loss 1.8092, time 4299.47ms, mfu 0.00%\r\n","step 1985: train loss 1.7720, val loss 1.9067\r\n","iter 1985: loss 1.7449, time 4660.38ms, mfu 0.00%\r\n","step 1986: train loss 1.7818, val loss 1.9069\r\n","iter 1986: loss 1.7122, time 4985.84ms, mfu 0.00%\r\n","step 1987: train loss 1.7786, val loss 1.9037\r\n","iter 1987: loss 1.7748, time 4421.75ms, mfu 0.00%\r\n","step 1988: train loss 1.7773, val loss 1.8865\r\n","saving checkpoint to out-shakespeare-char\r\n","iter 1988: loss 1.7446, time 4506.27ms, mfu 0.00%\r\n","step 1989: train loss 1.7803, val loss 1.8974\r\n","iter 1989: loss 1.8863, time 4571.66ms, mfu 0.00%\r\n","step 1990: train loss 1.7780, val loss 1.9049\r\n","iter 1990: loss 1.7662, time 4508.10ms, mfu 0.00%\r\n","step 1991: train loss 1.7753, val loss 1.9083\r\n","iter 1991: loss 1.7430, time 4468.11ms, mfu 0.00%\r\n","step 1992: train loss 1.7762, val loss 1.9036\r\n","iter 1992: loss 1.8374, time 4434.64ms, mfu 0.00%\r\n","step 1993: train loss 1.7853, val loss 1.9030\r\n","iter 1993: loss 1.7092, time 5022.24ms, mfu 0.00%\r\n","step 1994: train loss 1.7776, val loss 1.9094\r\n","iter 1994: loss 1.7869, time 4524.25ms, mfu 0.00%\r\n","step 1995: train loss 1.7852, val loss 1.9035\r\n","iter 1995: loss 1.7709, time 4398.05ms, mfu 0.00%\r\n","step 1996: train loss 1.7796, val loss 1.9066\r\n","iter 1996: loss 1.7788, time 4378.06ms, mfu 0.00%\r\n","step 1997: train loss 1.7890, val loss 1.9004\r\n","iter 1997: loss 1.8201, time 4553.99ms, mfu 0.00%\r\n","step 1998: train loss 1.7813, val loss 1.9060\r\n","iter 1998: loss 1.8200, time 4307.71ms, mfu 0.00%\r\n","step 1999: train loss 1.7841, val loss 1.9083\r\n","iter 1999: loss 1.8063, time 4473.62ms, mfu 0.00%\r\n","step 2000: train loss 1.7796, val loss 1.9091\r\n","iter 2000: loss 1.6105, time 5247.45ms, mfu 0.00%\r\n"]}],"source":["!python train.py config/train_shakespeare_char.py --device=cpu --compile=False --eval_interval=1 --log_interval=1 --block_size=64 --batch_size=12 --n_layer=4 --n_head=4 --n_embd=128 --max_iters=2000 --lr_decay_iters=2000 --dropout=0.0"]},{"cell_type":"markdown","id":"cd543a64","metadata":{"papermill":{"duration":0.152252,"end_time":"2025-02-20T06:19:47.812266","exception":false,"start_time":"2025-02-20T06:19:47.660014","status":"completed"},"tags":[]},"source":["# 4. Creating Shekespeare style writing"]},{"cell_type":"code","execution_count":7,"id":"d4a5e904","metadata":{"execution":{"iopub.execute_input":"2025-02-20T06:19:48.164665Z","iopub.status.busy":"2025-02-20T06:19:48.164317Z","iopub.status.idle":"2025-02-20T06:20:09.025571Z","shell.execute_reply":"2025-02-20T06:20:09.024266Z"},"papermill":{"duration":21.024398,"end_time":"2025-02-20T06:20:09.027363","exception":false,"start_time":"2025-02-20T06:19:48.002965","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Overriding: out_dir = out-shakespeare-char\r\n","/kaggle/working/nanoGPT/sample.py:38: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\r\n","  checkpoint = torch.load(ckpt_path, map_location=device)\r\n","number of parameters: 0.80M\r\n","Loading meta from data/shakespeare_char/meta.pkl...\r\n","\r\n","\r\n","All beford with and is the the dispok of our take?\r\n","\r\n","QUEEN CELIar:\r\n","Farthalle us him to bardetlacome, away, my feand\r\n","acke of the own proof it here this nown\r\n","As egrives son contlatinctior, by weard\r\n","Will man is wansed lesing than this my cormen:\r\n","As that speaw you love she raddered to whom\r\n","the shall Sake on in on her evicke to the\r\n","Murch high him conto hall his but thand thrught! I ar wors!\r\n","\r\n","QUEENCESTIO:\r\n","O the shall should? I a do suke stild is:\r\n","Sadain the age sto ding mike are not reathme\r\n","And for hi\r\n","---------------\r\n","\r\n","MOUS:\r\n","From, I grivest I way my me!\r\n","\r\n","SICINIUS:\r\n","Wentuly the me madand the capling on thee;\r\n","Ind the much frepectss atcas lom to is she lace.\r\n","\r\n","SOF YORK:\r\n","Thou say, to ded the dear's coure by some so upon sir\r\n","Of Richard: there loves, sheir good duch dee.\r\n","\r\n","YORKEN ENCER:\r\n","I'll againce uncest the leve buch nomer the welch.\r\n","\r\n","Prive, more him aghold. I knother bearn,\r\n","The doughter I dood time thee nown the but\r\n","Thappoten'd what than the custeremely like\r\n","To that knew you, detwath, therefort in for Gloonk.\r\n","\r\n","GLOU\r\n","---------------\r\n","\r\n","MAULINCE INTE:\r\n","\r\n","Nays, I'll should him Mourck in the prive,\r\n","and the copace of his well otherse minds so me\r\n","wortinginentis of the him a word, and a burther!\r\n","\r\n","BRASTEDI:\r\n","Beforethere call, I vend and in thim blaid,\r\n","Withing feeeing biwn not my have eart I die;\r\n","Sive thou forles wee to must brood!\r\n","And forther of jurt the sown than there.\r\n","And and be love is come a clotty doth him:\r\n","And your firty: there caven the they gain.\r\n","\r\n","GHERD OLOK:\r\n","I mutherdernamty me with tenemman, light:\r\n","He yee our sonernemole, wel\r\n","---------------\r\n","\r\n","Thing with kings; and peock, say, as and the work\r\n","Alates corsunt.\r\n","\r\n","PORALONCE:\r\n","Where his, sail I gen wach preing him knows.\r\n","\r\n","\r\n","NORKENTER:\r\n","Were dears a rished are than shough, that becouse him,\r\n","And as o' go, this send with hernouse: the wife\r\n","OUn shim I I heave courcity, of morge,\r\n","As lattand we twith sen mon for But of the burse\r\n","What inser'lward, I unswery agotht wine,\r\n","And known with say, thy prost him your not\r\n","shongan at welce for on the Ripat!\r\n","Not brift I men'er good his suke to flather,\r\n","That\r\n","Marc\r\n","---------------\r\n","\r\n","BUTIS:\r\n","O, kin thy him? Why a cant your lese.\r\n","\r\n","KING RICHARD II:\r\n","Why said be and thee before comer do.\r\n","\r\n","GLEORD:\r\n","You lot harly.\r\n","If me I well ad me for bsore of hath,\r\n","To sould your thee heaver as you perty thy come them.\r\n","On groth that new I do lame, cour nonds;\r\n","Then all not then unter some son,\r\n","But Encead heremen comain not art upon in has be.\r\n","\r\n","Clive:\r\n","Leven youse da my she could to he dook.\r\n","\r\n","DUKEN EDWARD IV:\r\n","My lade my them movence of you pain:\r\n","Inaite you Godg here knows of fintent of your warmane;\r\n","\r\n","---------------\r\n","\r\n","\r\n","MEONTES:\r\n","Dist make hast for ustarl, theirs, goor, me lord.\r\n","I amperst and envery, a but a beand,\r\n","It what you; whou to thou crity my chame\r\n","That what dewn deathours to nother oblad.\r\n","Pecingure thee coldive mance, the moself you\r\n","his compy should king to condervess, thou bithin shold,\r\n","Yor to the ganto wearth'd faind aclegen\r\n","Crowaich old the prace in the artherno\r\n","When to kencan you.\r\n","\r\n","Sony corshall: hast is the drest the rumble grood crives your kto his\r\n","by to Richald he jrod brothinks.\r\n","\r\n","Secay:\r\n","Wha is t\r\n","---------------\r\n","\r\n","Shand the beaur lefest,\r\n","But spirnens's that king the not of ince in this all,\r\n","Your sold: cause: all your be ming uporters.\r\n","\r\n","KING ERYCUS:\r\n","Thou do lay hater a time a ahalk of thee.\r\n","\r\n","PARLIXUS:\r\n","I noth hather! Coming the me kim, whose word.\r\n","\r\n","First Cittinger:\r\n","The burrut ance to mose, say\r\n","And the with their he peast will cany meve.\r\n","\r\n","BRUCCIOLIY:\r\n","Of laffine, thenrouse to morrace a peonged their fach.\r\n","\r\n","MESS:\r\n","He cond of him diaten, wear and hathor appingerd: go carce;\r\n","What is nist then of on shand mand tha\r\n","---------------\r\n","\r\n","lagnery: new, loid not me, or thy behok heath\r\n","And the beith come they goode or sacks.\r\n","\r\n","KING SABERLLA:\r\n","Bastand it him tee.\r\n","\r\n","LORCUKES:\r\n","His benge there tenter at king Handurp;\r\n","What my to alpounions. I your from somer!\r\n","How of you sumer sought, hold strone sime!\r\n","That has wreate beed soul is, God,\r\n","Will my stake quentiouns and you not!\r\n","\r\n","Frors:\r\n","Sead wouldes of your hath the king we go thee?\r\n","\r\n","ROMEONE:\r\n","Thou ded mEdwcelian the words me sont: wor in in thuse\r\n","As bame wrow one son do me a domely the good my p\r\n","---------------\r\n","\r\n","GLEO:\r\n","So wrefore, in shall may coman's bear and moliny,\r\n","All known thing svay to breater,\r\n","I'll soos, not the imble of shall screes,\r\n","As sound is thou come them you countent,\r\n","Than his poother that cit the disce,\r\n","This nooplings: her and rrost thee conserver ach coal gament bechery oin\r\n","and shall veak his indling, and that man's dothers,\r\n","Ault of allong there she grast a but is in gut\r\n","Burttermal the givom my whones had; I\r\n","Both lothers sit with it hat ant.\r\n","\r\n","But ISABELLYt:\r\n","We that reservent, it fear was \r\n","---------------\r\n","\r\n","Hourther bod rathress:\r\n","And we my pay more. How you chald give.\r\n","\r\n","GLEONELY:\r\n","We had they slave a gay that that with ktas is hibes\r\n","Thing to this semert.\r\n","\r\n","MENENIUS:\r\n","And may, thou say he roor for was mutthers I do wo sorring,\r\n","You sento made striell my worself I im\r\n","Miline.\r\n","\r\n","LEONIUM:\r\n","To---way, you not shall EMardan:\r\n","Or me your with cound no come musther on thee\r\n","And of dedseracy merstlenencenten,\r\n","Your time such shall of I sing tither dead,\r\n","I morg his ti. Loshal Have yout be hather'd;\r\n","And of I dieth: thre\r\n","---------------\r\n"]}],"source":["! python sample.py --out_dir=out-shakespeare-char"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":2660706,"sourceId":4558658,"sourceType":"datasetVersion"},{"datasetId":6669576,"sourceId":10753606,"sourceType":"datasetVersion"}],"dockerImageVersionId":30887,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"papermill":{"default_parameters":{},"duration":9235.9307,"end_time":"2025-02-20T06:20:09.776907","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2025-02-20T03:46:13.846207","version":"2.6.0"}},"nbformat":4,"nbformat_minor":5}